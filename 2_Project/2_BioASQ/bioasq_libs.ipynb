{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451096cb-c4b3-4ee2-999b-5753fbb380f0",
   "metadata": {},
   "source": [
    "# `print_and_log_results`: Print and log results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1a0173-7c62-411d-9dd1-6a0909a0e22c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def print_and_log_results(trainer, log_path='./bioasq_results.txt'):\n",
    "    # Get metrics from the latest evaluation.\n",
    "    history         = trainer.state.log_history\n",
    "    accuracy        = round(history[0].get(\"eval_accuracy\", 0), 4)\n",
    "    f1_yes          = round(history[0].get(\"eval_f1_yes\", 0), 4)\n",
    "    f1_no           = round(history[0].get(\"eval_f1_no\", 0), 4)\n",
    "    macro_f1        = round(history[0].get(\"eval_macro_f1\", 0), 4)\n",
    "    train_loss      = round(history[1].get(\"train_loss\", 0), 4)\n",
    "    validation_loss = round(history[0].get(\"eval_loss\", 0), 4)\n",
    "\n",
    "    # Calculate total training time.\n",
    "    total_train_time = sum([log.get(\"train_runtime\", 0) for log in history])\n",
    "    epoch_time       = round(total_train_time / trainer.args.num_train_epochs, 2)\n",
    "    time_per_step    = round(total_train_time / trainer.state.max_steps, 4)\n",
    "\n",
    "    # Get hyperparameters.\n",
    "    hyperparameters  = trainer.args.to_dict()\n",
    "    learning_rate    = hyperparameters.get(\"learning_rate\", \"N/A\")\n",
    "    batch_size       = hyperparameters.get(\"per_device_train_batch_size\", \"N/A\")\n",
    "    num_epochs       = hyperparameters.get(\"num_train_epochs\", \"N/A\")\n",
    "\n",
    "    # Get model checkpoint.\n",
    "    checkpoint       = trainer.model.name_or_path\n",
    "\n",
    "    # Create log content.\n",
    "    log_content = (\n",
    "        f\"# {datetime.now().strftime('%Y.%m.%d.')}\\n\"\n",
    "        f\"<Hyperparameters>\\n\"\n",
    "        f\"- Model        : {checkpoint}\\n\"\n",
    "        f\"- Learning Rate: {learning_rate}\\n\"\n",
    "        f\"- Batch Size   : {batch_size}\\n\"\n",
    "        f\"- Epochs       : {num_epochs}\\n\\n\"\n",
    "        f\"<Results>\\n\"\n",
    "        f\"- Accuracy     : {accuracy}\\n\"\n",
    "        f\"- F1-yes       : {f1_yes}\\n\"\n",
    "        f\"- F1-no        : {f1_no}\\n\"\n",
    "        f\"- Macro-F1     : {macro_f1}\\n\"\n",
    "        f\"- Train Loss   : {train_loss}\\n\"\n",
    "        f\"- Validation Loss: {validation_loss}\\n\\n\"\n",
    "        f\"<Training Time>\\n\"\n",
    "        f\"- Total Time   : {round(total_train_time, 2)} seconds\\n\"\n",
    "        f\"- Time per Epoch: {epoch_time} seconds\\n\"\n",
    "        f\"- Time per Step : {time_per_step} seconds\\n\"\n",
    "    )\n",
    "\n",
    "    # Print results.\n",
    "    print(log_content)\n",
    "\n",
    "    # Write to log file.\n",
    "    with open(log_path, 'a') as f:\n",
    "        f.write(log_content + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046c8b4-2724-4027-a811-e52f64d0df9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `print_trial`: Print a trial of Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8f2d9-efda-4b0c-83f4-7a087f3fb97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trial(trial):\n",
    "    print('<Hyperparameters>')\n",
    "    print(f'- Model: {trial.user_attrs[\"model_name\"]}')\n",
    "    for k, v in trial.user_attrs['best_params'].items():\n",
    "        print(f'- {k:<15} : {v:.4}')\n",
    "    print()\n",
    "    \n",
    "    print('<Results>')\n",
    "    for k, v in trial.user_attrs['results'].items():\n",
    "        if k == 'eval_runtime':\n",
    "            continue\n",
    "        print(f'- {k:<15} : {v:.4}')\n",
    "    print(f'- {\"Training time\":<15}: {trial.user_attrs[\"training_time\"]:.4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ce251-93da-4358-875c-512640fa80a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[{'eval_loss': 0.5662793517112732,\n",
    "  'eval_accuracy': 0.7389705882352942,\n",
    "  'eval_f1_yes': 0.8498942917547568,\n",
    "  'eval_f1_no': 0.0,\n",
    "  'eval_macro_f1': 0.4249471458773784,\n",
    "  'eval_runtime': 1.5888,\n",
    "  'eval_samples_per_second': 171.196,\n",
    "  'eval_steps_per_second': 21.399,\n",
    "  'epoch': 1.0,\n",
    "  'step': 136},\n",
    " {'train_runtime': 9.0454,\n",
    "  'train_samples_per_second': 119.951,\n",
    "  'train_steps_per_second': 15.035,\n",
    "  'total_flos': 143727127541760.0,\n",
    "  'train_loss': 0.5790046243106618,\n",
    "  'epoch': 1.0,\n",
    "  'step': 136}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888fb51-a34b-4c81-be0d-d0a5da09d6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `compute_metrics`: Custom ftn to compute metrics of BioASQ-12b, 'yesno' task.\n",
    "- Accuracy.\n",
    "- F1-yes.\n",
    "- F1-no.\n",
    "- Macro-F1.\n",
    "- http://participants-area.bioasq.org/results/12b/phaseB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f3b6f8-cc7f-42bc-849c-b52c0f4b92ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds    = predictions.argmax(axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1_yes   = f1_score(labels, preds, pos_label=1)\n",
    "    f1_no    = f1_score(labels, preds, pos_label=0)\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\" : accuracy,\n",
    "        \"f1_yes\"   : f1_yes,\n",
    "        \"f1_no\"    : f1_no,\n",
    "        \"macro_f1\" : macro_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a53488-e521-4ed6-bd4d-f2548e2cecce",
   "metadata": {},
   "source": [
    "# `load_datasets`: load train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46492cb-e143-4293-8181-6b8492a8699e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def encode_label(ds):\n",
    "    \"\"\"Simply encode and change name of 'answer_exact', 'yes': 1, 'no': 0.\"\"\"\n",
    "    \n",
    "    ds = ds.map(\n",
    "        lambda x: {'answer_exact': 1 if x['answer_exact'] == 'yes' else 0}\n",
    "    )\n",
    "\n",
    "    ds = ds.rename_column('answer_exact', 'labels')\n",
    "\n",
    "    return ds\n",
    "\n",
    "def load_datasets_all():\n",
    "    \"\"\" Returns train_ds, valid_ds, and (merged) test_ds. \"\"\"\n",
    "    # Data path.\n",
    "    path_list = [\n",
    "        './datasets/training_12b.json',    # train set.\n",
    "        './datasets/12B1_golden.json',     # test set, split 1 ~ 4.\n",
    "        './datasets/12B2_golden.json',\n",
    "        './datasets/12B3_golden.json',\n",
    "        './datasets/12B4_golden.json'\n",
    "    ]\n",
    "\n",
    "    # Load.\n",
    "    df_list = []\n",
    "    for path in path_list:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            # Load json file.\n",
    "            data = json.load(f)   \n",
    "\n",
    "            # Read rows.\n",
    "            rows = []\n",
    "            for question in data['questions']:\n",
    "                if question['type'] == 'yesno':   # Load only samples with type = 'yesno'.\n",
    "                    row = {\n",
    "                        \"question\": question['body'],\n",
    "                        \"snippets\": \"\\n\".join([s['text'] for s in question['snippets']]),\n",
    "                        \"documents\": \"\\n\".join(question['documents']),\n",
    "                        \"answer_exact\": question.get('exact_answer', ''),\n",
    "                        \"answer_ideal\": question.get('ideal_answer', '')\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "                    \n",
    "            # Construct df.\n",
    "            df = pd.DataFrame(rows)\n",
    "            df['answer_ideal'] = df['answer_ideal'].apply(lambda x: x[0])\n",
    "            df_list.append(df)\n",
    "    \n",
    "    # Split train and test set.\n",
    "    train_df = df_list[0]\n",
    "    test_df  = df_list[1:]\n",
    "    \n",
    "    # Null check.\n",
    "    for df in df_list:\n",
    "        if df.isnull().values.any():\n",
    "            print(\"Null found!\")\n",
    "    \n",
    "    # df -> ds.\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    test_ds  = []\n",
    "    for df in test_df:\n",
    "        test_ds.append(Dataset.from_pandas(df))\n",
    "        \n",
    "    # Encode label.\n",
    "    train_ds = encode_label(train_ds)\n",
    "    for i in range(len(test_ds)):\n",
    "        test_ds[i] = encode_label(test_ds[i])\n",
    "        \n",
    "    # Train-Valid split.\n",
    "    split    = train_ds.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "    train_ds = split['train'].shuffle()\n",
    "    valid_ds = split['test'].shuffle()\n",
    "        \n",
    "    return train_ds, valid_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fcd95-bb4d-4aae-a4a4-5e093ee93e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(ds_type='train'):\n",
    "    \"\"\" Returns specified ds. 'train', 'test', 'test-1' ~ 'test-4'. \"\"\"\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Data paths.\n",
    "    path_list = [\n",
    "        './datasets/training_12b.json',    # train set.\n",
    "        './datasets/12B1_golden.json',     # test set, split 1 ~ 4.\n",
    "        './datasets/12B2_golden.json',\n",
    "        './datasets/12B3_golden.json',\n",
    "        './datasets/12B4_golden.json'\n",
    "    ]\n",
    "    \n",
    "    # Handle dataset type argument.\n",
    "    if ds_type == 'train':\n",
    "        selected_paths = [path_list[0]]  # Only training dataset.\n",
    "    elif ds_type == 'test':\n",
    "        selected_paths = path_list[1:]   # All test datasets.\n",
    "    elif ds_type.startswith('test-'):\n",
    "        idx = int(ds_type.split('-')[1]) - 1\n",
    "        selected_paths = [path_list[idx + 1]]  # Specific test split.\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid ds_type: {ds_type}. Choose from 'train', 'test', 'test-1', 'test-2', 'test-3', 'test-4'.\")\n",
    "\n",
    "    # Load datasets.\n",
    "    df_list = []\n",
    "    for path in selected_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            rows = []\n",
    "            for question in data['questions']:\n",
    "                if question['type'] == 'yesno':  # Load only 'yesno' samples.\n",
    "                    row = {\n",
    "                        \"question\": question['body'],\n",
    "                        \"snippets\": \"\\n\".join([s['text'] for s in question['snippets']]),\n",
    "                        \"documents\": \"\\n\".join(question['documents']),\n",
    "                        \"answer_exact\": question.get('exact_answer', ''),\n",
    "                        \"answer_ideal\": question.get('ideal_answer', '')\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "\n",
    "            df = pd.DataFrame(rows)\n",
    "            df['answer_ideal'] = df['answer_ideal'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "            df_list.append(df)\n",
    "\n",
    "    # Check for null values.\n",
    "    for df in df_list:\n",
    "        if df.isnull().values.any():\n",
    "            print(\"Null found!\")\n",
    "\n",
    "    # Convert DataFrame to Dataset.\n",
    "    if ds_type == 'train':\n",
    "        train_ds = Dataset.from_pandas(df_list[0])\n",
    "        \n",
    "        # Train-Validation split.\n",
    "        split = train_ds.train_test_split(test_size=0.2)\n",
    "        train_ds = split['train'].shuffle()\n",
    "        valid_ds = split['test'].shuffle()\n",
    "        return train_ds, valid_ds\n",
    "\n",
    "    elif ds_type == 'test':\n",
    "        test_ds = [Dataset.from_pandas(df) for df in df_list]\n",
    "        return test_ds\n",
    "\n",
    "    elif ds_type.startswith('test-'):\n",
    "        test_ds = Dataset.from_pandas(df_list[0])\n",
    "        return test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd92872-114c-4018-b174-557205cfdf84",
   "metadata": {},
   "source": [
    "# `get_logits_for_ensemble`: Calculate logits of each model for ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d923fd49-93f9-4795-aad9-6065227f3966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import disable_progress_bar\n",
    "\n",
    "\n",
    "# for soft voting, i.e. mean of logits.\n",
    "def get_logits_for_ensemble(ds, model, tokenizer, batch_size):\n",
    "    # Disable progress bar.\n",
    "    disable_progress_bar()\n",
    "    \n",
    "    # cuda device.\n",
    "    device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model_logits = []\n",
    "\n",
    "    for i in range(0, len(ds), batch_size):\n",
    "        # Extract the batch from the dataset\n",
    "        batch = ds[i:i + batch_size]\n",
    "        batch = Dataset.from_dict(batch)\n",
    "\n",
    "        # Tokenize the batch\n",
    "        def tokenize(sample):\n",
    "            return tokenizer(sample['input'], \n",
    "                             truncation      = True, \n",
    "                             padding         = 'max_length',\n",
    "                             max_length      = 512,\n",
    "                             return_tensors  ='pt')\n",
    "        \n",
    "        batch_tokenized = batch.map(tokenize, batched=False)\n",
    "        \n",
    "        # Calculate batch_logits.\n",
    "        batch_logits = []\n",
    "        for s in range(min(batch_size, len(batch_tokenized['input_ids']))):\n",
    "            input_ids = torch.tensor(batch_tokenized['input_ids'][s], device=device)\n",
    "            attention_mask = torch.tensor(batch_tokenized['attention_mask'][s], device=device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits.cpu().numpy()\n",
    "            \n",
    "            sample_logit = np.mean(logits, axis=0)\n",
    "            \n",
    "            batch_logits.append(sample_logit)\n",
    "            \n",
    "        model_logits.extend(batch_logits)\n",
    "\n",
    "    return model_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3055d7-9daf-481a-85c1-cd9bf6d2a480",
   "metadata": {},
   "source": [
    "# `split_by_snippets_docs`: for each sample, split each snippets and docs, and make a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a282bcff-f3c1-4f05-8d5d-a2fdc60d465b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_by_snippets_docs(ds, only_no):\n",
    "    splitted_inputs = []\n",
    "    splitted_labels = []\n",
    "    \n",
    "    for sample in ds:\n",
    "        idx_snippets  = sample['input'].find('\\nSnippets:')\n",
    "        idx_doc       = sample['input'].find('\\nRetrieved Chunks:')\n",
    "        question      = sample['input'][:idx_snippets]\n",
    "        snippets      = sample['input'][idx_snippets:idx_doc]\n",
    "        docs          = sample['input'][idx_doc + len('\\nRetrieved Chunks:\\n'):]\n",
    "        label         = sample['labels']\n",
    "\n",
    "        if only_no:\n",
    "            if label == 0:\n",
    "                # add with snippets.\n",
    "                splitted_inputs.append(question + snippets + \"\\nAnswer:\\n\")\n",
    "                splitted_labels.append(label)\n",
    "\n",
    "                # add with docs.\n",
    "                splitted_inputs.append(question + \"\\nSnippets:\\n\" + docs)\n",
    "                splitted_labels.append(label)\n",
    "                \n",
    "            # if 'yes', just copy original sample.\n",
    "            else:\n",
    "                splitted_inputs.append(sample['input'])\n",
    "                splitted_labels.append(label)\n",
    "        else:\n",
    "            # add with snippets.\n",
    "            splitted_inputs.append(question + snippets + \"\\nAnswer:\\n\")\n",
    "            splitted_labels.append(label)\n",
    "\n",
    "            # add with docs.\n",
    "            splitted_inputs.append(question + \"\\nSnippets:\\n\" + docs)\n",
    "            splitted_labels.append(label)\n",
    "            \n",
    "    splited_ds = Dataset.from_dict({\"input\": splitted_inputs, \"labels\": splitted_labels})\n",
    "    \n",
    "    return splited_ds   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be60b2e-db33-430f-ab2a-96a500a41e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
