
- Code: [here](https://github.com/aruwad-git/nlp-portfolio/tree/main/2_Project/2_BioASQ)
# 1. Introduction.

## 1.1. About.
- ì´ ê¸€ì€ **Medical NLP**ì—ì„œ **Top-tier Competition**ìœ¼ë¡œ ì¸ì •ë°›ëŠ” **BioASQ Competition**ì— ë„ì „í•œ ê³¼ì • ë° ê²°ê³¼ë¥¼ ì •ë¦¬í•œë‹¤.
- ê¸°ê°„ : 1/2 ~ 1/10, 2025.
- **Keywords**: RAG, medical NLP, LLM fine-tuning, data augmentation, asynchronized HTTP request.
- **Tech Stacks**:
  - **For RAG**: ğŸ¤—, Bitsandbytes, PEFT, Optuna, LangChain, FAISS ë“±.
  - **For HTTP Request**: aiohttp, asyncio, ThreadPoolExecutor, BeautifulSoup.
  - Others: sklearn, numpy, matplotlib, TensorBoard ë“±. 
- **Highlights**:
  - Highly domain-specificí•œ medical NLP taskì—ì„œ **ë†’ì€ ì„±ëŠ¥ì˜ RAG êµ¬ì¶•**.
    - Macro F1 of **0.9541** (top 1~20%). 
  *\* Performance is for post-competition and 'yes/no' subtask.*
    - RAG Pipeline: ğŸ¤—, `langchain`, `langsmith`.
  - **ëŒ€ìš©ëŸ‰ HTTP request ì²˜ë¦¬**ë¡œ íš¨ìœ¨ì ì¸ Vector DB êµ¬ì¶•.
    - Multithread: `ThreadPoolExecutor`.
    - Asyncronized request: `asyncio`.
    - 13,025 URL requestsë¥¼ single-threaded ëŒ€ë¹„ ì•½ x17ë°° ë¹ ë¥´ê²Œ ì²˜ë¦¬ **(26 hrs $\rightarrow$ 90 mins)**.
  - **Fine-Tuning of Domain-specific LLM** for medical applications.
    - BERT-like LLM for medical QA: BioBERT, PubMedBERT, BioELECTRA ë“±.
    - 4-bits Quantization: `bitsandbytes`.
    - Adapter-based PEFT, LoRA: `peft`.
    - íš¨ìœ¨ì ì¸ hyperparameter tuning: `optuna`.
  - **Data Augmentation** ë° **Class-weighted Training**ìœ¼ë¡œ ë§¤ìš° ë¶ˆê· í˜•í•œ class distributionì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì‘.
    - ì£¼ì–´ì§„ URLsë¡œë¶€í„° HTML retrieval: `bs4.BeautifulSoup`.
    - Similarity searchë¥¼ í†µí•œ document retrival: `faiss`.
    - Training cost ë° performanceë¥¼ ê³ ë ¤í•œ ìµœì ì˜ sampling ë°©ë²• íƒìƒ‰.
    - Class distributionì„ 74%ì—ì„œ 58%ê¹Œì§€ ë‚®ì¶”ê³ , Custom trainer for class-weighted Trainingìœ¼ë¡œ F1 score í¬ê²Œ ê°œì„ .

## 1.2. BioASQ Competition.

[![BioASQ Competition](https://velog.velcdn.com/images/aruwad/post/86016309-3b90-4e8f-92b4-d775f6cda116/image.JPG)](https://www.bioasq.org/)

- [**BioASQ Competition**](https://www.bioasq.org/)ì€ Medical NLP ë¶„ì•¼ì—ì„œ ê°€ì¥ ê¶Œìœ„ìˆëŠ” êµ­ì œ ëŒ€íšŒ ì¤‘ í•˜ë‚˜ë‹¤.
- CMU, Stanford ë“±ì˜ Top-tier participants, 2,000í¸ ì´ìƒì˜ publications ë“± academic society ë° industryì—ì„œ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆë‹¤.
- **ê¸°ê°„** : 2013ë…„ ì´ë˜ë¡œ ë§¤ë…„ ê°œìµœë˜ê³  ìˆìœ¼ë©°, ì˜¬í•´ë¡œ 13ë…„ì°¨ë‹¤.
- **ì£¼ìµœ ê¸°ê´€** : National Centre for Scientific Research (NCSR) in Greece, University of Huston, USA ë“±.
- **Tasks**: ë§¤ë…„ ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ subtasksë¥¼ ì œì‹œí•˜ë©°, Exact answerë§Œ í¬í•¨í•˜ëŠ” ê¸°ì¡´ datasetsê³¼ ë‹¬ë¦¬ ê±°ì˜ ëª¨ë“  NLP taskë¥¼ í¬í•¨í•œë‹¤.
  - **Task a**: ìˆ˜ì²œë§Œ ì´ìƒì˜ PubMed ë…¼ë¬¸ì˜ ê²€ìƒ‰ ë° ë¶„ë¥˜ë¥¼ ìœ„í•œ MeSH ì¶”ì¶œ.
  - **Task b**: LLM ê¸°ë°˜ ë‹¤ì–‘í•œ NLP tasksë¥¼ ìˆ˜í–‰: Biomedical QA, Information retrieval, Summarization ë“±.
  - **Task Synergy**: Academic institutions, Industry ë“±ì´ ì—°ê³„í•˜ì—¬ PubMed ê¸°ë°˜ QA ì‹œìŠ¤í…œ êµ¬ì¶•. 
  - **Task MultiClinSum**: Lengthy clinical case reportsë¥¼ ë‹¤êµ­ì–´ë¡œ summarize.
  - **Task BioNNE-L**, **Task ELCardioCC**, **Task GutBrainIE** ë“±.
  
## 1.3. Task Description.
- BioASQ-12bì˜ subtask ì¤‘ í•˜ë‚˜ì¸ 'Exact answer classification'ì„ ëŒ€ìƒìœ¼ë¡œ í•˜ì˜€ë‹¤.
- Question ë° ê´€ë ¨ ì •ë³´ë“¤ì— ê¸°ë°˜í•´ 'yes' or 'no'ë¥¼ ë‹µë³€í•˜ëŠ” binary text classification ë¬¸ì œë‹¤.
- ê´€ë ¨ ì •ë³´ëŠ” ë¬¸ì¥ ë‹¨ìœ„ì˜ ì§¤ë§‰í•œ ì •ë³´ì¸ Snippets ë° ê´€ë ¨ PubMed ë…¼ë¬¸ì˜ URLë“¤ì¸ Documentsë¡œ êµ¬ì„±ëœë‹¤.
- Example I/O:
  - Question: "Is TIM-3 a target for cancer immunotherapy in NSCLC?"
  - Snippets: 
    - "Our results imply that implementing combined treatment on CIK cells before transfusion via antibodies targeting PD-L1, LAG-3, TIM-3, and CEACAM-1 might improve the efficiency of CIK therapy for NSCLC patients."
    - "Furthermore, TIM-3 and CEACAM1 were strongly expressed simultaneously during long-term CIK culture and showed a significant and mutually positive correlation."
  - Documents:
    - http://www.ncbi.nlm.nih.gov/pubmed/27699239
    - http://www.ncbi.nlm.nih.gov/pubmed/29440769
  - Answer: 1 (for 'yes').
- Metrics: accuracy, f1-yes, f1-no, macro f1.
- Evaluation: 
  - ì‹¤ì œ rankingì€ ì—¬ëŸ¬ phase (e.g. in 2024, phase A, phase A+, and phase B) ë° test splits (1~4)ì—ì„œ, ì—¬ëŸ¬ subtasksë“¤ì´ ë™ì‹œì— ê³ ë ¤ëœë‹¤.
  - ë³¸ í”„ë¡œì íŠ¸ëŠ” 2024ë…„ì˜ [Leaderboard](http://participants-area.bioasq.org/results/12b/phaseB/)ì„ ì°¸ê³ í•˜ì—¬ ëŒ€ëµì ìœ¼ë¡œ í‰ê°€í•˜ì˜€ë‹¤.
  
---
  
# 2. Data Preparation.
## 2.1. Data Retrieval.

```python
import json
from datasets import Dataset
import pandas as pd

def load_datasets_all():
...

# Load.
df_list = []
for path in path_list:
    with open(path, 'r', encoding='utf-8') as f:
        # Load json file.
        data = json.load(f)   

        # Read rows.
        rows = []
        for question in data['questions']:
            if question['type'] == 'yesno':   # Load only samples with type = 'yesno'.
                row = {
                    "question": question['body'],
                    "snippets": "\n".join([s['text'] for s in question['snippets']]),
                    "documents": "\n".join(question['documents']),
                    "answer_exact": question.get('exact_answer', ''),
                    "answer_ideal": question.get('ideal_answer', '')
                }
                rows.append(row)

        # Construct df.
        df = pd.DataFrame(rows)
        df['answer_ideal'] = df['answer_ideal'].apply(lambda x: x[0])
        df_list.append(df)
  
 ...

train_df.head()
```
![](https://velog.velcdn.com/images/aruwad/post/ff27c39f-8364-4247-823b-df9056e41cd0/image.JPG)


- DatasetëŠ” [ì—¬ê¸°ì„œ](http://participants-area.bioasq.org/datasets/) ê°„ë‹¨í•œ ì ˆì°¨ í›„ ì‰½ê²Œ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆë‹¤.
- `json` ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨íˆ parsingí•˜ê³  `pd.DataFrame`ì„ êµ¬ì¶•í•˜ì˜€ë‹¤.
- ê° features ë° labelsì€ ì•„ë˜ì™€ ê°™ë‹¤:

| Column         | Data Type | Description                                                       |
|----------------|-----------|-------------------------------------------------------------------|
| `question`     | Text      | One sentence for the question.                                   |
| `snippets`     | Text      | One to many sentences providing short information.               |
| `documents`    | URL       | One to many URLs linking to research papers from PubMed.         |
| `labels`       | Integer   | 0 for 'no', 1 for 'yes'.                                         |
| `answer_ideal` | Text      | Label for the ideal answer task.                                 |




## 2.2. Data Size.

![](https://velog.velcdn.com/images/aruwad/post/339ad31e-932f-462c-9eba-f6d11c080e0e/image.JPG)

```
n_samples   = 1,356
n_snippets  = 16,567
n_docs      = 13,025
```

- Sampleì€ ì´ 1,356ê°œë¡œ, large LLMì„ fine-tuning í•˜ê¸°ì—” í„°ë¬´ë‹ˆì—†ì´ ë¶€ì¡±í•˜ë‹¤.
- Mid~large ê·œëª¨ì˜ pretrained LLM ê¸°ë°˜ RAGë¥¼ ê¸°ë³¸ ë°©í–¥ìœ¼ë¡œ ì¡ì•¼ì•¼ê² ë‹¤.
- Full-trainingì€ ë‹¹ì—°íˆ ë°”ëŒì§í•˜ì§€ ì•Šë‹¤. Head-only, adapter-based PEFT, zero-shot learning ë“±ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.
- ê´€ë ¨ ì •ë³´ë“¤(snippetsì™€ documents)ì´ ê½¤ ë§ìœ¼ë¯€ë¡œ, data augmentationì„ ê³ ë ¤í•´ë³´ì.

## 2.3. Length of Features.

```python
# Columns of features.
cols     = ['question', 'snippets', 'documents']
len_cols = ['len_question', 'len_snippets', 'len_documents']

# Calculate each length.
train_df[len_cols] = train_df[cols].map(len)

# Display statistics of each length.
train_df[len_cols].describe()
train_df[len_cols].hist()
```
![](https://velog.velcdn.com/images/aruwad/post/c2c5ba97-6ed8-4512-aa62-4e8a6edc3ac0/image.JPG)

![](https://velog.velcdn.com/images/aruwad/post/2a213634-e608-400a-bdea-ade3bd3d7c41/image.JPG)

- Questionì˜ ê¸¸ì´ëŠ” ëŒ€ë¶€ë¶„ reasonableí•˜ë‹¤. ì ˆëŒ€ truncated ë˜ë©´ ì•ˆë˜ëŠ” ë¶€ë¶„ì´ì§€ë§Œ, í†µìƒì˜ `max_length=512`ì—ì„  ë¬¸ì œ ì—†ì„ ê²ƒ ê°™ë‹¤.
- SnippetsëŠ” ìƒë‹¹íˆ ë¶ˆì•ˆí•˜ë‹¤. ë¬¸ì¥ì˜ ê°œìˆ˜ë„ 1ê°œë¶€í„° ìˆ˜ì‹­ ê°œë¡œ ë‹¤ì–‘í•˜ê³ , ê° ë¬¸ì¥ì˜ ê¸¸ì´ë„ ì²œì°¨ë§Œë³„ì´ë‹¤. ë¬´ë ¤ 19,150ì˜ ê¸¸ì´ë¥¼ ê°–ëŠ” outlierë„ ìˆë‹¤. `tokenizer(stride=128)`ê³¼ ê°™ì€ íš¨ìœ¨ì ì¸ splitì„ ê³ ë¯¼í•´ì•¼ í•œë‹¤.
- Documentsì˜ ê²½ìš°, ë³¸ë¬¸ì´ ì•„ë‹Œ URLì´ë¯€ë¡œ ê¸¸ì´ëŠ” retrieval í•˜ê¸° ë‚˜ë¦„ì´ë‹¤. í•˜ë‚˜ì˜ urlì€ 43ì˜ ê³ ì • ê¸¸ì´ë¥¼ ê°€ì§€ë¯€ë¡œ, ëŒ€ëµ 5ê°œ ì „í›„ì˜ urlì´ ìˆë‹¤.

> #### Caution) Outliers in Documents for URL Request..
> Documentsì˜ 25%ëŠ” ëŒ€ëµ 10~15ê°œì˜ URLì„ ê°€ì§€ê³  ìˆìœ¼ë©°, ê°€ì¥ ë§ì€ ê²½ìš° ë¬´ë ¤ 100ê°œì— ê°€ê¹ë‹¤. ì´ëŠ” URL requestì—ì„œ ìƒë‹¹í•œ loadë¥¼ ì•¼ê¸°í•  ê²ƒì´ë¯€ë¡œ, ì£¼ì˜í•´ì„œ ì²˜ë¦¬í•´ì£¼ì–´ì•¼ í•œë‹¤.

## 2.4. Question and Snippets.
```
- Question: Is JTV519 (K201) a potential drug for the prevention of arrhythmias?

- Snippets: 
We compared the suppressive effect of K201 (JTV519), a multiple-channel blocker and cardiac ryanodine receptor-calcium release channel (RyR2) stabilizer, with that of diltiazem, a Ca(2+ )channel blocker, in 2 studies of isoproterenol-induced (n = 30) and ischemic-reperfusion-induced VAs (n = 38) in rats.

(and many others...)

```

- ì§ˆë¬¸ ë° ê´€ë ¨ ì •ë³´ëŠ” ìƒë‹¹íˆ ì „ë¬¸ì ì¸ ì˜ë£Œ ì§€ì‹ìœ¼ë¡œ, general pretrained LLMì€ ì˜ ë‹µë³€í•˜ì§€ ëª»í•  ê²ƒì´ë‹¤. ë”°ë¼ì„œ medical NLPì— íŠ¹í™”ëœ LLMì´ í•„ìš”í•˜ë©°, ìµœì†Œí•œ classification headë¼ë„ fine-tuningì€ í”¼í•  ìˆ˜ ì—†ì„ ê²ƒ ê°™ë‹¤.
- ëª‡ëª‡ ìˆ˜ë™ ëŒ€ì¡° ê²°ê³¼, SnippetsëŠ” Documentsì˜ ë…¼ë¬¸ìœ¼ë¡œë¶€í„° ì¶”ì¶œëœ ì •ë³´ë¡œ ë³´ì¸ë‹¤. ê·¸ëŸ¬ë‚˜ ë…¼ë¬¸ë“¤ì—ì„œ Snippetsìœ¼ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì€ ìœ ìš©í•œ ì •ë³´ê°€ ê½¤ ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. Documentsë¡œë¶€í„°ì˜ ì¶”ê°€ì ì¸ ì •ë³´ê°€ ë„ì›€ì´ ë  ê²ƒ ê°™ë‹¤.

## 2.5. Class Distribution.

```python
label_proportions = train_df['labels'].value_counts(normalize=True)
label_proportions.plot(kind='bar', legend=False)
```

```
labels
1    0.739171
0    0.260829
Name: count, dtype: float64
```

![](https://velog.velcdn.com/images/aruwad/post/93f01bdd-13cd-4b7b-bcff-c6df46d1aaf7/image.JPG)


- Train setì˜ labelì€ ë§¤ìš° unevení•˜ë‹¤. 'yes'ê°€ 74% ê°€ëŸ‰ì„ ì°¨ì§€í•˜ê³  ìˆë‹¤.
- Train setì´ ë§¤ìš° ì‘ë‹¤ëŠ” ì‚¬ì‹¤ê³¼ ë”ë¶ˆì–´, ì´ëŠ” ìƒë‹¹í•œ overfittingì„ ì•¼ê¸°í•  ê²ƒì´ë‹¤. ì´ì— ëŒ€í•œ ëŒ€ì±…ì´ ì‹œê¸‰í•˜ë‹¤.

---

# 3. Non-RAG.

## 3.1. Data Preparation.

```python
cols_query     = ['question', 'labels']
train_ds_query = train_ds.select_columns(cols_query)
valid_ds_query = valid_ds.select_columns(cols_query)

train_ds_query
```

```
Dataset({
    features: ['question', 'labels'],
    num_rows: 1085
})
```

- ì•„ì´ë””ì–´ë¥¼ ì–»ê¸° ìœ„í•´ `'question'`ê³¼ `'labels'`ë§Œì„ ì‚¬ìš©í•´ì„œ ëª‡ëª‡ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì.

## 3.2. No Fine-Tuning.

```python
import evaluate
from transformers import pipeline

# Model.
classifier      = pipeline("text-classification", model="distilbert-base-uncased")

# Metrics.
metric_accuracy = evaluate.load("accuracy")

# Predict.
predictions      = classifier(train_ds_query['question'])
y_pred           = [int(pred['label'].split('_')[-1]) for pred in predictions]     # convert to label.

# Compute accuracy.
accuracy = metric_accuracy.compute(predictions = y_pred, 
                                   references  = train_ds_query['labels'])

# Print.
print(f"Accuracy: {accuracy['accuracy']:.2f}")
```

```
Accuracy: 0.44
```

- ğŸ¤—ì˜ `pipeline`ê³¼ `evaluate`ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨íˆ ëŒë ¤ë³´ì.
- LLMì€ ì˜¤ëŠ˜ë„ ì–´ê¹€ì—†ì´ ë“±ì¥í•˜ëŠ” **DistilBERT** êµ°ìœ¼ë¡œ ì‹œì‘í•˜ì. Classification headë¥¼ ë”°ë¡œ í›ˆë ¨í•  í•„ìš”ë„ ì—†ë‹¤. <small>~~(ê±°ì˜ ë™ë„¤ë¶)~~</small>
- ë‹¹ì—°íˆ ì •í™•ë„ëŠ” ì¢‹ì§€ ì•Šë‹¤. `accuracy = 0.44`ë¡œ ë™ì „ ë˜ì§€ê¸°ë³´ë‹¤ ëª»í•˜ë‹¤.
- ê·¸ë ‡ë‹¤ë©´ ì´ ë„ë©”ì¸ì˜ ì „ë¬¸ê°€ ì¤‘ í•˜ë‚˜ì¸ **BioBERT**ëŠ” ì–´ë–¨ê¹Œ? ~~<small>(ìë‘ìŠ¤ëŸ° í•œêµ­ì¸ì˜ ëª¨ë¸)</small>~~

```python
checkpoint = "dmis-lab/biobert-base-cased-v1.2"
```

![](https://velog.velcdn.com/images/aruwad/post/a7749933-eda9-4bca-9522-1f9bc8e21cc5/image.JPG)

- ì •í™•ë„ê°€ 0.74ë¼ ë†’ì•„ë³´ì¼ ìˆ˜ ìˆê² ì§€ë§Œ ~~<small>(ì•ì— ì•ˆ ì½ìœ¼ì…¨êµ¬ë‚˜)</small>~~, f1ì„ ë³´ë©´ ëˆˆì¹˜ ë¹ ë¥¸ **BioBERT**ê°€ 1 epochë§Œì— ì–ì‚½í•œ ë°©ë²•ì„ íŒŒì•…í–ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ëŒ€ì¶© OMRì— í•œ ì¤„ë¡œ ê¸‹ëŠ” ê²ƒê³¼ ë‹¤ë¥¼ ê²ƒì´ ì—†ë‹¤. ê³µë¶€ê°€ í•„ìš”í•˜ë‹¤.
- ìƒë‹¹í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•œ **BioBERT**ì˜ ê²½ìš°ì—ë„ ì¶”ì¸¡ ê²°ê³¼ê°€ ë§¤ìš° ê·¹ë‹¨ì ì¸ ê²ƒìœ¼ë¡œ ë³´ì•„(ê±°ì˜ yes), ì§ˆë¬¸ë“¤ì´ **BioBERT**ì˜ í•™ìŠµ ì‹œì¡°ì°¨ë„ ë³´ì§€ ëª»í–ˆë˜ dataì´ê³ , ë”°ë¼ì„œ ì´ taskê°€ ìƒë‹¹íˆ domain-specificí•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

## 3.3. Basic Training.
### 3.3.1. Model and Tokenizer.

![](https://velog.velcdn.com/images/aruwad/post/0bd59c2a-13de-4ea5-a2d1-a0512b82f373/image.JPG)

- í†µìƒì ì¸ ğŸ¤—ì˜ `AutoTokenizer` ë° `AutoModel`ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
- `tokenize_query(x)`
  - `max_length=512` : ì•„ì§ question ë°–ì— ì—†ê¸´ í•˜ì§€ë§Œ, snippets ë° documents ë¥¼ ì¶”ê°€í•˜ë©´ ëŒ€ë¶€ë¶„ LLMë“¤ì˜ í•œê³„ì¸ 512ë¥¼ ëŒ€ë¶€ë¶„ ë„˜ê¸¸ ê²ƒì´ë¯€ë¡œ, 512ë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.
  - `padding`ì€ ì—¬ê¸°ì„œ í•˜ì§€ ì•ŠëŠ”ë‹¤. í•  ìˆ˜ë§Œ ìˆë‹¤ë©´ `DataCollator`ë¥¼ í†µí•´ dynamically padding í•˜ëŠ”ê²Œ ëŒ€ë¶€ë¶„ ë” ì¢‹ë‹¤.
- `train_ds_query.map()`: ê¸°ì´ˆì ì¸ ë¶€ë¶„ì´ì§€ë§Œ í˜¹ì‹œ ëª¨ë¥´ëŠ” ë¶„ë“¤ì„ ìœ„í•´ì„œ ~~+ì·¨ì—…ì„ ìœ„í•´ì„œ~~ ì–¸ê¸‰í•˜ë©´, `tokenize_query(train_ds_query)`ë¡œ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ê±´ ì¢‹ì§€ ì•Šë‹¤. `.map()`ì„ ì‚¬ìš©í•˜ë©´ ~~ğŸ¤—ê°€ ì—´ì‹¬íˆ ê´‘ê³ í•˜ëŠ”~~ Arrowì˜ ì¥ì , batch process ë“±ì„ ì‚¬ìš©í•˜ì—¬ ëˆˆì— ë„ê²Œ ë¹¨ë¼ì§„ë‹¤.
- `remove_columns=['question']`: í•­ìƒ ë‹¤ ì“´ columnì€ íœ´ì§€í†µì— ì˜ ë„£ì–´ì£¼ëŠ” ìŠµê´€ì„ ë“¤ì´ì. ë‚˜ì¤‘ì— shape ì•ˆ ë§ì•„ì„œ ê°œê³ ìƒ í•  ìˆ˜ ìˆë‹¤.
  
> #### ğŸ™‹â€â™€ï¸ Dynamic Paddingì´ ë­”ê°€ìš”?
> - ëŒ€ë¶€ë¶„ì˜ transformer ê¸°ë°˜ LLMë“¤ì€ í•œ batch ë‚´ì˜ sampleë“¤ì´ ë™ì¼í•œ ê¸¸ì´ë¥¼ ê°€ì§ˆ ê²ƒì„ ìš”êµ¬í•œë‹¤. ë”°ë¼ì„œ ì§§ì€ sampleë“¤ì€ paddingì´ í•„ìš”í•˜ë‹¤.
> - ê·¸ëŸ°ë° `tokenizer(padding='max_length')`ì™€ ê°™ì´ `tokenizer` ë‚´ë¶€ì—ì„œ ì§€ì •í•˜ë©´, ëª¨ë“  512 ë¯¸ë§Œì˜ sampleë“¤ì´ ì „ë¶€ 512ê¹Œì§€ padding ëœë‹¤. ì´ëŠ” ì—°ì‚°ì†ë„, ë©”ëª¨ë¦¬, ì„±ëŠ¥ ë“±ì— ìƒë‹¹íˆ ë¹„íš¨ìœ¨ì ì´ë‹¤.
> - ğŸ¤—ì˜ `DataCollator`ë¥¼ ì‚¬ìš©í•˜ë©´ ë”± batchê°€ êµ¬ì„±ëœ ì´í›„ì— ê·¸ batchì— ëŒ€í•´ specificí•˜ê²Œ, ë™ì ìœ¼ë¡œ paddingê³¼ ê°™ì€ ì—¬ëŸ¬ê°€ì§€ ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ **Dynamic Padding**ì´ë¼ê³  í•œë‹¤.
> - ì˜ˆë¥¼ ë“¤ì–´ ì´ë²ˆ batchì—ì„œ ê°€ì¥ ê¸´ ë†ˆì´ 24ê°œì˜ í† í°ì„ ê°€ì§€ê³  ìˆìœ¼ë©´, ì–˜ë³´ë‹¤ ì§§ì€ ì• ëŠ” 24ê°œê¹Œì§€ë§Œ padding ëœë‹¤.
> - ğŸ¤—ëŠ” ì´ ì™¸ì—ë„ `DataCollatorForTokenClassification`, `DataCollatorForSeq2Seq` ë“± ì—¬ëŸ¬ê°€ì§€ë¥¼ ì§€ì›í•˜ë‹ˆ ê¼­ [í™•ì¸í•´ë³´ì](https://huggingface.co/docs/transformers/main_classes/data_collator).

### 3.3.2. Train.

![](https://velog.velcdn.com/images/aruwad/post/f13665c1-f30c-4acd-998d-0525a81097cb/image.JPG)

- ğŸ¤—ì˜ `TrainingArguments` ë° `Trainer`ì™€ í†µìƒì ì¸ hyperparametersë¡œ í†µìƒì ì¸ trainingì„ êµ¬ì¶•í•˜ì˜€ë‹¤. ~~<small>ë­ ì£„ë‹¤ í†µìƒì ì´ë˜</small>~~
- `DataCollatorWithPadding`: ì•„ê¹Œ ë´¤ì§€ë§Œ token lengthê°€ ë§¤ìš° ë‹¤ì–‘í•˜ë¯€ë¡œ dynamic paddingì„ í•´ì£¼ì—ˆë‹¤.
- `bioasq_libs.compute_metrics`: Custom functionìœ¼ë¡œ BioASQ ëŒ€íšŒë¥¼ ìœ„í•œ metrics ê³„ì‚°ì„ í•´ì£¼ì—ˆë‹¤ (accuracy, f1-yes, f1-no, macro f1).
- line 40 : Head-only training.
- ë‹¹ì—°í•œ ê²°ê³¼ì´ì§€ë§Œ, í•™ìŠµì´ ì˜ ë˜ì§€ ì•ŠëŠ”ë‹¤. ë‚˜ê°™ì•„ë„ ì €ëŸ° QA ë³´ì—¬ì¤˜ë´¤ì 1ë„ ëª¨ë¥¼ë“¯.
- ì •í™•í•œ ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, questionë“¤ì´ ëŒ€ëµ IIDë¥¼ ë§Œì¡±í•œë‹¤ê³  ëŒ€ëµ ì§ì‘í•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìµœì†Œí•œ ë­”ê°€ë¼ë„ ë°°ì› ì„í…Œë‹ˆê¹Œ.
- ì–´ì°¨í”¼ ì§ˆë¬¸ì€ ë‚œìƒ ì²˜ìŒ ë³´ëŠ” í•´ê´´í•œ ê²ƒë“¤ì´ë¯€ë¡œ, ì‚¬ì‹¤ìƒ class distributionì„ ë³´ê³  ì°ì€ ê±°ë‚˜ ë‹¤ë¦„ì—†ë‹¤. ê·¸ë ‡ë‹¤ë©´ class distributionì„ ì œëŒ€ë¡œ ì¤˜ë³´ì.

### 3.3.3. Class-weighted Training.

![](https://velog.velcdn.com/images/aruwad/post/1aa41ae6-4db2-4f4d-afe9-861045fcfcbd/image.JPG)

- Pythonì˜ `Counter`ë¥¼ ì‚¬ìš©í•˜ì—¬ class weightsë¥¼ êµ¬í–ˆë‹¤. 'no'ì˜ weightì´ ì•½ 3ë°°ê°€ ë˜ì—ˆë‹¤.
- ì°¸ê³ ë¡œ ì—¬ê¸°ì„œ ì‚¬ìš©í•œ class weightsëŠ” $w_c = \frac{1}{n_c}$ë¡œ, ë‹¨ìˆœíˆ ìƒ˜í”Œ ìˆ˜ì˜ ì—­ìˆ˜ì´ë‹¤.

> #### ğŸ™‹â€â™€ï¸ ì™œ Class weightsë¥¼ normalize í•˜ì§€ ì•Šê³  ê·¸ëƒ¥ ì“°ë‚˜ìš”?
> ì—¬ê¸°ì„œ êµ¬í•œ Class weightsëŠ” ë°”ë¡œ ì•„ë˜ì—ì„œ training ì‹œ lossì˜ ê°’ì„ ë³´ì •í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. ë”°ë¼ì„œ class frequencyë¥¼ ì§ì ‘ lossì— ë°˜ì˜í•  ìˆ˜ ìˆëŠ” raw valuesê°€ ì¼ë°˜ì ìœ¼ë¡œ ë” ë‚«ë‹¤. 
> íŠ¹íˆ ë‚˜ëŠ” ë”ìš± ì„¸ë°€í•˜ê²Œ class-weighted training ê³¼ì •ì„ ì»¨íŠ¸ë¡¤í•˜ê¸° ìœ„í•´ hyperparameterë¥¼ ì¶”ê°€í•˜ê³  íŠœë‹í•˜ì˜€ëŠ”ë°, ì´ëŸ´ ê²½ìš° ë”ë”ìš± raw valuesê°€ ë‚«ë‹¤.

![](https://velog.velcdn.com/images/aruwad/post/7b4c87d3-f4a0-466a-bdbc-7d009d0c9508/image.JPG)

- Class-weighted trainingì„ ìœ„í•´ Trainerë¥¼ ìƒì†ë°›ì•„ custom trainerë¥¼ ë§Œë“¤ì—ˆë‹¤. `compute_loss`ë¥¼ overridingí•˜ë©´ ê¸°ì¡´ì˜ `CrossEntropy`ë¥¼ ê³„ì‚°í•  ë•Œ class weightsë¥¼ ê³ ë ¤í•œë‹¤. ì¦‰, ì´ ê²½ìš° 'no' sampleì˜ ê²½ìš° 'yes' ë³´ë‹¤ ëŒ€ëµ 3ë°°ì˜ íŒ¨ë„í‹°ë¥¼ ê°€í•œë‹¤.

> #### ğŸ™‹â€â™€ï¸ `global class_weights`ëŠ” ë­”ê°€ìš”? ê¸€ë¡œë²Œ ë³€ìˆ˜ë¼ë‹ˆ ë³´ê¸° ë¶ˆí¸í•˜ë„¤ìš”.
> ì•„ì‹œë‹¤ì‹œí”¼ lossëŠ” batch ë‹¨ìœ„ë¡œ ê³„ì‚°ëœë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œ `def compute_loss` ì•ˆì—ì„œ ê³„ì‚°í•˜ë©´, class_weightsëŠ” **ê·¸ ë°°ì¹˜ì˜ class_weights**ê°€ ë˜ì–´ë²„ë¦°ë‹¤.
> ì´ëŠ” ë‹¹ì—°íˆ ì˜ë„í•˜ì§€ ì•Šì€ ê²°ê³¼ì´ê³ , ê²°ê³¼ì ìœ¼ë¡œ class imbalanceë¥¼ í•´ê²°í•˜ì§€ë„ ëª»í•  ë¿ë§Œ ì•„ë‹ˆë¼, training ê²°ê³¼ë¥¼ sample ìˆœì„œì— ìƒë‹¹íˆ ë¯¼ê°í•˜ê²Œ ë§Œë“ ë‹¤.
>
> ì´ëŸ° ì‹¤ìˆ˜ëŠ” ìˆ™ë ¨ìë„ ë†“ì¹˜ê¸° ì‰¬ìš´ë°, ë”±íˆ ì—ëŸ¬ê°€ ë‚˜ëŠ” ê²ƒë„, training ì‹œ ëª…í™•íˆ ë“œëŸ¬ë‚˜ëŠ” ê²ƒë„ ì•„ë‹ˆë¼ **ë‚˜ì¤‘ì— ì°¾ê¸° ì§„ì§œ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤**. ëª…ì‹¬í•˜ì.

![](https://velog.velcdn.com/images/aruwad/post/b90b4eb5-07ac-4cba-8554-5c80b942940e/image.JPG)

- ì˜ˆìƒëŒ€ë¡œ ì„±ëŠ¥ì´ ì¢‹ì•„ì¡Œë‹¤. 'no'ë¥¼ í‹€ë¦¬ëŠ” ê²ƒì— 3ë°° íŒ¨ë„í‹°ë¥¼ ì£¼ì—ˆê³ , **DistilBERT** êµ°ì€ ë§ˆì§€ëª»í•´ ëª‡ëª‡ 'yes'ë¥¼ 'no'ë¡œ ë°”ê¿¨ì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ì—­ì‹œ class distributionë§Œì„ ë³´ê³  ëœë¤ ì¶”ì¸¡í•œ ê²ƒê³¼ ê±°ì˜ ë‹¤ë¥´ì§€ ì•Šë‹¤.
- 0.70 ë“± ìˆ«ìì— í˜„í˜¹ë˜ì§€ ë§ì. ëª¨ë‘ 'yes'ë¡œ ì˜ˆì¸¡í•˜ëŠ” naive classifierëŠ” ì •í™•ë„ê°€ 0.74ì´ê³  macro f1ì´ 0.4254ì´ë‹¤. ì´ë¥¼ ë„˜ì§€ ëª»í•˜ëŠ” ëª¨ë“  ëª¨ë¸ì€ ì“°ë ˆê¸°ë‹¤.

---

# 4. Training Optimization.
- ì§„ì§œ LLMìœ¼ë¡œ ì§„ì§œ trainingì„ í•˜ê¸° ì „ì— training ê³¼ì •ì˜ optimizationì´ í•„ìš”í•˜ë‹¤. ~~<small>ë¶€ìë¼ì„œ 4090 10ê°œ ìˆìœ¼ë©´ ì•ˆí•´ë„ ë ì§€ë„ ã…ã…</small>~~
- **Quantization**: ì¼ë°˜ì ìœ¼ë¡œ floating point ë³€ìˆ˜ ë° ì—°ì‚°ì— ì‚¬ìš©í•˜ëŠ” 16-bits/32-bitsë¥¼ 4-bits/8-bitsë¡œ ì••ì¶•í•´ì¤€ë‹¤. ìƒë‹¹í•œ memory savingì´ ê°€ëŠ¥í•˜ì—¬ ìš”ì¦˜ì€ ëŒ€ê·œëª¨ LLM ì‚¬ìš©ì— í•„ìˆ˜ë‹¤.
- **PEFT**: Parametric-Efficient Fine-Tuningì˜ ì•½ìë¡œ, ì´ë¦„ì²˜ëŸ¼ training í•  parameterì˜ ê°œìˆ˜ë¥¼ í¬ê²Œ ì¤„ì—¬ì¤€ë‹¤. ì˜ˆì»¨ëŒ€ 10ì–µê°œ íŒŒë¼ë¯¸í„° ì¤‘ 100ë§Œê°œë§Œ ì‚¬ìš©í•´ trainingì´ ê°€ëŠ¥í•  ì •ë„ë¡œ íŒŒê²©ì ì¸ ë°©ë²•ì´ë¼, ì—­ì‹œ ìš”ì¦˜ì€ í•„ìˆ˜ë‹¤.
ë³¸ í”„ë¡œì íŠ¸ì—ì„  ëŒ€í‘œì ì¸ adapter ë°©ì‹ ì¤‘ LoRAë¥¼ ì‚¬ìš©í•œë‹¤.
- **Optuna**: ë³¸ê²©ì ì¸ hyperparamter tuningì„ ìœ„í•´ ì‚¬ìš©í•œë‹¤. íŠœë‹í•  ë³€ìˆ˜ì˜ ì´ë¦„ê³¼ ë²”ìœ„ë¥¼ ì§€ì •í•´ì£¼ë©´, ì•Œì•„ì„œ 'ë˜‘ë˜‘í•˜ê²Œ' ì°¾ì•„ì¤€ë‹¤. NLP ì™¸ì—ë„ ë‹¤ì–‘í•œ ML ë¶„ì•¼ì— êµ­ë£°ë¡œ ìë¦¬ì¡ê³  ìˆë‹¤.

> #### ğŸ™‹â€â™€ï¸ QLoRAëŠ” ë­”ê°€ìš”?
> - ì›ë˜ Quantizationê³¼ PEFTëŠ” ì „í˜€ ë³„ê°œì˜ ì˜ì—­ì´ì—ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš”ì¦˜ì€ ëŒ€ë¶€ë¶„ ë‘˜ì„ (íŠ¹íˆ LoRAì™€) ë¬¶ì–´ **QLoRA**ë¡œ ë¶€ë¥´ë©° ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆë‹¤. ì™œì¼ê¹Œ?
> - ë³´í†µ quantizationì„ í•œ ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ì˜ libraryë¡œ ì§ì ‘ trainingì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ëŒ€ë¶€ë¶„ì˜ libraryëŠ” ìµœì†Œ 16-bitsë¥¼ ê°€ì •í•˜ê³  êµ¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤.
> - ê·¸ëŸ¬ë‚˜ **LoRA**ì™€ ê°™ì€ adapter-based ë°©ì‹ì€ ê¸°ì¡´ ëª¨ë¸ì˜ layerì— adapterë¼ëŠ” ìƒˆë¡œìš´ layer ìì²´ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ë¯€ë¡œ, quantized modelë„ í›ˆë ¨í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.
> - í•œ ë§ˆë””ë¡œ **QLoRA**ëŠ” ë‹˜ ì»´í“¨í„°ì—ì„œë„ llama-10B ê°™ì€ ë¶„ë“¤ì„ training í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°€ì¥ í¸ë¦¬í•œ ë°©ë²•ì´ë‹¤!

## 4.1. QLoRA.

```python
# 4-bits Quantization.
from transformers import BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

skip_modules = ["classifier", "pre_classifier"]    # This modules are not quantized. Need to check with different pretrained LLMs.

quantization_config = BitsAndBytesConfig(
    load_in_4bit            = True,                # Quantize into 4-bits.                  
    llm_int4_threshold      = 6.0,                 # Layers whose norm of weights <= 6.0 are not quantized. 
    bnb_4bit_compute_dtype  = torch.float16,       
    low_cpu_mem_usage       = True,
    llm_int8_skip_modules   = skip_modules         # Need to check with different pretrained LLMs.
)

model_query = prepare_model_for_kbit_training(model_query)

# LoRA.
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r              = 8,                     # Rank of the low-rank decomposition.
    lora_alpha     = 32,                    # Scaling factor.
    lora_dropout   = 0.1,                   # Dropout.
    target_modules = "all-linear",          # Model-agnostic.
    task_type      = "SEQ_CLS",             # Task type = sequential clf.
    init_lora_weights="olora")
    
model_query = get_peft_model(model_query, lora_config)

# Print how many parameters we can ignore thx to LoRA :)
model_query.print_trainable_parameters()
```

```
trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925
```

- ì´ì œëŠ” ğŸ¤— ì‹êµ¬ê°€ ëœ `bnb`ì™€ `peft`ë¡œ QLoRAë¥¼ êµ¬í˜„í•˜ì˜€ë‹¤.
- `skip_modules`: ê°€ì¥ ì†Œì¤‘í•œ ë¶€ë¶„ì¸ Classification headëŠ” quantizationì—ì„œ ì œì™¸í•˜ì˜€ë‹¤.
- `llm_int4_threshold=6.0`: Layerì˜ weightsì˜ normì´ 6.0ì„ ë„˜ì§€ ì•Šìœ¼ë©´ quantized ë˜ì§€ ì•ŠëŠ”ë‹¤. Normì´ ë‚®ì„ìˆ˜ë¡ quantizationì˜ ì™œê³¡ì´ ì‹¬í•´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ì‰½ê²Œ ë§í•˜ë©´ 100ì—ì„œ 1 ë¹¼ë©´ 1%ì§€ë§Œ 10ì—ì„œ 1 ë¹¼ë©´ 10%ë¼ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•˜ë©´ ëœë‹¤.
- `lora_dropout`: ìƒˆë¡œ ì¶”ê°€ëœ LoRA adapterë“¤ì˜ dropoutì´ë‹¤. ë‹¹ì—°í•œ ì–˜ê¸°ì§€ë§Œ ì¤‘ìš”í•œ hyperparameterë‹¤.
- `target_modules='all_linear'`: êµ­ë£°ì²˜ëŸ¼ ì‚¬ìš©í•˜ëŠ” ì„¸íŒ…ì´ë‹¤. ì‚¬ì‹¤ ë‚œ headê¹Œì§€ ê±´ë“œëŠ”ê²Œ ë§ˆìŒì— ì•ˆë“¤ì–´ ì˜ ì•ˆì“°ì§€ë§Œ, ë‹¤ì–‘í•œ checkpointì— ë¬¸ì œ ì—†ì´ í˜¸í™˜ë˜ì–´ ìì£¼ ì“°ì¸ë‹¤.
- LoRA ë•ë¶„ì— ì´ 6700ë§Œê°œì˜ parameter ì¤‘ 74ë§Œê°œë§Œ í›ˆë ¨í•˜ë©´ ëœë‹¤. ì•„ì£¼ í›Œë¥­í•˜ë‹¤. ~~(ê·¼ë° ì™œ 1%ì§€..?)~~

> #### ğŸ’â€â™‚ï¸ 4-bitsë¼ë©´ì„œ ì™  `'llm_int8_skip_modules'` ì¸ê°€ìš”? ì˜¤íƒ€ì•„ë‹˜?
> ì•„ë‹ˆë‹¤. ì•ˆíƒ€ê¹ê²Œë„ ğŸ¤— `transformers ver. 4.40`ë¶€í„° `bnb quantization_config`ì™€ `peft`ê°€ ì¶©ëŒì´ ë‚˜ê³  ìˆë‹¤. `transformers`ì— ì˜í•´ `Linear4bit`ë¡œ ë°”ë€ weightsê°€ `modules_to_save`ì— ì¶”ê°€ë˜ê³ , ì´ë¥¼ `peft`ê°€ trainingí•˜ê¸° ìœ„í•´ gradients ê³„ì‚°ì„ í™œì„±í™”ì‹œí‚¤ë ¤ í•˜ê¸° ë•Œë¬¸ì— `TypeError`ê°€ ë‚œë‹¤.
> ì´ëŠ” ì ì–´ë„ ì‘ë…„ 5ì›”ë¶€í„° ìˆë˜ ë¬¸ì œì¸ë° ì•„ì§ë„ ì•ˆ ê³ ì³ì¡Œê³ , workaroundë¡œì„œ `'llm_int8_skip_modules'`ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ì•Œë ¤ì ¸ìˆìœ¼ë‹ˆ [ì°¸ê³ í•˜ì](https://github.com/huggingface/peft/issues/1720).

![](https://velog.velcdn.com/images/aruwad/post/d266fb0b-b182-4da8-a8d2-1f9ef71ff603/image.JPG)

![](https://velog.velcdn.com/images/aruwad/post/06c128c6-2f0c-4b39-b305-782673d2b52a/image.JPG)


- QLoRAë¥¼ ì ìš©í•˜ê¸° ì „(ìœ„)ê³¼ í›„(ì•„ë˜)ì˜ training timeì„ ë¹„êµí•˜ì˜€ë‹¤. ë‘˜ ë‹¤ DistilBERTë¥¼ 1 epochë§Œ í›ˆë ¨í•˜ì˜€ë‹¤.
- ì¥ë‚œí•˜ëƒê³  í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, ëª¨ë¸ì´ ì»¤ì§€ê³  toy dataê°€ ì•„ë‹Œ real dataë¡œ í›ˆë ¨í•˜ë©´ ì—„ì²­ë‚œ ì°¨ì´ê°€ ë²Œì–´ì§„ë‹¤. 
(í•„ìì˜ 3070ì˜ ê²½ìš° ë‹¹ì¥ DistilBERTë¡œ IMDbë§Œ í•´ë³´ë”ë¼ë„ í‰ê·  x5ë°° ì´ìƒ ì°¨ì´ê°€ ë‚¬ë‹¤.)

## 4.2. Optuna.

```python
import optuna

# Objective ftn.
def objective(trial, model):
    # Hyperparameters to tune.
    hyperparams = {
        'learning_rate': trial.suggest_float('learning_rate', 3e-5, 5e-5, log=True)
    }
    
    ...
    
# Create Optuna study and optimize.
study = optuna.create_study(direction='maximize')
study.optimize(lambda trial: objective(trial, model_query), n_trials=2)

# Show best trial.
study.best_trial

```

- Hyperparameter tuningì„ ìœ„í•œ objective í•¨ìˆ˜ë‹¤.
- Optunaì˜ objectiveëŠ” íš¨ìœ¨ì ì¸ íƒìƒ‰ì„ ìœ„í•´ í•˜ë‚˜ì˜ ë…ë¦½ì ì¸ ì‹¤í–‰ë‹¨ìœ„ë¡œ ì·¨ê¸‰ë˜ë¯€ë¡œ, ê°€ëŠ¥í•˜ë‹¤ë©´ ë°–ì— ìˆëŠ” variable ë° functionsë¥¼ ê°€ì ¸ì˜¤ì§€ ì•ŠëŠ” ê²Œ ì¢‹ë‹¤. ~~<small>ì‘ì€ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê·¸ëƒ¥ trainingì— í•„ìš”í•œ ê²ƒë“¤ì„ ë‹¤ ë•Œë ¤ë°•ìœ¼ë©´ ëœë‹¤.</small>~~
- `trial.suggest`: ìµœì í™” í•  ê°’ë“¤ì„ ì´ëŸ° ì‹ìœ¼ë¡œ ì œì•ˆí•´ì£¼ë©´ ì•Œì•„ì„œ ì°¾ì•„ì¤€ë‹¤.
- `n_trials=2`: ì´ íšŸìˆ˜ë§Œí¼ hyperparameter combinationì„ ê³ ë¥´ê³  ì‹œë„í•œë‹¤.
- `study.optimize()`: ì´ì œ ëŒë ¤ë†“ê³  í¸í•˜ê²Œ ë†€ê³  ì˜¤ë©´ ëœë‹¤ ã…ã….

# 5. Document Retrieval.

## 5.1. Data Preparation.

```python
cols_rag     = ['question', 'labels', 'snippets', 'documents']
train_ds_rag = train_ds.select_columns(cols_rag)
valid_ds_rag = valid_ds.select_columns(cols_rag)
```

- ë“œë””ì–´ ë³¸ê²©ì ì¸ task ì‹œì‘ì´ë‹¤! `snippets`ì™€ `documents`ë¥¼ ê°€ì ¸ì™€ training setì„ ì™„ì„±í•´ë³´ì.

## 5.2. Prompt.

```python
prompt_exact = """\
Question: {question}
Snippets:
{snippets}
Retrieved Chunks:
{retrieved_chunks}
Answer:
"""
```

- Retrievalì— ì‚¬ìš©í•  ê°„ë‹¨í•œ promprtì´ë‹¤.
- ë¬¼ë¡  promptë¥¼ ì˜ ê¹ì•„ queryì— ì…í˜€ì•¼í•˜ì§€ë§Œ, ë‹¤ì–‘í•œ promptë¥¼ ë’·ë°›ì¹¨í•  ì‹œê°„ê³¼ computational resourceê°€ ë„ì €íˆ ì—†ì—ˆê¸° ë•Œë¬¸ì— ê·¸ëƒ¥ retrievalì—ì„œ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬í–ˆë‹¤.
- ì‚¬ì‹¤ ë‹¤ì–‘í•œ ë°©ì‹ì˜ prompted engineeringì„ ì‹œë„í–ˆê³  ëª‡ëª‡ì€ ê½¤ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ì§€ë§Œ ì»¤ë„ì´ ì£½ì–´ì„œ ì£„ë‹¤ ë‚ ë ¤ë²„ë ¸ë‹¤ ^^;

## 5.3. Retrieval from docs: raw docs -> DB -> retrieved docs.

```python
from langchain.text_splitter import CharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

def retrieve_from_docs(query, documents, top_k_retrieval, chunk_size, chunk_overlap):
    
    # Text splitter.
    text_splitter = CharacterTextSplitter(separator      = ". ", 
                                          chunk_size     = chunk_size, 
                                          chunk_overlap  = chunk_overlap)
    
    # Embedding model.
    model_kwargs    = {'device': device}
    model_name      = "sentence-transformers/all-MiniLM-L6-v2"
    embedding_model = HuggingFaceEmbeddings(model_name    = model_name,
                                            model_kwargs  = model_kwargs)
    
    # For each document, split, embed, and retrieve the most relevant chunk.
    retrievals = []
    chunks     = []
    for doc in documents:
        chunks.extend(text_splitter.split_text(doc))                     # Split the document into chunks
        
    vector_store = FAISS.from_texts(chunks, embedding_model)             # FAISS vector store.

    result = vector_store.similarity_search(query, k=top_k_retrieval)    # Similarity search to retrieve top-k relevant documents.
    retrievals.extend([res.page_content for res in result])              # Store relevant chunks.
    
    return retrievals
```

- ëª¨ë“  URLsë¡œë¶€í„° ê°€ì ¸ì˜¨ documentsë¥¼ í•œë²ˆì— ì²˜ë¦¬í•˜ì—¬ vector DBì— ì €ì¥í•˜ê³ , top-k relevent docsë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ì´ë‹¤.
- ì›ë˜ëŠ” ê° URL, ì¦‰ ê° ë…¼ë¬¸ìœ¼ë¡œë¶€í„° top-kê°œë¥¼ ê°€ì ¸ì˜¤ë ¤ í–ˆìœ¼ë‚˜, 100ì‹œê°„ì´ ë„˜ëŠ” ì˜ˆìƒ ì‹œê°„ê³¼ PubMedê°€ requestë¥¼ ê±°ë¶€í–ˆë‹¤ëŠ” ë©”ì„¸ì§€ë¥¼ ë³´ë©° ë°”ë¡œ ë§ˆìŒì„ ë°”ê¿¨ë‹¤ ^^;
- `CharacterTextSplitter`: `langchain`ì˜ character ê¸°ë°˜ splitterë‹¤. Question/Snippets/Documentsê°€ ëª…í™•í•œ predefined wordë¡œ êµ¬ë¶„ë˜ê³ , ë˜ êµ¬ë¶„í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, í†µìƒì ì¸ `RecursiveCharacterTextSplitter` ëŒ€ì‹  ì‚¬ìš©í–ˆë‹¤.
- `HuggingFaceEmbeddings`: í†µìƒì ì¸ NLP tasksì— ë„ë¦¬ ì“°ì´ëŠ” `sentence-transformers/all-MiniLM-L6-v2`ë¥¼ ì‚¬ìš©í–ˆë‹¤.
- Vector db ë° searchì—ëŠ” í†µìƒì ì¸ `FAISS`ë¥¼ ì‚¬ìš©í–ˆë‹¤.

## 5.4. Retrieval from URL: URL -> raw docs.

- URLsë¡œë¶€í„° raw docs (ë…¼ë¬¸)ì„ ê°€ì ¸ì˜¤ëŠ” ë¶€ë¶„ì´ë‹¤.
- ë…¼ë¬¸ì˜ ë³¸ë¬¸ì—ëŠ” ë¬´ë£Œë¡œ ì ‘ê·¼í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, abstractë¥¼ ê°€ì ¸ì™”ë‹¤.
- ìš°ìŠµê²Œ ë´¤ë‹¤ê°€ í•˜ë£¨ì¢…ì¼ ì”¨ë¦„í–ˆë˜ ë¶€ë¶„ì´ë‹¤. ê·¸ì¹˜ë§Œ ë•ë¶„ì— ì‹¤ë¬´ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ë§ì´ ë°°ì› ë‹¤.

### 5.4.1. Single Sample.

```python
from langchain.document_loaders import WebBaseLoader

def preprocessing_rag(sample):
    results = []
    doc_urls  = sample['documents'].split('\n')    # URL of each document.
    documents = []

    for url in doc_urls:
        loader           = WebBaseLoader(url)
        document         = loader.load()[0]
        abstract_pattern_begin = 'AbstractPubMedPMID\n\n\n        Abstract\n        \n      \n\n\n      \n      '
        abstract_idx_begin     = document.page_content.find(abstract_pattern_begin) + len(abstract_pattern_begin)
        abstract_idx_end       = document.page_content.find('\n', abstract_idx_begin)

        abstract         = document.page_content[abstract_idx_begin:abstract_idx_end]
        documents.append(abstract)

    retrieved_chunks = retrieve_from_each_doc(query     = sample["question"],
                                              documents = documents)

    # Join retrieved chunks into a single string.
    retrieved_chunks = "\n".join(retrieved_chunks)

    # Use the prompt template to create the input text.
    input_text = prompt.format(
        question          = sample["question"],
        snippets          = sample["snippets"],
        retrieved_chunks  = retrieved_chunks
    )

    # Use the encoded label directly.
    labels = sample["labels"]
    
    return {'input_text': input_text, 
            'labels': labels}
```

- í•˜ë‚˜ì˜ sampleì˜ URLsë¥¼ ê°ê° ì½ê³  í•œë•€í•œë•€ ê°€ì ¸ì˜¤ëŠ” ê²ƒë¶€í„° ì‹œì‘í–ˆë‹¤. ì¦‰ í•˜ë‚˜ì˜ processê°€ (ë‹¹ì—°íˆ) ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
- `abstract_pattern_begin`: ê·¸ë ‡ë‹¤, í•˜ë“œì½”ë”© ë§ë‹¤. ì• ì´ˆì— multithread & async êµ¬í˜„í•˜ë ¤ë©´ ë‹¤ ê°ˆì•„ì—ì–´ì•¼ í•œë‹¤ëŠ” ê±¸ ì•Œê¸°ì— ì¼ë‹¨ ëŒ€ì¶© ì‘ì„±í–ˆë‹¤. ~~<small>ëŒ€í‘œì ì¸ ë³´ì—¬ì£¼ê¸°ì‹ ì½”ë”© ã…ã…</small>~~
- ë„ˆ~~ë¬´ë‚˜ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¤ì„œ ê·¸ëƒ¥ ë°”ë¡œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°”ë‹¤.

### 5.4.2. Single Batch.

```python
total_urls_in_batch = 0
for idx in range(len(batch["question"])):
    question  = batch['question'][idx]
    snippets  = batch['snippets'][idx]
    documents = batch['documents'][idx]
    label     = batch['labels'][idx]

    # Split documents into URLs.
    doc_urls = documents.split("\n")
    doc_contents = []

    total_urls_in_batch += len(doc_urls)
```

- ë‹¤ìŒìœ¼ë¡œ êµ¬í˜„ì„ batchë¡œ í™•ì¥í–ˆë‹¤. ë³¸ì§ˆì€ requestì˜ ë³‘ë ¬í™”ì´ë¯€ë¡œ ì•„ì§ í° ì°¨ì´ëŠ” ì—†ê² ì§€ë§Œ, ê·¸ë˜ë„ ì¢€ ë¹¨ë¼ì¡Œë‹¤. ~~<small>(ì‘ 26ì‹œê°„\~)</small>~~

### 5.4.3. Multithread using `ThreadPoolExecutor`.

```python
def fetch_url(url):
    """Fetch content from a URL with timeout handling."""
    try:
        loader   = WebBaseLoader(url)
        document = loader.load()[0]
     ...

from concurrent.futures import ThreadPoolExecutor, as_completed
def process_batch_parallel(batch):
    """Call fetch_url with multithreaded processes!"""
    inputs = []
    labels = []

    for idx in range(len(batch["question"])):
        question = batch['question'][idx]
        snippets = batch['snippets'][idx]
        documents = batch['documents'][idx]
        label = batch['labels'][idx]

        doc_urls = documents.split("\n")
        doc_contents = []

        # Parallel fetching of URLs.
        global num_workers
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            future_to_url = {executor.submit(fetch_url, url): url for url in doc_urls}
            for future in as_completed(future_to_url):
                doc_contents.append(future.result())
        
        retrieved_chunks = retrieve_from_each_doc(query=question, documents=doc_contents)
        retrieved_chunks = "\n".join(retrieved_chunks)
        
        ...
  ```

- `fetch_url`: ìœ„ì—ì„œ í–ˆë˜ requestë¥¼ multithreadë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ í•¨ìˆ˜ë¡œ ë¬¶ì—ˆë‹¤.
- `ThreadPoolExecutor(max_workers=num_workers)`: ì—¬ëŸ¬ threadë“¤ì´ requestë¥¼ í•˜ë‚˜ì”© ì§‘ì–´ ë‹´ë‹¹í•œë‹¤. 
- `as_completed`: ëª¨ë“  threadë“¤ì´ request ìˆ˜í–‰ì„ ì™„ë£Œí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ëª¨ì•„ì„œ append í•œë‹¤.
- Single process ë•Œ 26ì‹œê°„ì´ ê±¸ë ¸ë˜ ê²ƒì— ë¹„í•´ ëŒ€ëµ 6~7ì‹œê°„ìœ¼ë¡œ ë‹¨ì¶•í•  ìˆ˜ ìˆì—ˆë‹¤! ê·¸ëŸ¬ë‚˜ ì—¬ì „íˆ ì—¬ëŸ¬ ë°©ì‹ì„ í…ŒìŠ¤íŠ¸í•˜ê¸°ì—” ë„ˆë¬´ ê¸¸ì—ˆë‹¤.

> #### ğŸ’â€â™‚ï¸ URL Requestì—ì„œ Multi-threading ë°©ì‹ì˜ í•œê³„.
> - URL requestëŠ” ë‚´ ë¦¬ì†ŒìŠ¤ê°€ ì•„ë¬´ë¦¬ ë›°ì–´ë‚˜ë„ ì„œë²„ì— ë”°ë¼ ì‹¤í–‰ì‹œê°„ì´ ì°¨ì´ê°€ ë§ì´ ë‚œë‹¤.
> - Multi-threadingì€ ë™ì‹œì— ì—¬ëŸ¬ threadê°€ requestí•˜ì—¬ í•œ ë²ˆì— ë§ì€ requestê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, ê²°êµ­ í•œ thread ë‹¹ í•˜ë‚˜ì˜ requestë§Œ ê°€ëŠ¥í•˜ê³ , ë‹¤ë¥¸ ëª¨ë“  threadê°€ ìˆ˜í–‰ ì™„ë£Œí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•˜ë©°, ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ ë©”ëª¨ë¦¬ ë“± ë¦¬ì†ŒìŠ¤ë¥¼ ê°€ì§„ ì±„ë¡œ ìˆì–´ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.
> - ë°˜ë©´ ì•„ë˜ì˜ asyncì—ì„œ í•˜ë‚˜ì˜ threadëŠ” ë‹¤ë¥¸ ë…€ì„ë“¤ì´ë‚˜ requestì˜ responseë¥¼ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ê³„ì† requestë¥¼ ë‚ ë¦°ë‹¤. ì ê¹ë§Œ ìƒìƒí•´ë´ë„ ì–¼ë§ˆë‚˜ ë¹¨ë¼ì§ˆì§€ ê¸°ëŒ€ë˜ì§€ ì•ŠëŠ”ê°€? ~~<small>ì§ì ‘ ê²ªì–´ë³´ë©´ ë¶„ëª… ê¸°ëŒ€ë  ê²ƒì´ë‹¤</small>~~

### 5.4.4. `asyncio`.

```python
import aiohttp
import asyncio
from bs4 import BeautifulSoup

async def fetch_url_async(session, url):
    """Fetch content from a URL asynchronously."""
    try:
        async with session.get(url, timeout=10) as response:
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            abstract_div = soup.find('div', class_='abstract-content selected')
            
            if abstract_div:
                abstract_list = abstract_div.find_all('p')
                
                # if abstract is found, return with title.
                if abstract_list:   
                    abstract = " ".join(p.get_text(strip=True) for p in abstract_list)
                    return abstract
                else:
                    return f"abstract not found."
                
            else:
                return f"abstract_div not found."
            
    except asyncio.TimeoutError:
        return f"Timeout occurred: {url}"
    
    except Exception as e:
        return f"Failed to load URL: {str(e)}"

async def fetch_all_urls_async(doc_urls):
    """Fetch all URLs concurrently using aiohttp."""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url_async(session, url) for url in doc_urls]
        
        return await asyncio.gather(*tasks)
```

- ëŒ€ë§ì˜ ë¹„ë™ê¸°ì  request êµ¬í˜„ì´ë‹¤! ì‹¤í–‰ì‹œê°„ì´ ë¬´ë ¤ **80ë¶„**ìœ¼ë¡œ ì¤„ì—ˆë‹¤!
- `aiohttp.ClientSession()`: async requestë¥¼ ìœ„í•œ sessionì´ë‹¤. ê¸°ì¡´ì˜ ë™ê¸° ë°©ì‹ì¸ `ThreadPoolExecutor` ë° `WebBaseLoader`ë¥¼ ëŒ€ì²´í•œë‹¤.
- `BeautifulSoup`: ê¸°ì¡´ì˜ í•˜ë“œì½”ë”© ëŒ€ì‹  HTML parsingì„ ìœ„í•œ bs4ì´ë‹¤. ê¹”ë”í•˜ê²Œ abstractë¥¼ ì°¾ëŠ”ë‹¤.

> #### ğŸ’â€â™‚ï¸ Many Requests at Once.
> ë‹¨ì‹œê°„ì— ë„ˆë¬´ ë§ì€ requestë¥¼ ë‚ ë¦¬ë©´ ì„œë²„ì—ì„œ ë‹¹ì‹ ì„ ì°¨ë‹¨í•  ìˆ˜ ìˆë‹¤!
> ì´ëŠ” ê¸°ìˆ ì ì¸ ë¬¸ì œë¼ê¸°ë³´ë‹¨ í˜„ì‹¤ì ì¸ ë¬¸ì œ, ê·¸ë¦¬ê³  ì–‘ì‹¬ì˜ ë¬¸ì œë¡œ, ë³¸ê²©ì ìœ¼ë¡œ ë‚ ë¦¬ê¸° ì „ì— API ë¬¸ì„œë‚˜ í…ŒìŠ¤íŠ¸ë¥¼ ê¼­ í•´ë³´ì!

## 5.5. Retrieval Result.

![](https://velog.velcdn.com/images/aruwad/post/76e08807-50d6-4c7a-b70b-77b036e8e160/image.JPG)

- ì†Œì¤‘í•œ ë°ì´í„°ì´ë‹ˆë§Œí¼ ì €ì¥ì„ ìŠì§€ ë§ì!
- ìš”ì²­í•œ prompt ì–‘ì‹ìœ¼ë¡œ ì˜ ë§Œë“¤ì–´ ì¤€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

# 6. RAG Model Training.

## 6.1. Data Augmentation by Splits.

```python
def split_by_snippets_docs(ds, only_no):
    if only_no:
        if label == 0:
            # add with snippets.
            splitted_inputs.append(question + snippets + "\nAnswer:\n")
            splitted_labels.append(label)

            # add with docs.
            splitted_inputs.append(question + "\nSnippets:\n" + docs)
            splitted_labels.append(label)

        # if 'yes', just copy original sample.
        else:
            splitted_inputs.append(sample['input'])
            splitted_labels.append(label)
    else:
        # add with snippets.
        splitted_inputs.append(question + snippets + "\nAnswer:\n")
        splitted_labels.append(label)

        # add with docs.
        splitted_inputs.append(question + "\nSnippets:\n" + docs)
        splitted_labels.append(label)
```

- Uneven class distribution ë° lack of dataë¥¼ handlingí•˜ê¸° ìœ„í•´ data augmentationì„ ì‹œë„í–ˆë‹¤.
- ì›ë˜ì˜ 'question-snippets-documents' êµ¬ì¡°ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ë¶„í• í–ˆë‹¤.
  - **'q-s'** and **'q-d'**: snippetsê³¼ documentsë¥¼ ë‚˜ëˆ„ê¸° (2ê°œ).
  - **'q-s_1', 'q-s_2', ..., 'q-s_n'** and **'q-d'**: ê° snippet ë³„ë¡œ ëª¨ë‘ ë‚˜ëˆ„ê³ , ì¶”ê°€ë¡œ documents ì§¸ë¡œ ë‚˜ëˆ„ê¸° (n+1ê°œ).
  - **'q-s_1', 'q-s_2', ..., 'q-s_n'** and **'q-d_1', 'q-d_2', ..., 'q-d_m'**: ê° snippets ë° documents ë³„ë¡œ ëª¨ë‘ ë‚˜ëˆ„ê¸° (n x mê°œ).
  - Rare classì˜ ê²½ìš°ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ ë” ìì£¼ ë‚˜ëˆ„ê¸°.
  - (ê¸°íƒ€ ìˆ˜ë§ì€ ì‹œë„ë“¤...)
- 20ì‹œê°„ ê°€ëŸ‰ì˜ ìˆ˜ë§ì€ ì‹œë„ë“¤ ëì—, **'no' sampleë§Œ 'q-s' and 'q-d'ë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹**ì´ í•©ë¦¬ì ì¸ training time ë‚´ì— ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒˆë‹¤. 
~~<small>(ì‚´ë ¤ì£¼ì„¸ìš” Optunaë‹˜ ã… ã… )</small>~~

> #### ğŸ’â€â™‚ï¸ Data Augmentation and Shuffle.
> ì´ ë°©ì‹ì˜ Data Augmentation í›„ì—ëŠ” ê¼­ Datasetì„ ì„ì–´ì•¼ í•œë‹¤. **'q-s'**ì™€ **'q-d'**ëŠ” ë‹¹ì—°íˆ ë†’ì€ ìƒê´€ì„±ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

## 6.2. Stride.

```python
# Tokenization.
def tokenize_rag(sample):
    return tokenizer_rag(sample['input'], 
                         truncation      = True, 
                         padding         = 'max_length',
                         max_length      = hyperparams['max_length'],
#                        stride          = hyperparams['stride'],
#                        return_overflowing_tokens = True,
                         return_tensors  ='pt')

# Custom collate function to handle the shape after stride.
def collate_fn(batch):
    input_ids       = []
    attention_masks = []
    labels          = []

    for example in batch:
        input_ids.extend(example['input_ids'])                          # Extend all chunks.
        attention_masks.extend(example['attention_mask'])               # Extend attention masks.
        labels.extend([example['labels']])  # Extend labels.
#            labels.extend([example['labels']] * len(example['input_ids']))  # Duplicate labels, for stride.

    return {
        'input_ids'      : torch.tensor(input_ids),
        'attention_mask' : torch.tensor(attention_masks),
        'labels'         : torch.tensor(labels),
    }

```

- ì´ taskì²˜ëŸ¼ sequenceì˜ ê¸¸ì´ê°€ ë¶ˆê·œì¹™í•˜ê³  íˆ­í•˜ë©´ `max_length`ë¥¼ ë„˜ëŠ” ê²½ìš°, `stride`ê°€ ë„ë¦¬ ì‚¬ìš©ëœë‹¤.
- `stride`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° `input`ì˜ shapeì´ ë³€í•˜ë¯€ë¡œ, ì´ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•´ì¤˜ì•¼ í•œë‹¤. ì˜ì™¸ë¡œ ì´ê±¸ ì˜ ëª¨ë¥´ì‹œëŠ” ë¶„ë“¤ì´ ë§ì•„ ê°„ë‹¨íˆ ì„¤ëª…í•´ë³´ê² ë‹¤.
  
  
> #### ğŸ’â€â™‚ï¸ `tokenizer(stride)` í›„ shape ì²˜ë¦¬í•˜ê¸°.
> - ì˜ˆì»¨ëŒ€ `txt = [1,2,3,4,5,6], max_length=4, stride=2`ë¼ê³  í•˜ì.
>  - Tokenize ê²°ê³¼ëŠ” `token_1 = [1,2,3,4], token_2 = [3,4,5,6]`ì´ ëœë‹¤.
>  - ì´ì œ `input`ì˜ shapeëŠ” `(n_samples, 6)`ì—ì„œ `(n_samples, 4, 2)`ê°€ ëœë‹¤.
>  - ê·¼ë° ì—¬ê¸°ì„œ '2'ëŠ” txtì˜ ê¸¸ì´ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. ì˜ˆì»¨ëŒ€ `[1,2, ..., 8]`ì€ `[1,2,3,4], [3,4,5,6], [5,6,7,8]`ë¡œ '3'ì´ ëœë‹¤.
>  - ë”êµ°ë‹¤ë‚˜ `label`ì˜ shapeì€ ì•„ì§ `(n_samples, 1)`ë¡œ ë³€í•˜ì§€ë„ ì•Šì•˜ë‹¤.
> - ì´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„ ,
>   1. ê° tokenì„ êº¼ë‚´ì£¼ê³  ê°ê° labelì„ ë‹¬ì•„ì¤˜ì„œ ë³„ê°œì˜ sampleë¡œ ì·¨ê¸‰í•˜ë˜ê°€ (ì˜ˆì»¨ëŒ€ (4, 2)ë¥¼ (4,1)ì§œë¦¬ 2ê°œë¡œ),
>   2. 1ì²˜ëŸ¼ í•˜ë˜ ê° token ë³„ë¡œ logitsì„ ê³„ì‚°í•˜ê³  í‰ê·  ë“±ìœ¼ë¡œ ì·¨í•©í•´ sample ë³„ë¡œ í•˜ë‚˜ì˜ predictionì„ ë§Œë“¤ë˜ê°€ ~~<small>(ì˜ë  ì•™ìƒë¸”)</small>~~,
>   3. Longformerë‚˜ BigBirdì²˜ëŸ¼ ê¸´ sequence ì „ìš© transformerë¥¼ ì“°ë˜ê°€,
>   4. Summarization ë“± ë¬´ìŠ¨ ìˆ˜ë¥¼ ì¨ì„œë¼ë„ length ìì²´ë¥¼ ì¤„ì´ë˜ê°€, ë“±ë“± ë§ì€ ë°©ë²•ì´ ìˆë‹¤.
> - ë‚˜ëŠ” **ìˆ˜ë§ì€** ì‹œë„ ëì—, `stride`ë¥¼ í•˜ì§€ ì•Šê³ , ì•ì„œ ì–¸ê¸‰í•œ data augmentation ë°©ë²•ìœ¼ë¡œ lengthë¥¼ ì¤„ì´ê³ (ë°©ë²• 4), logitsì˜ í‰ê·  ê³„ì‚°(ë°©ë²• 2)ì„ ê²°í•©í•˜ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤.

## 6.3. Hyperparameter Tuning.

```python
# Hyperparameters to tune.
hyperparams = {
    'n_epochs': 30,
    'batch_size': trial.suggest_categorical('batch_size', [8, 16]),
    'max_length': 512,
    'weight_decay': trial.suggest_float('weight_decay', 2e-2, 3e-2),
#    'stride': trial.suggest_categorical('stride', [128, 256]),
    'warmup_ratio': trial.suggest_float('warmup_ratio', 0.08, 0.12),     # warmup_steps = total_steps * warmup_ratio.
    'learning_rate': trial.suggest_float('learning_rate', 5e-5, 5e-4),
    'lora_dropout': trial.suggest_float('lora_dropout', 0.1, 0.2),
    'class_weights_penalty': trial.suggest_float('class_weights_penalty', 1.2, 2.0),     # multiply class weights of less frequent class, i.e. 'no'.
    'early_stopping_patience': 3,
    'early_stopping_threshold': 1e-3,
    'max_grad_norm': 1.0,
#    'gradient_accumulation_steps': trial.suggest_categorical('gradient_accumulation_steps', [0, 1]),
#    'lr_scheduler_type': trial.suggest_categorical(
#        'lr_scheduler_type', ['linear', 'cosine', 'cosine_with_restarts']
#    )
}

checkpoint = trial.suggest_categorical(
    "checkpoint",
    [
        "bert-base-uncased",
#            "distilbert-base-uncased",
        "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",
        "dmis-lab/biobert-base-cased-v1.2",
        "kamalkraj/bioelectra-base-discriminator-pubmed",
    ]
)

```



- ì‚¬ìš©í•œ hyperparameterì˜ ê°’ ë° suggestionì´ë‹¤. ë²”ìœ„ë¥¼ ì¤„ì´ê³  ì¤„ì—¬ì„œ ë§ˆì§€ë§‰ ë¬´ë µì— studyí•œ ê²ƒë“¤ì´ë‹¤.
- `n_epochs: 30`: ë§ì€ ê²½ìš°ì²˜ëŸ¼ í° epochsë¥¼ ë‘ê³  `earlystopping`ìœ¼ë¡œ ì¡°ì ˆí–ˆë‹¤.
- `batch_size`: ë¬¼ë¡  16ì´ ë” ë¹¨ëì§€ë§Œ, small datasetê³¼ class imbalance, domain specific task ë“±ì„ ê³ ë ¤í•´ overfittingì— ëŒ€ì‘í•˜ê¸° ìœ„í•´ ë§ì€ ê²½ìš° 8ì´ ì„ í˜¸ë˜ì—ˆë‹¤. íŠ¹íˆë‚˜ long sequenceê°€ ë§ì€ task íŠ¹ì„±ìƒ 16ë§Œ í•´ë„ ìì£¼ ì»¤ë„ì´ ì£½ì–´ì„œ ë°˜ê°•ì œë¡œ 8ì„ ì„ í˜¸í•˜ê²Œ ë˜ì—ˆë‹¤.
- `max_length: 512`: `stride`ë¥¼ ì ìš©í•˜ê³  128, 256, 512ë¥¼ ì—¬ëŸ¬ ì¡°í•©ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•´ë´¤ë‹¤. ê·¸ì¹˜ë§Œ ê²°ë¡ ì€ í—ˆë¬´í•˜ê²Œë„ `stride` ë¹¼ê³  512 ê³ ì •ì´ë‹¤.
- `weight_decay`: Overfitting ê°€ëŠ¥ì„±ì´ ë†í›„í•œ task íŠ¹ì„±ìƒ ì•½ê°„ ë†’ê²Œ ì‹œì‘í•˜ì—¬ ë‚®ì¶°ë‚˜ê°”ë‹¤.
- `warmup_ratio`: Overfitting ê°€ëŠ¥ì„±ì´ ë†í›„í•œ task íŠ¹ì„±ìƒ (ì´í•˜ ë™ì¼ ã…ã…) í•˜ì§€ ì•Šì„ ìˆ˜ ì—†ì—ˆë‹¤. íŠ¹íˆ ì´ taskì—ì„  **ë†’ì€ learning rate**ì˜ catastrophic forgetting ë“±ì˜ ë¶ˆì•ˆì •ì„±ì„ ë‚®ì¶°ì¤Œìœ¼ë¡œì¨ ì¢‹ì€ ì‹œë„ˆì§€ë¥¼ ë³´ì˜€ë‹¤.
- `learning_rate`: ì´ taskì—ì„  domain specificí•œ dataë“¤ì„ aggressiveí•˜ê²Œ í•™ìŠµí•´ì•¼ í•œë‹¤ê³  íŒë‹¨í•´ì„œ í†µìƒì ì¸ (1e-5, 5e-5) ìˆ˜ì¤€ë³´ë‹¤ ë†’ê²Œ ë‘ê³  overfittingì„ handlingí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í–ˆë‹¤. ~~<small>(ë³¸ê²© ë³‘ì£¼ê³  ì•½ì£¼ê¸°)</small>~~
- `lora_dropout`: ì—­ì‹œ í†µìƒ ìˆ˜ì¤€ì¸ 0.1ë³´ë‹¤ ì•½ê°„ ë†’ê²Œ ë‘ì—ˆë‹¤. ì²˜ìŒì—” task-specific learningì´ë¼ëŠ” íŠ¹ì§•ì„ í™œìš©í•´ë³´ê³ ì ë‚®ì¶”ê³  ë‹¤ì–‘í•˜ê²Œ adapterë¥¼ ì¶”ê°€í•´ë³´ì•˜ëŠ”ë°, ì˜ ì‘ë™í•˜ì§€ ì•Šì•„ ê·¸ëƒ¥ ëºë‹¤. 
- `class_weights_penalty`: Unfrequent classì— ë”ìš± penaltyë¥¼ ì£¼ì—ˆë‹¤. Data augmentationì„ í•˜ì§€ ì•Šì€ ê²½ìš° 1.5ëŠ” ì¤˜ì•¼ ê²¨ìš° ê·¹ì‹¬í•œ 'yes' í¸í–¥ì„ ë²—ì–´ë‚˜ëŠ” ê²ƒ ê°™ì•˜ë‹¤. 'yes'ë¡œ ë‹¤ ì°ì–´ë„ 70%ê°€ ë³´ì¥ëœë‹¤ëŠ”ê²Œ ëª¨ë¸ë“¤ì—ê²Œ ì°¸ ìœ„ì•ˆì´ ë˜ì—ˆë‚˜ë³´ë‹¤ ^^.
- `early_stopping_patient` ë° `threshold`: í•œ epochì— 30ë¶„ ì´ìƒì´ ê±¸ë ¤ì„œ ì°¸ì„ ìˆ˜ê°€ ì—†ì—ˆë‹¤...
- `max_grad_norm`: ë†’ì€ learning rateë¡œ ì¸í•œ gradient explosionì„ ì œì–´í•˜ê¸° ìœ„í•´ clippingì„ ì¶”ê°€í•˜ì˜€ë‹¤.
- `gradient_accumulation_steps`: í° `batch_size`ê°€ ë¶ˆê°€ëŠ¥í•´ ì‹œë„í•´ë³´ì•˜ì§€ë§Œ, ì—­ì‹œ ê³¼ë„í•œ overfittingìœ¼ë¡œ ë„ëŠ”ê²Œ ë‚˜ì•˜ë‹¤.
- `lr_scheduler_type`: ë§ì´ ê³ ë¯¼í–ˆë˜ ë¶€ë¶„ì´ë‚˜, í˜„ì‹¤ì ìœ¼ë¡œ ì¥ê¸°ê°„ trainingì´ ì–´ë ¤ì› ê³  ê¸°ê»í•´ì•¼ 20 epochs ì •ë„ì—¬ì„œ ê·¸ëƒ¥ `linear`ë¡œ ê°”ë‹¤.

## 6.4. Ensemble.

```python
# Soft voting.
# Perform ensemble (mean of logits)
ensemble_logits = np.mean(logits_list, axis=0)

# Convert logits to predictions (soft voting)
preds_soft = np.argmax(ensemble_logits, axis=1)

# Convert preds_soft_onehot to one-hot encoding.
preds_soft_onehot = np.zeros((len(preds_soft), 2))
preds_soft_onehot[np.arange(len(preds_soft)), preds_soft] = 1


# Hard voting.
preds_hard = np.stack(preds_list, axis=0)
preds_hard = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=preds_hard)

# Convert final_predictions_hard to one-hot encoding.
preds_hard_onehot = np.zeros((len(preds_hard), 2))
preds_hard_onehot[np.arange(len(preds_hard)), preds_hard] = 1

# Compute metrics.
test_results_soft = bioasq_libs.compute_metrics((preds_soft_onehot, test_ds['labels']))
test_results_hard = bioasq_libs.compute_metrics((preds_hard_onehot, test_ds['labels']))
```

- Classification ëŒ€íšŒì˜ êµ­ë£°ì¸ Ensembleë„ ì‹œë„í•´ë³´ì•˜ë‹¤. ì „í˜•ì ì¸ soft/hard votingë¶€í„° ì‹œì‘í–ˆë‹¤.
- í˜„ì‹¤ì ì¸ ì‹¤í–‰ ì‹œê°„ì„ ê³ ë ¤í•´ training ì‹œ ê³„ì‚°í•œ predictionì„ ì €ì¥í–ˆë‹¤ ì´í›„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤.
- ì• ì¨ êµ¬í˜„í–ˆì§€ë§Œ ëŒë¦¬ìë§ˆì ê·¹ì‹¬í•˜ê²Œ ëŠ˜ì–´ë‚˜ëŠ” ì‹¤í–‰ ì‹œê°„ì— í™©ê¸‰íˆ ì‚­ì œí•´ë²„ë ¸ë‹¤ ^^.

## 6.5. Results.

### 6.5.1. Metrics.

![](https://velog.velcdn.com/images/aruwad/post/2428796a-5c43-4c2b-b23e-e8ef44848248/image.JPG)

![](https://velog.velcdn.com/images/aruwad/post/f37f38d4-1238-40cb-b3df-a465544fb4db/image.JPG)


- ê²°ê³¼ì ìœ¼ë¡œ best_paramsì—ì„œ 0.9541ì˜ macro f1ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.
- ì „í˜•ì ì¸ Overfittingê³¼ì˜ ì „ìŸì´ì—ˆê³ , ìœ„ì— ì–¸ê¸‰í•œ ì—¬ëŸ¬ ë°©ë²•ë“¤ë¡œ ì¤„ì—¬ë‚˜ê°„ ê²°ê³¼ì´ë‹¤.
- 3ì¼ ë°¤ë‚®ì„ ëˆˆì´ ë¹ ì ¸ë¼ ëŒë¦¬ë©° ì–»ì€ ê²°ê³¼ë‹¤. ì—¬ëŸ¬ì°¨ë¡€ ì—°ì´ì–´ training í–ˆê¸° ë•Œë¬¸ì— 1 epochë¶€í„° ê²°ê³¼ê°€ ì¢‹ì•„ë³´ì´ì§€ë§Œ, ëŒ€ëµ 1~20 epoch ì •ë„ ì•ì— ì¶”ê°€í•´ì„œ ìƒìƒí•˜ë©´ ëœë‹¤. ~~<small>ë³´ê³ ìˆìë‹ˆ ì™ ì§€ í—ˆë¬´í•˜ë‹¤...</small>~~
- ê·¸ ë†ˆì˜ ë©”ëª¨ë¦¬ ë•Œë¬¸ì— ìê¾¸ ì»¤ë„ì´ ì£½ì–´ì„œ TensorBoard ê²°ê³¼ë„ ë‚ ë ¤ë¨¹ì—ˆë‹¤. ì „ì²´ ê·¸ë˜í”„ ë³´ì—¬ë“œë¦¬ì§€ ëª»í•œ ì  ì‹¬ì‹¬í•œ ì‚¬ê³¼ì˜ ë§ì”€...

### 6.5.2. Feature Importances.

![](https://velog.velcdn.com/images/aruwad/post/eb8be2e3-3293-45ec-b369-0b225eb9e74d/image.JPG)

- ì‚¬ì‹¤ ì²« studyì˜ ê²°ê³¼ëŠ” í™•ì‹¤íˆ ë‹¬ëë‹¤. Learning rateì™€ weight_decayì´ í›¨ì”¬ ë†’ì•˜ëŠ”ë°, ìœ„ ê·¸ë¦¼ì€ ë§‰ë°”ì§€ studyì—ì„œ tuningí•  ëŒ€ë¡œ í•œ ì´í›„ë¼ ì˜í–¥ì´ í¬ì§€ ì•Šì•„ ë³´ì¸ë‹¤.
- ë‹¹ì—°í•˜ê²Œë„ ëª¨ë¸ì˜ ë¹„ì¤‘ì´ ë§¤ìš° ë†’ë‹¤. ë¹„ìŠ·í•œ BERT ê³„ì—´ì˜ bio-specificí•œ ë†ˆë“¤ + Bert-base-uncased í•˜ë‚˜ ë‚¨ê²¼ëŠ”ë°ë„ ì°¨ì´ê°€ ê½¤ ì»¸ë‹¤.
- Dropoutì€ ë­... ì–´ë–¤ taskì—ì„œë„ í•­ìƒ ë†’ê³  ë”±íˆ ëº„ ìˆ˜ë„ ì—†ë‹¤.
- Warmupì˜ ê²½ìš° ìœ„ì— ì ê¹ ì–¸ê¸‰í–ˆì§€ë§Œ, í™•ì‹¤íˆ ë†’ì€ learning rateì˜ ê²½ìš° ì¤‘ìš”ë„ê°€ ì¦ê°€í•˜ëŠ” ê²½í•­ì„ ë³´ì˜€ë‹¤.

# 7. ê²°ë¡  ë° ì•„ì‰¬ìš´ ì .
- ë³¸ í”„ë¡œì íŠ¸ëŠ” domain specific taskë¡œ ì•…ëª… ë†’ì€ medical NLP ë¶„ì•¼ì˜ ìœ ëª…í•œ competitionì— ë„ì „í•´ë³´ì•˜ë‹¤.
- Industryì˜ small project ìˆ˜ì¤€ì˜ ì ë‹¹í•œ ê¹Šì´ë¡œ, RAGë¥¼ êµ¬ì¶•í•˜ê³  fine-tuningí•˜ì—¬ ì ë‹¹í•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ì˜€ë‹¤. ì´ë¡œì„œ RAGì™€ pretrained LLMì˜ fine-tuningì˜ ê¸°ì´ˆëŠ” í™•ì‹¤í•˜ê²Œ ë‹¤ì§„ ê²ƒ ê°™ë‹¤.
- ì‹œë„í•´ë³´ê³  ì‹¶ì€ ê²ƒë“¤ì´ ì •ë§ ë§ì•˜ì§€ë§Œ, ì´ë¯¸ ì¢…ë£Œëœ ëŒ€íšŒë¼ëŠ” ì , ê·¸ë¦¬ê³  ì§€ê¸ˆ ë‚˜ì˜ ìƒí™©ì´ portfolioë¥¼ ìœ„í•´ ë‹¨ê¸°ê°„ì— ë‹¤ì–‘í•œ taskì—ì„œ ë‹¤ì–‘í•œ tech stackì— ëŒ€í•œ ìˆ™ë ¨ë„ë¥¼ ë³´ì—¬ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ì ì—ì„œ, ì•„ì‰½ì§€ë§Œ ë” íŒŒê³ ë“¤ì§€ ëª»í•˜ê³  ë§ˆë¬´ë¦¬í–ˆë‹¤.
- ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê³  í¬ìŠ¤íŒ…í•˜ëŠ” ê²ƒì´ ìƒê°ë³´ë‹¤ ì—„ì²­ë‚˜ê²Œ ì‹œê°„ê³¼ ë…¸ë ¥ì´ ë§ì´ ë“ ë‹¤. TensorBoard ë“± visualization toolì„ ì ê·¹ í™œìš©í•˜ê³ , ë¸”ë¡œê·¸ë¥¼ ê°„ê²°í•˜ê²Œ ì“°ëŠ” ì—°ìŠµì„ ê³„ì† í•´ì•¼ê² ë‹¤.
