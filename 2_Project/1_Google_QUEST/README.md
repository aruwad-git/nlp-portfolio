# 1. Overview.

## Highlights.
- **Open competition** on Kaggle, held by Google.
- **Rank**: Spearman's Correlation = 0.3813, 127 / 1,572 (top 8%; post-competition).
- Skip Connection: Designed metadata features (question category, host website) as a skip connection, bypassing the transformer and directly connecting to the head to preserve information and enhance performance.
- Stage-wise Fine-tuning: Repeated combinations of full training and head-only training, configuring different learning rates and schedulers for each stage to enhance robustness against overfitting and improve performance.


---

## About Project.

| **Name**               | **Description**                                                                                     |
|:------------------------|:----------------------------------------------------------------------------------------------------|
| **Name**               | Google QUEST Q&A Labeling.                                                                          |
| **Period**             | 2025.1.1 ~ 1.2. (2 days).                                                                       |
| **Category**           | `Text Classification`<br>`Multi-label Classification`<br>`Question-Answering`                       |
| **Tags**               | `NLP`, `LLM`, `Text Classification`<br>`QA`, `Kaggle`, `Competition`                        |
| **Blog**               | [Velog]().                                                                                         |
| **GitHub Repository**  | [GitHub]().                                                                                        |



## About Competition.

| **Name**                   | **Description**                                                                                     |
|:----------------------------|:----------------------------------------------------------------------------------------------------|
| **Name**                   | Google QUEST Q&A Labeling.      |
| **Host**                   | Google.                                                                                            |
| **Period**                 | 2019.11.23 ~ 2020.2.11 (80 days).                                                                  |
| **Prizes**                 | $25,000.                                                                                           |
| **Participation**          | `10,582 Entrants`<br>`1,904 Participants`<br>`1,571 Teams`<br>`27,817 Submissions`                  |
| **Notebook Requirements**  | `CPU Notebooks â‰¤ 9 hours run-time`<br>`GPU Notebooks â‰¤ 2 hours run-time`<br>`Internet must be turned off` |
| **Citation**               | Danicky, Praveen Paritosh, Walter Reade, Addison Howard, and Mark McDonald. Google QUEST Q&A Labeling.<br>Kaggle, https://kaggle.com/competitions/google-quest-challenge, 2019. |


## Feature List.

| **Category**       | **Features**             |
|:--------------------|:-------------------------|
| **Metadata**        | `qa_id`                 |
|                     | `question_user_name`    |
|                     | `question_user_page`    |
|                     | `answer_user_name`      |
|                     | `answer_user_page`      |
|                     | `url`                   |
|                     | `category`              |
|                     | `host`                  |
| **Question**        | `question_title`        |
|                     | `question_body`         |
| **Answer**          | `answer`                |

## Label List.

| **Category**       | **Output Labels**                                                                                   |
|:--------------------|:----------------------------------------------------------------------------------------------------|
| **Question Labels** | `question_asker_intent_understanding`<br>`question_body_critical`<br>`question_conversational`<br>`question_expect_short_answer`<br>`question_fact_seeking`<br>`question_has_commonly_accepted_answer`<br>`question_interestingness_others`<br>`question_interestingness_self`<br>`question_multi_intent`<br>`question_not_really_a_question`<br>`question_opinion_seeking`<br>`question_type_choice`<br>`question_type_compare`<br>`question_type_consequence`<br>`question_type_definition`<br>`question_type_entity`<br>`question_type_instructions`<br>`question_type_procedure`<br>`question_type_reason_explanation`<br>`question_type_spelling`<br>`question_well_written` |
| **Answer Labels**   | `answer_helpful`<br>`answer_level_of_information`<br>`answer_plausible`<br>`answer_relevance`<br>`answer_satisfaction`<br>`answer_type_instructions`<br>`answer_type_procedure`<br>`answer_type_reason_explanation`<br>`answer_well_written` |


- Code: [here](https://github.com/aruwad-git/nlp-portfolio/tree/main/2_Project/1_Google_QUEST)
# 1. Introduction.

## Overview.
- ì´ ê¸€ì€ Googleì´ Kaggleì—ì„œ ì£¼ìµœí•œ **["Google QUEST Q&A Labeling Competition"](https://www.kaggle.com/competitions/google-quest-challenge)**ì— ë„ì „í•œ ê²°ê³¼ë¥¼ ì •ë¦¬í•œë‹¤.
- Q&A ì…ë ¥ì— ëŒ€í•´ ë‹¤ì–‘í•œ Labelì„ ì˜ˆì¸¡í•˜ëŠ”, ì „í˜•ì ì¸ **Multi-label Text Classification** ë¬¸ì œë‹¤.
- í†µìƒì ì¸ Pretrained LLM with Fine-Tuning ê¸°ë°˜ ë‹¤ì–‘í•œ ê¸°ë²• ì‚¬ìš©, **127/1,572 (top 8%)** ë‹¬ì„±! 
  - Custom model with skip connections.
  - Stage-wise Fine-Tuning, Alternating training between Full and Head-Only, etc.
  - One, Complex Head ë“±.
- ëŒ€íšŒ ì¢…ë£Œ í›„ ìˆ˜í–‰, ìˆœìœ„ëŠ” [Leaderboard](https://www.kaggle.com/competitions/google-quest-challenge/leaderboard?)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°.
- í”„ë¡œì íŠ¸ëŠ” ì‹¤ì œ ë¬¸ì œë¥¼ í†µí•´ LLM Fine-Tuning Workflowì— ëŒ€í•œ ëª…í™•í•œ ì´í•´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ìˆ˜í–‰ $\rightarrow$ ì‹¤ì œ ëŒ€íšŒë¥¼ ìœ„í•œ Ensemble, Optuna, QLoRA ë“±ì€ ìƒëµ. 
- **Tech Stacks**: ê·¸ëƒ¥ì €ëƒ¥ í†µìƒì ì¸ NLP libraryë“¤. PyTorch, Transformer, BERT, bitsandbytes, scikit-learn ë“±.

---

## What is 'Google QUEST Q&A Labeling'?

- **Description:**  
  - This project aims to predict multiple attributes of question-answer pairs, such as clarity, relevance, and helpfulness, by analyzing their textual content. 
  - This task is challenging due to the need for multi-label predictions that capture nuanced semantic and contextual information. 
  - The dataset reflects real-world scenarios with varied and unstructured inputs, making it highly relevant to applications in search engines, chatbots, and Q&A platforms.
  - Successfully solving this problem demonstrates the ability to handle complex NLP challenges.

- **Task Type:**
  - Text Classification.
  - Multi-label Classification.
  - Supervised Learning.

- **Input:**
  - QA datasets corrected by CrowdSource team at Google Research.
  - Gathered from nearly 70 different websites.
  - Main part consists of **Question Title**, **Question Body**, and **Answer Text**.
  - Full list is summarized below.
  - Example:
    - Question Title: "Best way to learn Python for data science?"
    - Question Body: "I want to learn Python for data science. Should I start with courses or books?"
    - Answer Text: "Start with courses on Coursera or Udemy; they're hands-on. Books like 'Python for Data Analysis' are also helpful."

- **Output:**
  - Target values of **30 labels** for each QA pair.
  - Full list is summarized below.
  - Each label has range of [0, 1].
  - Example:
    - "question_asker_intent_understanding": 0.92
    - And other 29 labels.

- **Evaluation:**
  - Mean column-wise Spearman's correlation coefficient.
  - $$\text{Score}(y_{\text{pred}}, y_{\text{test}}) = \frac{1}{m} \sum_{j=1}^{m} \rho_j$$
    - $$y_{\text{pred}}$$: Predicted values for all samples across $$m$$ labels.
    - $$y_{\text{test}}$$: Ground truth values for all samples across $$m$$ labels.
    - $$m$$: Number of labels (columns).
    - $$\rho_j$$: Spearman's rank correlation coefficient for the $$j$$-th label.
  - $$\rho_j = 1 - \frac{6 \sum_{i=1}^n (r_{ij} - \hat{r}_{ij})^2}{n (n^2 - 1)}$$
    - $$n$$: Number of samples.
    - $$r_{ij} = \text{Rank}(y_{\text{test}, ij})$$
    - $$\hat{r}_{ij} = \text{Rank}(y_{\text{pred}, ij})$$

> ### ìŒ ê·¸ëƒ¥ í‰ë²”í•œ Text Classification ë¬¸ì œ ì•„ë‹˜?  

- **ì•„ë‹ˆë‹¤.** ì´ ë¬¸ì œê°€ ì–´ë ¤ìš´ ëª‡ ê°€ì§€ ì´ìœ ê°€ ìˆë‹¤.
- ë¬´ë ¤ **30ê°œì˜ ë¼ë²¨**ì— ëŒ€í•œ, ê° scoreë¥¼ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤. ì‚¬ì‹¤ìƒ **Regression** ë¬¸ì œë‹¤.
- ë¿ë§Œ ì•„ë‹ˆë¼, ë¼ë²¨ ì¤‘ ìƒë‹¹ìˆ˜ëŠ” **ì£¼ê´€ì **ì´ë©°, **ì£¼ì–´ì§„ Contextë¡œë¶€í„° ì¶”ë¡ í•˜ê¸° ì–´ë µë‹¤.**
  - ì˜ˆì»¨ëŒ€, `'answer_plausible'` ë¼ë²¨ì€ í‰ê°€ìì˜ ì£¼ê´€ì ì¸ í‰ê°€ë¥¼ ì˜ë¯¸í•˜ë©° ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•  ìˆ˜ ì—†ë‹¤.
  - ë˜í•œ ëª¨ë¸ì€ ì˜¤ì§ ì§ˆë¬¸-ë‹µë³€ ë°ì´í„°ë§Œ ë³´ê³  ì¶”ë¡ í•´ì•¼ í•˜ë¯€ë¡œ(ì¦‰ ì§ˆë¬¸ìì˜ í›„ê¸° ë¹„ìŠ·í•œ ê²ƒë„ ì—†ìŒ), ì£¼ì–´ì§„ Context ë°–ì—ì„œ ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬´ì–¸ê°€ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤.
  - ì¦‰, ëª¨ë¸ì€ ì‚¬ëŒì˜ ì£¼ê´€ì ì¸ ê°ì •ì„ í‘œìƒí•˜ê³  ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” **ì¼ë°˜í™”ëœ ê·œì¹™**ì„ í•™ìŠµí•´ì•¼ í•œë‹¤! (ì˜ˆì»¨ëŒ€ ê·œì¹™ : 'ì¶œì²˜'ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì€ ì‚¬ëŒì„ 'ë§Œì¡±'ì‹œí‚¨ë‹¤, ì¶”ë¡  : ì´ ë‹µë³€ì˜ ì´ ë¶€ë¶„ì€ 'ì¶œì²˜'ë¥¼ ì œê³µí•˜ë¯€ë¡œ, ì´ ë‹µë³€ì€ ì‚¬ëŒì„ 'ë§Œì¡±'ì‹œí‚¬ ê²ƒì´ë‹¤)
  - NLP ì¢€ ì¹˜ëŠ” ë¶„ë“¤ì€ ëˆˆì¹˜ì±„ì…¨ê² ì§€ë§Œ, ì´ëŠ” **Attention ê¸°ë°˜ Transformerë“¤ì´ ì˜í•˜ì§€ ëª»í•˜ëŠ” ì˜ì—­**ì´ë‹¤.
- ë§ˆì§€ë§‰ìœ¼ë¡œ, **ì§ˆë¬¸ì**ì™€ **í‰ê°€ì**ê°€ ë‹¤ë¥´ë‹¤! ê·¸ëŸ¬ë‹ˆê¹Œ ì•„ë¬´ ìƒê´€ ì—†ëŠ” ì œ 3ìê°€ ì§ˆë¬¸ìì˜ ì˜ë„, ë§Œì¡±ë„ ë“±ì„ ì¶”ë¡ í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.
- ì´ì— ëŒ€í•œ ë°°ë ¤ë¼ë„ í•˜ëŠ” ê±´ì§€, Evaluationì´ ì •í™•í•œ ê°’ì´ ì•„ë‹Œ **rankì— ëŒ€í•œ ì •í™•ë„ (Spearsman)**ë¡œ ìˆ˜í–‰ë˜ì—ˆë‹¤.

> ### ê·¸ë˜ì„œ ì–´ë–»ê²Œ í• ê±´ë°?

- ì¼ë‹¨ Competitionì€ ë‚´ê°€ í•œì°½ í•˜ë˜ ì˜ˆì „ì´ë‚˜ ì§€ê¸ˆì´ë‚˜ ëŒ€ë¶€ë¶„ ~~ë‹¥ì¹˜ê³ ~~ **Ensemble**ì´ë‹¤. ë‹¤ë“¤ ë¹„ìŠ·í•œ Pretrained LLMì„ ì‚¬ìš©í•˜ëŠ” NLPì—ì„  ë”ìš± ê·¸ë ‡ë‹¤. [Leaderboard](https://www.kaggle.com/competitions/google-quest-challenge/leaderboard)ì—ì„œ ì‹¤ì œ ìš°ìŠ¹ íŒ€ë“¤ì˜ í›„ê¸°ë¥¼ ë³´ë©´ ê±°ì˜ ë‹¤ ì‚¬ìš©í–ˆë‹¤.
- ë˜í•œ Competitionì— ì§„ì§œ ì§„ì‹¬ì´ë¼ë©´ ~~ë‹¥ì¹˜ê³ ~~ **Feature Engineering** ë…¸ê°€ë‹¤ë¥¼ í•˜ëŠ”ê²Œ êµ­ë£°ì´ë‹¤. Feature scaling ì´ëŸ°ê±° ì–˜ê¸°í•˜ëŠ”ê²Œ ì•„ë‹ˆë‹¤. ê·¸ ëŒ€íšŒì˜ **Task-specific**í•œ featureë“¤ì´ë‹¤ (ì˜ˆì»¨ëŒ€ Stackoverflowì˜ ë‹µë³€ë“¤ì„ ê¸ì–´ì™€ì„œ **ì§ì ‘** ëŠê»´ë³´ê³  Postprocessingì„ í•œë‹¤ë˜ì§€ ~~ì§ì ‘ ì•™ìƒë¸”ì— ë“¤ì–´ê°€ ì¸ê°„ voterê°€ ë˜ì–´ë³´ì!~~). ë‚˜ë„ ëª‡ëª‡ ê½¤ ì¢‹ì€ ê²ƒë“¤ì„ ë°œê²¬í–ˆì§€ë§Œ, ëë‚œ ëŒ€íšŒì— ë…¸ê°€ë‹¤ë¥¼ í•˜ê¸´ ì¢€ ê·¸ë˜ì„œ ê´€ë’€ë‹¤.
- ì „í†µì ì¸ Hyperparameter Tuningë§Œìœ¼ë¡  ì–´ë ¤ìš¸ ê²ƒ ê°™ê³ , Context ì™¸ì˜ Featureë“¤ì„ ì–´ë–¤ êµ¬ì¡°ë¡œ ëª¨ë¸ë§í•˜ëƒê°€ í•µì‹¬ì¼ ê²ƒ ê°™ë‹¤.
- ë‹¹ì—°í•œ ê·€ê²°ì´ì§€ë§Œ, Body (Transformer) ì•ˆì—ì„œ ëª»í•˜ë©´ Head (Dense)ì—ì„œ í•˜ë©´ ëœë‹¤. ëª‡ëª‡ [ìƒìœ„ê¶Œ ì°¸ê°€ì](https://www.kaggle.com/competitions/google-quest-challenge/discussion/129927)ë“¤ì€ Denseë¥¼ ëª‡ ê°œ ìŒ“ê³  íŠ¹ì • featureë§Œ ì·¨í•˜ëŠ” ë“±ì˜ Custom Headë¥¼ ì‚¬ìš©í–ˆê³ , ê·¸ëƒ¥ Default Headë¡œ ë°€ì–´ë¶™ì¸ ì‚¬ëŒì€ ê±°ì˜ ì—†ì—ˆë‹¤.
- ë‚˜ëŠ” Non-linguistic featureë¥¼ Transformer ë°–ìœ¼ë¡œ bypassí•˜ëŠ” ë°©ì‹(ê´€ì‹¬ ìˆìœ¼ì‹œë©´ [ì´ ë…¼ë¬¸](https://arxiv.org/abs/1606.07792?utm_source=chatgpt.com) ì°¸ì¡°), Complex Head ë° Iterative training ë“±ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. (ìì„¸í•œ ê±´ ì•„ë˜ ã„±ã„±)
- HeadëŠ” ë¼ë²¨ ë³„ êµ¬ì„±ì´ ì•„ë‹Œ, í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ Denseë¡œ ì…ë ¥ë°›ì•˜ë‹¤. í•´ë‹¹ ë¬¸ì œëŠ” ê° ë¼ë²¨ì˜ ê°’ì´ ì•„ë‹Œ ë¼ë²¨ ê°„ ìˆœìœ„ë¥¼ ë§ì¶”ëŠ” ê²ƒì´ë¯€ë¡œ, ë¼ë²¨ ê°„ ìƒëŒ€ì ì¸ ìˆ˜ì¹˜ ì •ë³´ë¥¼ ìƒëŠ” ê²ƒì€ ë„ˆë¬´ í° ì†í•´ë¼ê³  ìƒê°í–ˆë‹¤.
- 30ê°œ ë¼ë²¨ì˜ ìˆœì„œë¼ëŠ”, êµ‰ì¥íˆ Task-specificí•œ íƒ€ê²Ÿì„ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ, Adapter ë°©ì‹ì˜ PEFT (e.g. LoRA)ê°€ ì í•©í•  ê²ƒ ê°™ë‹¤. ì´ë²ˆ í”„ë¡œì íŠ¸ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ ìƒëµí–ˆë‹¤. <small>~~ì•„ë¬´íŠ¼ ì í•©í• ê±°ë‹¤~~</small>
- ê·¸ ë°–ì˜ Tokenizationì´ë‚˜ Pretraining Model ì„ íƒ(ë‹¹ì—°íˆ BERT-like encoder-only ëª¨ë¸ë“¤) ë“±ì€ ë‹¤ë“¤ ê±°ê¸°ì„œ ê±°ê¸°ì¸ ë“¯ í•˜ë‹¤. Data augmentation ë“±ì„ ì‹œë„í•œ ê¸€ë“¤ì€ ë§ì•˜ìœ¼ë‚˜ ëŒ€ë¶€ë¶„ ê·¸ë‹¥ì¸ ê²ƒ ê°™ë‹¤.

---

# 2. Preprocessing.

## 2.1. Data Load.

```python
# Load.
from sklearn.model_selection import train_test_split
data                = pd.read_csv('./dataset/train.csv')
train_set, test_set = train_test_split(data, test_size=0.2)

label_col_idx = train_set.columns.get_loc('question_asker_intent_understanding')  # Label Columns start from it.
x_train       = train_set.iloc[:, :label_col_idx]
y_train       = train_set.iloc[:, label_col_idx:]
x_test        = test_set.iloc[:, :label_col_idx]
y_test        = test_set.iloc[:, label_col_idx:]

# Null Check.
is_null = data.isnull().values.any()
print(f'Any null values? {is_null}')

# Copy train_set for EDA. (Only if train_set < 2 GB)
train_set_size = train_set.memory_usage(deep=True).sum() / (1024 ** 3)  # In GB.

if train_set_size < 2:               
    train_cp   = train_set.copy()
    x_train_cp = x_train.copy() 
    y_train_cp = y_train.copy()
else:
    print("Train set copy failed! It's more than 2 GB!")
```

- Competition ê¸€ì¸ ë§Œí¼ ë‹¤ë“¤ ì•„ëŠ” ë‚´ìš©ë“¤ì€ ë„˜ì–´ê°€ê³  ëª‡ ê°€ì§€ë§Œ ê°„ë‹¨íˆ ì§šê³  ê°€ì.
- ë°›ìë§ˆì **Null Check**ë¥¼ í•˜ëŠ” ìŠµê´€ì„ ë“¤ì´ì.
- ì™ ë§Œí•˜ë©´ Original datasetì€ **copy**í•´ì„œ ì“°ì. <small>~~ë‚˜ì¤‘ì— ì¬ìˆ˜ì—†ì–´ì„œ ìºì‹œ ë‚ ë ¤ë¨¹ê³  ìƒ¤ë“œ ì°¾ê³  í•˜ë‹¤ë³´ë©´ ì•„~~</small>
- ë³¸ í”„ë¡œì íŠ¸ì—ì„  `pd.DataFrame`ë¡œ ë°”ë¡œ ë°›ì•˜ì§€ë§Œ, ğŸ¤—ë¥¼ ì“°ëŠ” ê²½ìš°ëŠ” ì™ ë§Œí•˜ë©´ `datasets.load_dataset()`ìœ¼ë¡œ ë°›ì. 
Arrowì™€ Memory mapping ë“±ì„ ì‚¬ìš©í•˜ì—¬ ê°€ëœ©ì´ë‚˜ ë¶€ì¡±í•œ RAMë„ ì•„ë¼ê³ , type conversionì— interfaceë§Œ ë°”ê¾¸ëŠ” ë“±, ì•”íŠ¼ íš¨ìœ¨ì ì´ë‹¤. [ì°¸ê³ ](https://huggingface.co/docs/datasets/about_arrow?utm_source=chatgpt.com)

> #### Caution) Null Check for Future Input.
í˜„ì—…ì—ì„œëŠ” Train setì— **Nullì´ ì—†ì–´ë„ ë°˜ë“œì‹œ!** handling í•´ì¤˜ì•¼í•œë‹¤. Test setì´ë‚˜ Real dataì—ì„œ ë­ê°€ ë“¤ì–´ì˜¬ì§€ ëª¨ë¥´ê¸° ë•Œë¬¸ì´ë‹¤. 
ì‚¬ì‹¤ Adversary input ë“± í›¨ì”¬ ìì„¸í•˜ê²Œ ì²˜ë¦¬í•´ì¤˜ì•¼ í•˜ë‚˜, ì–´ì§€ê°„í•œ ì¶œì‹œ ì§ì „ ë‹¨ê³„ê°€ ì•„ë‹ˆê³ ì„œì•¼ ~~ê³ ê·€í•˜ì‹  DSë“¤ì€~~ ê·€ì°®ì•„ì„œ ì˜ ì•ˆí•˜ëŠ”ê²Œ êµ­ë£°ì´ì§€ë§Œ ã…ã… ê·¸ë˜ë„ Null handlingë§Œí¼ì€ í•´ì£¼ì! 

## 2.2. Feature Engineering.

### Table: Feature List.

| **Category**       | **Features**             |
|:--------------------|:-------------------------|
| **Metadata**        | `qa_id`                 |
|                     | `question_user_name`    |
|                     | `question_user_page`    |
|                     | `answer_user_name`      |
|                     | `answer_user_page`      |
|                     | `url`                   |
|                     | `category`              |
|                     | `host`                  |
| **Question**        | `question_title`        |
|                     | `question_body`         |
| **Answer**          | `answer`                |


- `qa_id` : Sampleì˜ indexì´ë‹¤. Submit í•  ë•Œ ë¹¼ê³¤ 1ë„ ì“¸ëª¨ì—†ë‹¤.
- `question_title`, `question_body`, and `answer` : ì¼ë°˜ì ì¸ textì´ë‹¤. íŠ¹ì´í•œ ì ì€, ì¼ì¢…ì˜ category í˜¹ì€ summaryì˜ ì—­í• ì„ í•˜ëŠ” **`question_title`**ì„ ì œê³µí•œë‹¤ëŠ” ì ì´ë‹¤. ì´ëŠ” ë‹¹ì—°íˆ ë³„ë„ì˜ Transformerë¡œ í•™ìŠµí•˜ê³  mergeë¥¼ í•˜ë˜ ensembleì„ í•˜ë˜ í•´ì•¼ê² ì§€ë§Œ, ì‹œê°„ì´ ì—†ì–´ì„œ ëª»í–ˆë‹¤ (ì§„ì§œë¡œ ã… ). ì‹¤ì œ ì°¸ì—¬í–ˆë‹¤ë©´ ë‹¹ì—°íˆ í–ˆì„ ê²ƒì´ë‹¤.

### Asker-specific Pattern?

- `question_user_name`, `question_user_page` : ì§ˆë¬¸ì ì´ë¦„ ë° í”„ë¡œí•„ í˜ì´ì§€ë‹¤. 
  - í”„ë¡œí•„ì€ ëŒ€ì¶© [ì´ëŸ°](https://serverfault.com/users/111705/beingalex) í˜•íƒœì´ë‹¤.
  - URL íƒ€ê³  ê°€ì„œ ì§ˆë¬¸ìì— ëŒ€í•œ ë©”íƒ€ë°ì´í„°ë¼ë„ ê¸ì–´ì˜¤ë¼ê³  ì¤€ê±¸ê¹Œ? ~~(ë‹¹ì—°íˆ ì•„ë¬´ë„ ì•ˆí•¨ ã…‹ã…‹)~~
  - ë‹¤ë“¤ ìì—°ìŠ¤ëŸ½ê²Œ 'ì§ˆë¬¸ì ë³„ë¡œ íŠ¹ì§•ì´ ìˆë‹¤ë©´?'ì´ë¼ëŠ” ìƒê°ì´ ë“¤ì—ˆì„ ê²ƒì´ë‹¤. ì‹¬ì§€ì–´ ì˜ì™¸ë¡œ ë™ì¼ ì§ˆë¬¸ìê°€ ê½¤ë‚˜ ë°˜ë³µí•´ì„œ ë‚˜ì˜¨ë‹¤?
  
  ```python
  uname = 'question_user_name'
x_train_cp[uname].value_counts().hist(bins=8)
  ```
![ì—…ë¡œë“œì¤‘..](blob:https://velog.io/5884cdf4-a1ad-4341-b5d9-20b79646f748)

  - ë‚˜ëŠ” ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì´ ë¬¸ì œëŠ” context ë°–ì˜ featureê°€ êµ‰ì¥íˆ ì†Œì¤‘í•˜ë‹¤ê³  íŒë‹¨í–ˆë‹¤. ë”°ë¼ì„œ ë§ì´ ê³ ë¯¼í–ˆì§€ë§Œ, ê³¼ê°íˆ ë²„ë¦¬ê¸°ë¡œ í–ˆë‹¤.
    - 1) Test setì—ë„ ë¹„ìŠ·í•œ íŒ¨í„´ì´ ë‚˜ì˜¨ë‹¤ëŠ” ë³´ì¥ì´ ì—†ë‹¤. ì—„ì²­ë‚œ overfittingì„ ì•¼ê¸°í•˜ê±°ë‚˜, ì•„ë¬´ ì“¸ëª¨ë„ ì—†ì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. 
    - 2) ê²°ì •ì ìœ¼ë¡œ, ì§ˆë¬¸ìì™€ í‰ê°€ìê°€ ë‹¤ë¥´ë‹¤. ì„¤ë§ˆ í‰ê°€ìê°€ ì§ˆë¬¸ì ì´ë¦„ì„ ë³´ê³  ê³ ìœ í•œ í‰ê°€ íŒ¨í„´ì„ ë³´ì˜€ì„ê¹Œ?
    - 3) ì§€ê·¹íˆ í˜„ì‹¤ì ì¸ ì´ìœ ì§€ë§Œ, ì´ë¯¸ ì¢…ë£Œëœ ëŒ€íšŒì— ì´ëŸ° Feature ë…¸ê°€ë‹¤ë¥¼ í•˜ê¸´ ì¢€ ê·¸ë ‡ë‹¤ ã… .


### Delete Redundant Features.

```python
# Delete redundant features.
redundant_features = ['qa_id', 'question_user_name', 'question_user_page',
                      'answer_user_name', 'answer_user_page', 'url']
x_train_cp = x_train_cp.drop(columns=redundant_features) 
```

- `question_user_page`, `answer_user_name` ë“± ë‹¤ë¥¸ featureë“¤ ì—­ì‹œ ê°™ì€ ì´ìœ ë¡œ ì œê±°í•˜ì˜€ë‹¤.


### Categorical Encoding.

```python
# Encode `category` and `host`.
# Find Top N hosts.
n_hosts   = 10
top_hosts = x_train_cp['host'].value_counts().nlargest(n_hosts).index

# Convert others into 'Others'.
x_train_cp['host'] = x_train_cp['host'].apply(lambda x: x if x in top_hosts else 'Others')

# One-hot Encoding.
from sklearn.preprocessing import OneHotEncoder

cols_to_enc = ['category', 'host']
one_enc     = OneHotEncoder(handle_unknown='ignore')          # Zero vector for unknown category.
x_enc       = one_enc.fit_transform(x_train_cp[cols_to_enc])

```

```
Length: 5
category
TECHNOLOGY       1960
STACKOVERFLOW    1022
CULTURE           769
SCIENCE           560
LIFE_ARTS         552
Name: count, dtype: int64

Length: 63
host
stackoverflow.com                1022
english.stackexchange.com         185
superuser.com                     176
electronics.stackexchange.com     173
serverfault.com                   165
Name: count, dtype: int64
```
- `category` : ì ë‹¹í•œ ì •ë„ì˜ classë¥¼ ê°–ëŠ” í‰ë²”í•œ ì¹´í…Œê³ ë¦¬ì¸ ê²ƒ ê°™ë‹¤. Feature dimensionë„ ë³„ë¡œ ì•ˆ í¬ë‹ˆ ì ë‹¹íˆ One-hot encoding í•´ì£¼ì.
- `host` : í˜¸ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ëŠ” ê½¤ ì¤‘ìš”í•œ feature ê²ƒ ê°™ìœ¼ë‚˜, 63ê°œëŠ” ë„ˆë¬´ ë§ë‹¤. 10ê°œë§Œ ê³¨ë¼ì„œ ë§ˆì°¬ê°€ì§€ë¡œ One-hot encoding í•´ì£¼ì.


### Merge Text Columns.

```python
# Merge sentences into one column.
cols_txt = ['question_title', 'question_body', 'answer']
x_train_cp['txt_merged'] = x_train_cp[cols_txt].apply(lambda row: ' '.join(row), axis=1)
x_train_cp = x_train_cp.drop(columns=cols_txt)
```

- 3ê°œ text columnì„ ê·¸ëƒ¥ í•©ì³¤ë‹¤. ì•ì„œ ì–¸ê¸‰í–ˆì§€ë§Œ ì ì–´ë„ `question_title`ì€, ê°€ëŠ¥í•˜ë©´ 3ê°œ ëª¨ë‘ ë³„ë„ë¡œ êµ¬ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. 
(ê°€ë¬¼ê°€ë¬¼ í•˜ì§€ë§Œ 1ë“±ì´ ê°ì í›ˆë ¨í•´ì„œ stacking í•œ ê±¸ë¡œ ê¸°ì–µí•¨)

### Preprocessing Pipeline.

```python
def preprocess(x_train, n_hosts=10):
    # 2.2.1. Drop redundant features.
    redundant_features   = ['qa_id', 'question_user_name', 'question_user_page', 
                            'answer_user_page', 'answer_user_name', 'url']
    x_train = x_train.drop(columns=redundant_features)

    # 2.2.2. Encode categorical features.
    # Converts other categories into 'Others'.
    top_hosts = x_train['host'].value_counts().nlargest(n_hosts).index
    x_train['host'] = x_train['host'].apply(lambda x: x if x in top_hosts else 'Others')

    # Encode `category` and `host`.
    categorical_features = ['category', 'host']
    one_enc     = OneHotEncoder(handle_unknown='ignore')    # Zero vector for unknown category.
    x_enc       = one_enc.fit_transform(x_train[categorical_features])

    # Convert back to DataFrame.
    enc_columns = one_enc.get_feature_names_out(categorical_features)
    x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train.index)
    x_train     = x_train.drop(columns=categorical_features)   # Drop original 'category' and 'host' columns.
    x_train     = pd.concat([x_train, x_enc_df], axis=1)       # Concatenate the encoded columns back.

    # 2.2.3. Merge txt columns.
    cols_txt = ['question_title', 'question_body', 'answer']
    x_train['txt_merged'] = x_train[cols_txt].apply(lambda row: ' '.join(row), axis=1)
    x_train  = x_train.drop(columns=cols_txt)    # Drop original txt cols.
    
    # Return.
    return x_train
```

```
category_CULTURE                                                                    0.0
category_LIFE_ARTS                                                                  0.0
category_SCIENCE                                                                    0.0
category_STACKOVERFLOW                                                              0.0
category_TECHNOLOGY                                                                 1.0
host_Others                                                                         1.0
host_askubuntu.com                                                                  0.0
host_electronics.stackexchange.com                                                  0.0
host_english.stackexchange.com                                                      0.0
host_math.stackexchange.com                                                         0.0
host_physics.stackexchange.com                                                      0.0
host_rpg.stackexchange.com                                                          0.0
host_serverfault.com                                                                0.0
host_stackoverflow.com                                                              0.0
host_superuser.com                                                                  0.0
host_tex.stackexchange.com                                                          0.0
txt_merged                            Ensuring successful merges in Subversion Subve...
```

---

# 3. Tokenization.

## 3.1. Pretrained LLM.

```python
checkpoints = {'distilbert' : 'distilbert-base-uncased',
               'bert' : 'bert-base-uncased',
               'roberta' : 'roberta-base'}
```
- Pretrained LLMì„ ì“¸ ë•ŒëŠ” LLMê³¼ ë™ì¼í•œ Tokenizerë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ, ì§€ê¸ˆ í›„ë³´ë¥¼ ì •í•´ì¤€ë‹¤. ~~(ì„¤ë§ˆ ëª¨ë¥´ì‹œëŠ” ë¶„ ì—†ì£ ?)~~

## 3.2. How Many Tokens to Preserve?

- RNN and variationsì™€ ë‹¤ë¥´ê²Œ, transformerëŠ” ê³ ì •ëœ ê¸¸ì´ì˜ ì…ë ¥ì„ ë°›ëŠ”ë‹¤. ëŒ€ë¶€ë¶„ì˜ Typical LLMì€ 512ë‹¤. ì¦‰ ìµœëŒ€ 512ê°œê¹Œì§€ í† í°ì„ ë°›ì„ ìˆ˜ ìˆë‹¤.
- ì´ë¥¼ ë„˜ëŠ” Sampleì€ ìë¥´ë˜ê°€ (**truncation**) ë‚˜ëˆ ì•¼í•œë‹¤ (**stride**).
- `tokenizer`ì˜ **`max_length`**ëŠ” ë§¤ìš° ì¤‘ìš”í•œ hyperparameterë‹¤. ì¤„ì´ë©´ Input dimensionì´ í¬ê²Œ ì¤„ì–´ë“¤ì§€ë§Œ, ë‹¹ì—°íˆ ì •ë³´ ì†ì‹¤ ì—­ì‹œ ì»¤ì§„ë‹¤.

```python
x_train_tokenized['input_ids']    = list(tokenized['input_ids'])
x_train_tokenized['token_length'] = x_train_tokenized['input_ids'].apply(len)
x_train_tokenized['token_length'].hist(bins=200).set_xlim(0, 2000)
```
![ì—…ë¡œë“œì¤‘..](blob:https://velog.io/fe485e31-96b7-47f4-8d2a-a184231ca350)

```python
# How many tokens are covered, i.e. not truncated, with the given length?
n_train       = len(x_train_tokenized['token_length'])
token_lengths = [128, 256, 512]

for token_length in token_lengths:
    rank         = (x_train_tokenized['token_length'] <= token_length).sum()
    quantile     = (rank / n_train) * 100
    print(f"max_length={token_length} covers {quantile:.2f}% of samples!")
```

```
max_length=128 covers 7.38% of samples!
max_length=256 covers 35.10% of samples!
max_length=512 covers 73.06% of samples!
```

- ì•„ì‰½ê²Œë„ ë§ì€ Sampleë“¤ì´ 512ë¥¼ ê°€ë¿íˆ ë„˜ê³  ìˆë‹¤. ë¶ˆê³¼ 73%ë°–ì— ì»¤ë²„ë˜ì§€ ì•ŠëŠ”ë‹¤ (ì¦‰ Sample ì¤‘ 27%ëŠ” ë í˜¹ì€ ìƒë‹¹ ë¶€ë¶„ì´ ì˜ë¦°ë‹¤).
- ~~ë¦¬ë·° ì¢€ ì‘ì‘ ì“°ë¼ê³ ! ë¼ê³  ìƒê°í•  ìˆ˜ë„ ìˆì§€ë§Œ~~ ì´ëŠ” ì•„ì£¼ ìì—°ìŠ¤ëŸ¬ìš´ ê²°ê³¼ë‹¤. 3ê°œ textë¥¼ í•©ì³¤ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë¯¸ ëª‡ ë²ˆ ì–¸ê¸‰í–ˆì§€ë§Œ, ì‹œê°„ê³¼ ìì›ì´ í—ˆë½í•œë‹¤ë©´ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ë‚˜ëˆ ì„œ Ensembleì„ í•´ë³´ì!

## 3.3. Tokenize.

```python
def tokenize(df):  
    # Define Tokenizer.                   
    tokenized = tokenizer(
        list(df['txt_merged']),
        padding          = True,
        truncation       = True,
        max_length       = max_length,
#       stride           = 0,           # Can be kept if you want overlapping tokens.
        return_tensors   = "np"  
    )

    # And tokenize.
    df['input_ids']      = list(tokenized['input_ids'])
    df['attention_mask'] = list(tokenized['attention_mask'])
    
    df = df.drop(columns=['txt_merged'])      # Drop original text column.
    
    return df

```

- `stride` : ì¼ë°˜ì ìœ¼ë¡œ Truncationì´ ë§ì€ ê²½ìš° `stride`ëŠ” ì¢‹ì€ ì„ íƒì´ë‹¤. í•˜ì§€ë§Œ í•„ìëŠ” ì—¬ëŸ¬ê°€ì§€ ì‹œë„í•´ë³´ê³  ëºë‹¤. Title, question, answerì´ ì¡íƒ•ìœ¼ë¡œ ì„ì—¬ìˆê³  ê°ìì˜ ê¸¸ì´ê°€ êµ‰ì¥íˆ ë‹¤ì–‘í•´ì„œ, Fixed lengthë¡œ ë°˜ë³µí•˜ëŠ” `stride`ëŠ” ê²°êµ­ ë§ˆì´ë„ˆìŠ¤ê°€ ë” ì»¸ë‹¤. ~~(ë‚˜ëˆ•ì‹œë‹¤ ã…ã…)~~

---

# 4. Model.

# 4.1. Custom Model.

```python
class CustomModel(nn.Module):
    def __init__(self, checkpoint, num_labels, additional_feature_dim):
        super(CustomModel, self).__init__()
        
        # Load pretrained transformer.
        self.transformer = AutoModel.from_pretrained(checkpoint)

        # Expose the transformer's config.
        self.config = self.transformer.config
        
        # Combine transformer outputs with additional features
        transformer_hidden_size = self.transformer.config.hidden_size
        self.fc1 = nn.Linear(transformer_hidden_size + additional_feature_dim, num_labels)
        
#       self.fc2 = nn.Linear(256, num_labels)   # For complex Head.
#       self.dropout = nn.Dropout(0.1)          # For dropout.
        
    def forward(self, input_ids, attention_mask, additional_features):
        # Transformer output.
        transformer_output = self.transformer(
            input_ids      = input_ids,
            attention_mask = attention_mask
        )
        
        # Use [CLS] token for concatenation.
        cls_output     = transformer_output.last_hidden_state[:, 0, :]
        combined_input = torch.cat([cls_output, additional_features], dim=1)
        
        # Pass through fully connected layers
#       x = self.dropout(torch.relu(self.fc1(combined_input)))
        output = self.fc1(combined_input)
        
        return output
```

- ì´ˆë°˜ë¶€ì— ì–¸ê¸‰í–ˆì§€ë§Œ, ë‚˜ëŠ” Direct context from dialogue ì™¸ì˜ FeatureëŠ” Transformerê°€ ì˜ detect í•˜ì§€ ëª»í•  ê±°ë¼ê³  ìƒê°í•œë‹¤. ~~(ë„ëŒ€ì²´ ì–´ë–¤ corpusë¥¼ í•™ìŠµí•´ì•¼ 'ì œ 3ìëŠ” ì´ ë‹µë³€ì— ê¸°ë»í• ê±°ì•¼! ì¢‹ì•„í• ê±°ì•¼!' ì´ëŸ°ê±¸ í•™ìŠµí•œë‹¨ ë§ì¸ê°€?)~~
- ë”°ë¼ì„œ 1) ì¼ë‹¨ QA featuresë¥¼ Transformerì— Feedforward ì‹œí‚¤ê³ , 2) ë‚˜ì˜¨ Hidden stateë¥¼ `host` ë° `category`ì™€ Concatenate í•˜ì—¬ Headì— Feedforward ì‹œí‚¤ë©°, 3) ì¡°ê¸ˆ ë” Complex architecture for Headë¥¼ ì‹œë„í•´ë³´ì•˜ë‹¤.
- ì§€ê¸ˆì€ ì—†ì§€ë§Œ, ì‚¬ì‹¤ `self.fc1`ê³¼ `self.fc2` ì‚¬ì´ì—ëŠ” êµ‰ì¥íˆ ë§ì€ ì‹œë„ë“¤ì´ ìˆì—ˆê³ , ëª‡ëª‡ RNN ê¸°ë°˜ ì‹œë„ë“¤ì€ ìƒë‹¹íˆ ì„±ê³µì ì´ì—ˆë‹¤. í•˜ì§€ë§Œ ë¶ˆìŒí•œ ë‚˜ì˜ 3070ì´ í„°ì§€ê¸° ì§ì „ì´ì–´ì„œ, ì–´ì©” ìˆ˜ ì—†ì´ ë‹¤ ì§€ì›Œë²„ë ¸ë‹¤.
- ì‚¬ì‹¤ ìš”ì¦˜ íŠ¸ë Œë“œëŠ” ë§‰ëŒ€í•œ ìë³¸ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ LLMì„ ê³ ì´ ëª¨ì‹œëŠ” ìª½ì´ë‹¤ (Optimized full-training). í•˜ì§€ë§Œ ì´ ëŒ€íšŒì²˜ëŸ¼ ì ë‹¹í•œ ê·œëª¨ë¡œ, ì¼ë°˜ì¸ ìœ„ì£¼ì˜, TaskëŠ” ë‹¨ìˆœí•˜ì§€ë§Œ êµ‰ì¥íˆ Domain-specificí•œ ê²½ìš°, ì ë‹¹í•œ ê·œëª¨ì˜ LLM + Complex Headê°€ ì‹œê°„/ìì›ì´ ì œí•œëœ ëŒ€íšŒì—ì„œ ë¨¹íˆëŠ” ê²½ìš°ê°€ ê½¤ ìˆë‹¤.

## 4.2. HF Wrapper.

```python
# Define HF Wrapper for Custom Model.
class HuggingFaceModelWrapper(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model         # Custom model.
        self.config     = base_model.config  # Expose the base model's config.

    def forward(self, input_ids, attention_mask, additional_features, labels=None):
        # Forward pass through the base model.
        output = self.base_model(input_ids           = input_ids, 
                                 attention_mask      = attention_mask, 
                                 additional_features = additional_features)
        
        # If labels are provided, calculate loss.
        logits = output
        loss   = None
        if labels is not None:
            loss_fn = nn.BCEWithLogitsLoss()
            loss    = loss_fn(logits, labels)
        
        return {"loss": loss, "logits": logits}

    def prepare_inputs_for_generation(self, *args, **kwargs):
        # Delegate to the base model.
        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)
```
- Custom PyTorch ëª¨ë¸ê³¼ ğŸ¤— libraryë“¤(Trainer ë“±)ì„ ì—°ë™í•˜ê¸° ìœ„í•œ wrapperë‹¤.
- ê¸‰í•˜ê²Œ ì‘ì„±í•˜ëŠë¼ ëª‡ê°€ì§€ ì‹¤ìˆ˜ê°€ ë³´ì¸ë‹¤(`nn.Module`ì„ ìƒì†í–ˆë‹¤ë˜ì§€). ì—­ì‹œë‚˜ ì´í›„ PEFT ë“± ëª‡ê°€ì§€ ë¬¸ì œê°€ ìˆì—ˆì§€ë§Œ ê·¸ëŸ­ì €ëŸ­ ë‹¤ë¥¸ ë¬¸ì œëŠ” ë°œê²¬ë˜ì§€ ì•Šì•˜ë‹¤.
<small>~~(ì‘ ì–´ì°¨í”¼ ì•ˆì“¸ë¼í–ˆìŒ)~~</small>

---

# 5. Fine-Tuning.

## 5.1. Metric.

```python
from scipy.stats import spearmanr

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions    = np.argmax(logits, axis=1) if logits.ndim == 3 else logits

    # Calculate Spearman's correlation for each label.
    spearman_corrs = []
    for i in range(labels.shape[1]):
        corr, _ = spearmanr(predictions[:, i], labels[:, i])
        spearman_corrs.append(corr)

    # Return the mean of Spearman's correlation.
    mean_spearman = np.nanmean(spearman_corrs)  # Handle NaNs if any.
    return {"spearman": mean_spearman}
```

- ë³¸ ëŒ€íšŒì—ì„œëŠ” **Spearman's Correlation**ì„ ì‚¬ìš©í•œë‹¤. scipyë¡œ êµ¬í˜„í•´ì£¼ì. ~~(ì´ì•¼ ê°œì˜¤ëœë§Œ ã… ã… )~~
- ë”°ë¼ì„œ í•œ ë¼ë²¨ì˜ **ì •í™•í•œ ê°’**ì„ ë§ì¶”ëŠ” ê²ƒì´ ì•„ë‹Œ(ê·¸ë˜ë´¤ì í™•ë¥ ì´ì§€ë§Œ), **Rank**ë¥¼ ë§ì¶”ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.
- ë”°ë¼ì„œ ë‚˜ëŠ” Headë¥¼ ë¼ë²¨ ë³„ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  í•˜ë‚˜ì— ë‹¤ ë°›ëŠ” ê²ƒìœ¼ë¡œ êµ¬ì„±í–ˆë‹¤.
- ì´ëŠ” **Tradeoff**ê°€ ìˆë‹¤. Task ë³„ë¡œ êµ¬ì„±í•˜ë©´ ê°œë³„ ë¬¸ì œ ìì²´ëŠ” í›¨ì”¬ ì‰¬ì›Œì§€ê¸° ë•Œë¬¸ì´ë‹¤. íŠ¹íˆ ì´ ëŒ€íšŒì²˜ëŸ¼ ë°ì´í„°ê°€ ì ì€ ê²½ìš°, ì´ ë°©ì‹ì´ ë” ì¢‹ì„ ìˆ˜ ìˆë‹¤.
- ê³ ë¯¼ì„ ë§ì´ í–ˆì—ˆëŠ”ë°, [2ë“±](https://www.kaggle.com/competitions/google-quest-challenge/discussion/129978)ì´ ë‚˜ëˆˆ ê±¸ ë³´ë©´ ê·¸ë‹¤ì§€ ì¢‹ì€ ì„ íƒì€ ì•„ë‹ˆì—ˆì„ì§€ë„..?

## 5.2. Training Hyperparameter.

```python
# Initialize the model.
checkpoint = checkpoints['bert']     # Other candidates : 'distilbert', 'roberta'.

num_labels              = 30
additional_features_dim = len(x_train_tokenized.columns) - 2    # Except 'input_ids' and 'attention_mask'.

model = HuggingFaceModelWrapper(
    base_model=CustomModel(checkpoint, num_labels, additional_features_dim))

# Define Callbacks.
from transformers import EarlyStoppingCallback
early_stopping_callback = EarlyStoppingCallback(
    early_stopping_patience  = 3,     # Stop after this consecutive non-improving eval steps
    early_stopping_threshold = 1e-5   # Minimum improvement threshold
)

# Define TrainingArguments.
from transformers import AutoModel, TrainingArguments, Trainer

# Hyperparameters.
batch_size    = 8
gra_steps     = 1
eval_steps    = 100
warmup_steps  = 0
logging_steps = 100

training_args = TrainingArguments(
    output_dir="./results",                   # Directory for saving model checkpoints
    overwrite_output_dir=True,                # Training from scratch, not from last training
    optim="adamw_bnb_8bit",                   # 8-bits Quantization of Optimizer.
    eval_strategy="steps",                    # Evaluate every few steps
    eval_steps=eval_steps,                    # Evaluation interval
    logging_dir="./logs",                     # Directory for TensorBoard logs
    logging_steps=logging_steps,              # Logging interval
    per_device_train_batch_size=batch_size,   # Batch size for training
    per_device_eval_batch_size=batch_size,    # Batch size for evaluation
    gradient_accumulation_steps=gra_steps,    # Steps for gradient accumulation
    lr_scheduler_type="linear",               # Learning scheduling: "linear"
    warmup_steps=warmup_steps,                # Warmup steps: 150. 
    weight_decay=2e-2,                        # Weight decay
    save_strategy="steps",                    # Save model checkpoints periodically
    save_steps=500,                           # Save every 500 steps
    save_total_limit=3,                       # Keep the last 3 checkpoints
    fp16=True,                                # Enable mixed precision (if supported)
    load_best_model_at_end=True,              # Load the best model after training
    metric_for_best_model="eval_spearman",    # Use Spearman for metric for this competition.
    greater_is_better=True,                   # Greater is better for Spearman.
    seed=42
)

```
- í†µìƒì ì¸ Train loop with ğŸ¤—. ì¤‘ê¸‰ì ì´ìƒ ë…ìë¥¼ ìƒê°í•´ì„œ ë”±íˆ í•  ë§ì´.. ê·¸ë˜ë„ êµ³ì´ ì¨ë³´ë©´ :
  - `EarlyStoppingCallback` : ì´ê±° ì•ˆì“°ì‹œëŠ” ë¶„ ì—†ì£ ..? ì¼ë‹¨ Large epoch í•˜ê³  Early stopping parameters ì¡°ì •í•˜ëŠ”ê²Œ êµ­ë£°.
  c.f. ì´ˆë°˜ì— ë„ˆë¬´ ë‚®ê²Œ ì¡ì§€ ë§ì. ~~(ì–´ì°¨í”¼ í•˜ë£¨ì¢…ì¼ learning curve ë³´ë©´ì„œ íŠœë‹í• ê±°ì–ìŠ´ ã…‹ã…‹)~~
  - `batch_size=8` and `gra_steps=1` : 3ê°œ txtë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ë°”ëŒì—(~~ë˜ ë„ˆì•¼~~) + Collate functionë„ ì œëŒ€ë¡œ ì•ˆ ì •í•˜ê³  í•˜ëŠ” ë°”ëŒì—, `batch_size`ë¥¼ ëŠ˜ë¦¬ë©´ í›ˆë ¨ì´ ì—„ì²­ ë¶ˆì•ˆì •í•´ì§€ëŠ” ê¸°ì ì´...
  - `optim='adamw_bnb_8bit'` : í™•ì‹¤íˆ `bnb` ì•ˆì“°ê³  ìì²´ ì§€ì›í•´ì£¼ë‹ˆê¹Œ ë„ˆë¬´ í¸í–ˆë‹¤. 4-bitsëŠ” ì–¸ì œ ì§€ì›í•´ì£¼ì‹œë‚˜ìš”...
  - `weight_decay=2e-2` : Defaultë¡œ `1e-2`ë¡œ ë‘ê³  overfittingì‹œ ì¡°ì ˆí•˜ëŠ”ê²Œ êµ­ë£°.
  - `metric_for_best_model="eval_spearman"` : Custom metric ì§€ì •í•´ì£¼ë©´ `early_stopping_threshold`, `greater_is_better` ë“±ë„ ê°™ì´ ì‹ ê²½ì¨ì£¼ì! ~~ë‹¤ë“¤ í•œë²ˆì¯¤ ì´ê±° ê¹œë¹¡í•˜ê³  ë©°ì¹ ì¹˜ training ë‚ ë ¤ë¨¹ì–´ë´¤ì–ìŠ´ ã…ã…~~
- `warmup_steps=0`.
  - ì´ ë†ˆì€ ì¢€ ì–¸ê¸‰ì„ í•˜ê³  ì‹¶ë‹¤.
  - í†µìƒ `n_steps`ì˜ 10% ì „í›„ë¡œ ì£¼ì–´ ì´ˆê¸° ë¶ˆì•ˆì •ì„±ì„ ë‚®ì¶˜ë‹¤.
  - ê·¸ëŸ°ë° ì–´ì°Œëœ ì¼ì¸ì§€ ì‘ì€ warmupë§Œìœ¼ë¡œë„ ë†’ì€ í™•ë¥ ë¡œ saddle pointë¡œ ì§í–‰í•˜ëŠ” ê²ƒ ê°™ë‹¤. ì•„ë¬´ë¦¬ Tensorboardë¥¼ ë’¤ì ¸ë´ë„ ì›ì¸ì„ ëª»ì°¾ì•˜ë‹¤.
  - ì´ì œì™€ì„œ ë“œëŠ” ìƒê°ì¸ë° ì„¤ë§ˆ Non-IID dataëŠ” ì•„ë‹ˆê² ì§€..? <small>~~ì¼ë‹¨ shuffle ì•ˆí•˜ê¸´ í–ˆìŒ ã…ã…~~</small>
  - ì¼ë‹¨ ì§€ê¸ˆì€ íŒ¨ìŠ¤ ã… .
## 5.3. Training.

```python
%%time

# Define Trainer and train.

# Full-Training.
training_args.num_train_epochs = 10
training_args.learning_rate    = 4e-5
trainer_full = Trainer(
    model            = model,                          
    args             = training_args,              # TrainingArguments.
    train_dataset    = train_dataset,              # Training dataset.
    eval_dataset     = test_dataset,               # Validation dataset.
    processing_class = tokenizer,                  # Tokenizer.
    compute_metrics  = compute_metrics,            # Spearsman.
    callbacks        = [early_stopping_callback]   # EarlyStopping callback.
)
trainer_full.train()

# Freeze Body.
for param in model.base_model.transformer.parameters():
    param.requires_grad = False

# Head-only Training.
training_args.num_train_epochs = 20
training_args.learning_rate    = 3e-5
trainer_head = Trainer(
    model=model,                          
    args=training_args,                   # TrainingArguments
    train_dataset=train_dataset,          # Training dataset
    eval_dataset=test_dataset,            # Validation dataset
    processing_class=tokenizer,           # Tokenizer
    compute_metrics=compute_metrics,      # Evaluation metric function
    callbacks=[early_stopping_callback]   # EarlyStopping Callback.
)
trainer_head.train()

# Notification for finish.
# import winsound
# winsound.PlaySound("Alarm03", winsound.SND_ALIAS)
```

- **Alternating Training**.
  - Headì— ë§ì€ ê´€ì‹¬ì„ ê¸°ìš¸ì¸ë§Œí¼, Full-trainingê³¼ Head-only trainingì„ ë‚˜ëˆ„ì–´ í›ˆë ¨í–ˆë‹¤.
  - ë‚˜ëŠ” Domain-specific íŠ¹ì„±, ê·¸ë¦¬ê³  ì—„ì²­ heavyí•œ LLMì´ ì•„ë‹ˆë¼ Fullì„ ì¢€ ë§ì´ ì£¼ê³ (3~5), Head-onlyì™€ iterationí•˜ë©° ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì‹œë„í•œ ê²°ê³¼, í†µìƒì ì¸ ë°©ë²• (Fullì˜ ëë¶€ë¶„ë§Œ ì¡°ê¸ˆì”© ë…¹ì´ë©° Fine-tuning)ë³´ë‹¤ ê½¤ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆë‹¤. 
  - ë‹¹ì‹œ ì‹¤ì œ ëŒ€íšŒë„ ì•„ë‹ˆê³  ë¸”ë¡œê·¸ë¡œ ì •ë¦¬í•  ì¤„ë„ ëª°ëì–´ì„œ, ì •í™•í•œ ë°ì´í„°ë¥¼ ë³µì›í•˜ì§€ ëª»í•œ ì  ì‹¬ì‹¬í•œ ì‚¬ê³¼ì˜ ë§ì”€ì„...
  - ì½”ë“œì—” `training_args.num_train_epochs = 10`ì´ì§€ë§Œ ëŒ€ë¶€ë¶„ Early Stoppingì´ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ëŠì—ˆë‹¤. ~~(ê·¸ë ‡ë‹¤ ì¢…ì¼ ì³ë‹¤ë³´ê³  ìˆì—ˆë‹¤)~~
- **Stage-Wise Learning Rate**.
  - Learning rateë„ ë‹¤ë¥´ê²Œ ì£¼ì—ˆë‹¤.
  - ì¼ë°˜ì ì¸ ì˜ˆìƒê³¼ëŠ” ë‹¬ë¦¬, Head-onlyì—ì„œ ì‘ì€ Learning rateë¥¼ ì£¼ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ì•˜ë‹¤. Full-trainingì—ì„œ ì´ë¯¸ ê°™ì´ í•™ìŠµí–ˆì„í…Œë‹ˆ, ì¼ë°˜ì ì¸ schedulingì— ë¶€í•©í•œë‹¤. ì´ìƒí•œ í˜„ìƒì€ ì•„ë‹ˆë‹¤.
  
## 5.4. Final Parameters.

### BERT.

| Parameter                   | Value                   |
|-----------------------------|-------------------------|
| Model                       | bert-base-uncased       |
| Max Length                  | 512                     |
| Learning Rate (Full)        | 4e-5                    |
| Learning Rate (Head)        | 3e-5                    |
| Weight Decay                | 2e-2                    |
| Warmup Steps                | 0                       |
| Batch Size                  | 8                       |
| Gradient Accumulation Steps | 1                       |
| Evaluation Steps            | 100                     |
| Early Stopping Patience     | 3                       |
| Early Stopping Threshold    | 1e-5                    |
| Full Fine-Tuning Epochs     | 10      |
| Head Fine-Tuning Epochs     | 20                      |
| Spearman Correlation        | 0.3813 (127 / 1572)     |
| CPU Time                    | 10min 32s              |
| Wall Time                   | 9min 26s               |
| Train Runtime               | 125.9416 seconds        |
| Train Samples Per Second    | 772.262                |
| Train Steps Per Second      | 96.553                 |
| Training Loss               | 0.324978                |
| Global Steps                | 1200                   |

- ë¶„ëª… 0.39ë¥¼ ë„˜ì—ˆë˜ ë•Œë„ ìˆì—ˆëŠ”ë°, ì•„ë¬´ë¦¬ checkpointë¥¼ ë’¤ì ¸ë´ë„ ëª»ì°¾ì•˜ë‹¤ ã… .
- ì‚¬ì‹¤ ì´ê±´ í•œ Full-Head cycleì— ëŒ€í•œ ê¸°ë¡ì´ê³ , Alternating Trainingì— ëŒ€í•œ ê¸°ë¡ì„ ì°¾ì§€ ëª»í–ˆë‹¤. ì—¬ëŸ¬ëª¨ë¡œ ì•„ì‰¬ì› ë‹¤.

---

# 6. Conclusion.

- ì˜¤ëœë§Œì— ë‹¤ì‹œ ëŒ€íšŒì— ë„ì „í•´ë³´ì•˜ê³ , 127/1,572 (top 8%)ë¼ëŠ” ê´œì°®ì€ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆë‹¤!
- BERTì— Custom Head, Stage-wise Fine-Tuning ë“± ì—¬ëŸ¬ê°€ì§€ ê¸°ë²• ë° íŠœë‹ì„ ì‹œë„í–ˆë‹¤.
- ê°‘ì‘ìŠ¤ëŸ½ê²Œ ì¤€ë¹„í•˜ì—¬ Optuna, PEFT, bitsandbytes, ETensorboard ë“± ì‹¤ì œ Fine-Tuningì— ì‚¬ìš©ë˜ëŠ” Tech stacksë¥¼ ì†Œê°œí•˜ì§€ ëª»í•´ ì•„ì‰¬ì› ë‹¤.
- ë­ class-weighted training ë“± ì—¬ëŸ¬ê°€ì§€ë¥¼ ì–¸ê¸‰í•´ì„œ ìœ ì‹í•œ ì²™í• ìˆœ ìˆê² ì§€ë§Œ, ê·¸ëƒ¥ ensembleì´ ê°‘ì¸ ì „í˜•ì ì¸ ì¼€ì´ìŠ¤ì˜€ë‹¤. 
íŠ¹íˆ label ë³„ë¡œ weak classifierë¥¼ êµ¬ì„±í•´ adaboostë¥¼ ëŒë¦¬ë©´ ì˜ ë¨¹í ê²ƒ ê°™ì•˜ëŠ”ë°, ì œëŒ€ë¡œ í•´ë³´ì§€ ëª»í•´ ì—­ì‹œ ì•„ì‰¬ì› ë‹¤.
- ë¸”ë¡œê·¸ ì‘ì„±ë§Œ 6ì‹œê°„ 38ë¶„ì´ ê±¸ë ¸ë‹¤. ë§‰ìƒ ì“´ ê±° ë³´ë©´ í—ˆìˆ í•œë° ì´ê±° ìƒê°ë³´ë‹¤ ì‹œê°„ì´ ì—„ì²­ ê±¸ë¦°ë‹¤. ìš•ì‹¬ì„ ì¢€ ë‚´ë ¤ë†”ì•¼ê² ë‹¤.



