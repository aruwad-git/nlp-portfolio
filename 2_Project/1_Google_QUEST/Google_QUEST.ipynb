{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf9ee1d-2027-43f5-96e4-d91233d0a20c",
   "metadata": {},
   "source": [
    "# 0. Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3da4ba2-a841-4fe9-986c-9605ec3d9cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random Seed.\n",
    "from transformers import set_seed\n",
    "import tensorflow as tf\n",
    "\n",
    "set_seed(42)              # For HF.\n",
    "tf.random.set_seed(42)    # For tf, np, and python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68286540-618c-4f3e-9b57-5cb50a610cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        float: left;\n",
       "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1a0db-7c14-4dfd-9986-b6c9ddf7efe8",
   "metadata": {},
   "source": [
    "# 2. Preprocessing.\n",
    "- Choose nonlinguistic features : **Category** and **Host**.\n",
    "- Preprocess : One-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26731822-8af4-4b42-a928-f1c3031480d4",
   "metadata": {},
   "source": [
    "## 2.1. Load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da3672c5-8fc9-4909-91c2-d945f73da998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any null values? False\n"
     ]
    }
   ],
   "source": [
    "# Load.\n",
    "from sklearn.model_selection import train_test_split\n",
    "data                = pd.read_csv('./dataset/train.csv')\n",
    "train_set, test_set = train_test_split(data, test_size=0.2)\n",
    "\n",
    "label_col_idx = train_set.columns.get_loc('question_asker_intent_understanding')  # Label Columns start from it.\n",
    "x_train       = train_set.iloc[:, :label_col_idx]\n",
    "y_train       = train_set.iloc[:, label_col_idx:]\n",
    "x_test        = test_set.iloc[:, :label_col_idx]\n",
    "y_test        = test_set.iloc[:, label_col_idx:]\n",
    "\n",
    "# Null Check.\n",
    "is_null = data.isnull().values.any()\n",
    "print(f'Any null values? {is_null}')\n",
    "\n",
    "# Copy train_set for EDA. (Only if train_set < 2 GB)\n",
    "train_set_size = train_set.memory_usage(deep=True).sum() / (1024 ** 3)  # In GB.\n",
    "\n",
    "if train_set_size < 2:               \n",
    "    train_cp   = train_set.copy()\n",
    "    x_train_cp = x_train.copy() \n",
    "    y_train_cp = y_train.copy()\n",
    "else:\n",
    "    print(\"Train set copy failed! It's more than 2 GB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1576b2-c0bc-42ee-9b76-4308efc9a1e7",
   "metadata": {},
   "source": [
    "> #### Note) Load Data with ðŸ¤—.  \n",
    "> In practice, use `datasets.load_dataset()`, then `df = ds.to_pandas()`.  \n",
    "> `df` loads directly to RAM, but 'ds' uses memory-mapping with disk.  \n",
    "> And conversion only temporary changes the interface, not actual conversation.  \n",
    "> Simply put, **it's a way more efficient.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b52e3896-1756-43fa-91be-41461319766c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate value counts\n",
    "name_counts = x_train_cp['question_user_name'].value_counts()\n",
    "\n",
    "# Filter names with count >= 5\n",
    "filtered_names = name_counts[name_counts >= 5].index\n",
    "\n",
    "# Replace names with 'Others' if their count is < 5\n",
    "x_train_cp['question_user_name'] = x_train_cp['question_user_name'].apply(\n",
    "    lambda x: x if x in filtered_names else 'Others'\n",
    ")\n",
    "\n",
    "# Plot histogram\n",
    "x_train_cp['question_user_name'].hist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711ba50-2e5b-4842-bf6d-46e8eb9986d0",
   "metadata": {},
   "source": [
    "## 2.2. Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc4d4dec-9415-4001-8fbe-c8231a77330f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qa_id                                                              2085\n",
       "question_title                 Ensuring successful merges in Subversion\n",
       "question_body         Subversion Re-education actually convinced me,...\n",
       "question_user_name                                                 vemv\n",
       "question_user_page    https://programmers.stackexchange.com/users/24071\n",
       "answer                Branching always begins with fear that merging...\n",
       "answer_user_name                                            Dipan Mehta\n",
       "answer_user_page      https://programmers.stackexchange.com/users/39294\n",
       "url                   http://programmers.stackexchange.com/questions...\n",
       "category                                                     TECHNOLOGY\n",
       "host                                      programmers.stackexchange.com\n",
       "Name: 1320, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show features.\n",
    "x_train_cp.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f175f0-e24b-4470-b317-5542bf790c94",
   "metadata": {},
   "source": [
    "### 2.2.1. Delete Redundant features.\n",
    "- `qa_id` : Just ID of the sample.\n",
    "- `question_user_name` : Not useful since it is not equal with rater.\n",
    "- `question_user_page` : Same reason.\n",
    "- `answer_user_name`   : For real competition, we could try to extract some info from the user page, but not for practice.\n",
    "- `answer_user_page`   : Same reason.\n",
    "- `url` : Same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e142faae-baa3-4ce4-a476-eb0099f1a911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Should I consider 'question_user_name' for asker-specific pattern?\n",
    "uname = 'question_user_name'\n",
    "x_train_cp[uname].value_counts().hist(bins=8)\n",
    "\n",
    "# Wait, asker is not same with rater! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b2d908b-19b1-45ea-b383-3aa44b3f0392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_title', 'question_body', 'answer', 'category', 'host'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete redundant features.\n",
    "redundant_features = ['qa_id', 'question_user_name', 'question_user_page',\n",
    "                      'answer_user_name', 'answer_user_page', 'url']\n",
    "x_train_cp = x_train_cp.drop(columns=redundant_features)  \n",
    "x_train_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a241ec-f93f-49ae-a152-17cb3a384897",
   "metadata": {},
   "source": [
    "### 2.2.2. Encode Categorical Columns.\n",
    "- `question_title`, `question_body`, `answer` : Should remain as string.\n",
    "- `category` : Only 5, let's encode.\n",
    "- `host` : There are clearly dominant sources. Let's try top $n$ and else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5532aa-c9d2-40ce-a0e4-380599518f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 5\n",
      "category\n",
      "TECHNOLOGY       1960\n",
      "STACKOVERFLOW    1022\n",
      "CULTURE           769\n",
      "SCIENCE           560\n",
      "LIFE_ARTS         552\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Length: 63\n",
      "host\n",
      "stackoverflow.com                1022\n",
      "english.stackexchange.com         185\n",
      "superuser.com                     176\n",
      "electronics.stackexchange.com     173\n",
      "serverfault.com                   165\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['question_title', 'question_body', 'answer', 'category_CULTURE',\n",
       "       'category_LIFE_ARTS', 'category_SCIENCE', 'category_STACKOVERFLOW',\n",
       "       'category_TECHNOLOGY', 'host_Others', 'host_askubuntu.com',\n",
       "       'host_electronics.stackexchange.com', 'host_english.stackexchange.com',\n",
       "       'host_math.stackexchange.com', 'host_physics.stackexchange.com',\n",
       "       'host_rpg.stackexchange.com', 'host_serverfault.com',\n",
       "       'host_stackoverflow.com', 'host_superuser.com',\n",
       "       'host_tex.stackexchange.com'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of each class.\n",
    "cols_cat = ['category', 'host']\n",
    "for col in cols_cat:\n",
    "    print(f'Length: {len(x_train_cp[col].value_counts())}')\n",
    "    print(x_train_cp[col].value_counts()[:5], end='\\n\\n')\n",
    "\n",
    "# Encode `category` and `host`.\n",
    "# Find Top N hosts.\n",
    "n_hosts   = 10\n",
    "top_hosts = x_train_cp['host'].value_counts().nlargest(n_hosts).index\n",
    "\n",
    "# Convert others into 'Others'.\n",
    "x_train_cp['host'] = x_train_cp['host'].apply(lambda x: x if x in top_hosts else 'Others')\n",
    "\n",
    "# One-hot Encoding.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cols_to_enc = ['category', 'host']\n",
    "one_enc     = OneHotEncoder(handle_unknown='ignore')          # Zero vector for unknown category.\n",
    "x_enc       = one_enc.fit_transform(x_train_cp[cols_to_enc])\n",
    "\n",
    "# Convert back to DataFrame.\n",
    "enc_columns = one_enc.get_feature_names_out(cols_to_enc)\n",
    "x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train_cp.index)\n",
    "\n",
    "x_train_cp   = x_train_cp.drop(columns=cols_to_enc)           # Drop original 'category' and 'host' columns.\n",
    "x_train_cp   = pd.concat([x_train_cp, x_enc_df], axis=1)      # Concatenate the encoded columns back to x_train_2.\n",
    "\n",
    "# Result.\n",
    "x_train_cp.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589e4d5-5574-46f5-8ee7-44499354fae0",
   "metadata": {},
   "source": [
    "### 2.2.3. Merge txt columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b744b6d9-4778-4260-beb7-8bd13cde9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sentences into one column.\n",
    "cols_txt = ['question_title', 'question_body', 'answer']\n",
    "x_train_cp['txt_merged'] = x_train_cp[cols_txt].apply(lambda row: ' '.join(row), axis=1)\n",
    "x_train_cp = x_train_cp.drop(columns=cols_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8a654-c6b8-4b09-bf74-0e888f50d96e",
   "metadata": {},
   "source": [
    "## 2.3. Preprocessing Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f7f6f4-3d08-4b1f-8a99-d2e886f7990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess(x_train, n_hosts=10):\n",
    "    # 2.2.1. Drop redundant features.\n",
    "    redundant_features   = ['qa_id', 'question_user_name', 'question_user_page', \n",
    "                            'answer_user_page', 'answer_user_name', 'url']\n",
    "    x_train = x_train.drop(columns=redundant_features)\n",
    "\n",
    "    # 2.2.2. Encode categorical features.\n",
    "    # Converts other categories into 'Others'.\n",
    "    top_hosts = x_train['host'].value_counts().nlargest(n_hosts).index\n",
    "    x_train['host'] = x_train['host'].apply(lambda x: x if x in top_hosts else 'Others')\n",
    "\n",
    "    # Encode `category` and `host`.\n",
    "    categorical_features = ['category', 'host']\n",
    "    one_enc     = OneHotEncoder(handle_unknown='ignore')    # Zero vector for unknown category.\n",
    "    x_enc       = one_enc.fit_transform(x_train[categorical_features])\n",
    "\n",
    "    # Convert back to DataFrame.\n",
    "    enc_columns = one_enc.get_feature_names_out(categorical_features)\n",
    "    x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train.index)\n",
    "    x_train     = x_train.drop(columns=categorical_features)   # Drop original 'category' and 'host' columns.\n",
    "    x_train     = pd.concat([x_train, x_enc_df], axis=1)       # Concatenate the encoded columns back.\n",
    "\n",
    "    # 2.2.3. Merge txt columns.\n",
    "    cols_txt = ['question_title', 'question_body', 'answer']\n",
    "    x_train['txt_merged'] = x_train[cols_txt].apply(lambda row: ' '.join(row), axis=1)\n",
    "    x_train  = x_train.drop(columns=cols_txt)    # Drop original txt cols.\n",
    "    \n",
    "    # Return.\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8265f40f-e37a-4a0d-9435-372a2d94b4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_CULTURE                                                                    0.0\n",
       "category_LIFE_ARTS                                                                  0.0\n",
       "category_SCIENCE                                                                    0.0\n",
       "category_STACKOVERFLOW                                                              0.0\n",
       "category_TECHNOLOGY                                                                 1.0\n",
       "host_Others                                                                         1.0\n",
       "host_askubuntu.com                                                                  0.0\n",
       "host_electronics.stackexchange.com                                                  0.0\n",
       "host_english.stackexchange.com                                                      0.0\n",
       "host_math.stackexchange.com                                                         0.0\n",
       "host_physics.stackexchange.com                                                      0.0\n",
       "host_rpg.stackexchange.com                                                          0.0\n",
       "host_serverfault.com                                                                0.0\n",
       "host_stackoverflow.com                                                              0.0\n",
       "host_superuser.com                                                                  0.0\n",
       "host_tex.stackexchange.com                                                          0.0\n",
       "txt_merged                            Ensuring successful merges in Subversion Subve...\n",
       "Name: 1320, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data are preprocessed!\n",
    "x_train_prep = preprocess(x_train, n_hosts=10)\n",
    "x_test_prep  = preprocess(x_test, n_hosts=10)\n",
    "x_train_prep.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac8e55-0336-400e-b772-beb9e418ae29",
   "metadata": {},
   "source": [
    "# 3. Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5d425-6c94-48c9-89ab-42106b35dc40",
   "metadata": {},
   "source": [
    "## 3.1. Choose Pretrained Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9f6b3b0-7b00-412d-8eea-a39300f7f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = {'distilbert' : 'distilbert-base-uncased',\n",
    "               'bert' : 'bert-base-uncased',\n",
    "               'roberta' : 'roberta-base'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbfc31-7329-4bbf-8fe2-8ea8700e20bd",
   "metadata": {},
   "source": [
    "## 3.2. Maximum Number of Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ccecd17-5521-4460-9fd0-87bc0c6a0ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (777 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Init data.\n",
    "x_train_tokenized = x_train_prep.copy()\n",
    "x_test_tokenized  = x_test_prep.copy()\n",
    "\n",
    "# Create tokenizer.\n",
    "from transformers import AutoTokenizer\n",
    "checkpoint = checkpoints['distilbert']\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# First, tokenize without padding and truncation.\n",
    "tokenized = tokenizer(\n",
    "    list(x_train_tokenized['txt_merged']), \n",
    "    padding=False,\n",
    "    truncation=False,  \n",
    "    return_tensors=\"np\"  \n",
    ")\n",
    "\n",
    "# Histogram - Token Length.\n",
    "x_train_tokenized['input_ids']    = list(tokenized['input_ids'])\n",
    "x_train_tokenized['token_length'] = x_train_tokenized['input_ids'].apply(len)\n",
    "x_train_tokenized['token_length'].hist(bins=200).set_xlim(0, 2000)               # 512 = Maximum num of tokens for many LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0504a619-729b-4a06-b22f-fba5bf8055c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2000.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_tokenized['token_length'].hist(bins=200).set_xlim(0, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0814bd9-37d0-4d2d-a91e-879a7599844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length=128 covers 7.38% of samples!\n",
      "max_length=256 covers 35.10% of samples!\n",
      "max_length=512 covers 73.06% of samples!\n"
     ]
    }
   ],
   "source": [
    "# How many tokens are covered, i.e. not truncated, with the given length?\n",
    "n_train       = len(x_train_tokenized['token_length'])\n",
    "token_lengths = [128, 256, 512]\n",
    "\n",
    "for token_length in token_lengths:\n",
    "    rank         = (x_train_tokenized['token_length'] <= token_length).sum()\n",
    "    quantile     = (rank / n_train) * 100\n",
    "    print(f\"max_length={token_length} covers {quantile:.2f}% of samples!\")\n",
    "\n",
    "# Final value for max_length.\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf477b-30e6-4cfc-84a0-0aa6ed507864",
   "metadata": {},
   "source": [
    "## 3.3. Tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92fef5a0-5c81-4220-ad7c-d775bbed879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple method for tokenization.\n",
    "def tokenize(df):  \n",
    "    # Define Tokenizer.                   \n",
    "    tokenized = tokenizer(\n",
    "        list(df['txt_merged']),\n",
    "        padding          = True,\n",
    "        truncation       = True,\n",
    "        max_length       = max_length,\n",
    "#       stride           = 0,                 # Can be kept if you want overlapping tokens.\n",
    "        return_tensors   = \"np\"  \n",
    "    )\n",
    "\n",
    "    # Tokenize.\n",
    "    df['input_ids']      = list(tokenized['input_ids'])\n",
    "    df['attention_mask'] = list(tokenized['attention_mask'])\n",
    "    \n",
    "    df = df.drop(columns=['txt_merged'])      # Drop original text column.\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Tokenize.\n",
    "x_train_tokenized = tokenize(x_train_prep)\n",
    "x_test_tokenized  = tokenize(x_test_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b7022-10f4-43b7-85e2-9e58778d6889",
   "metadata": {},
   "source": [
    "> #### Note) `stride` for truncated tokens.  \n",
    "> As you can see, 73.39% of tokens will be truncated even with `max_length=512`.  \n",
    "> `max_length=512` is the most popular size for pretrained LLMs, so it's probably not enough just considering other LLMs.  \n",
    "> Consider `stride` instead. It's omitted in this project because of resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d24901-8fbb-4ad6-85f8-d6fcc45709e7",
   "metadata": {},
   "source": [
    "# 4. Model.\n",
    "- **Transformer** : takes input_ids and attention_mask => outputs representation.\n",
    "- **Head** : takes representation from transformer AND other features (category and host) => outputs values of 30 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28693416-8432-4495-86fc-14e09fc62675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Custom Model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels, additional_feature_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained transformer.\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "        # Expose the transformer's config.\n",
    "        self.config = self.transformer.config\n",
    "        \n",
    "        # Combine transformer outputs with additional features\n",
    "        transformer_hidden_size = self.transformer.config.hidden_size\n",
    "        self.fc1 = nn.Linear(transformer_hidden_size + additional_feature_dim, num_labels)\n",
    "        \n",
    "#       self.fc2 = nn.Linear(256, num_labels)   # For complex Head.\n",
    "#       self.dropout = nn.Dropout(0.1)          # For dropout.\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, additional_features):\n",
    "        # Transformer output.\n",
    "        transformer_output = self.transformer(\n",
    "            input_ids      = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for concatenation.\n",
    "        cls_output     = transformer_output.last_hidden_state[:, 0, :]\n",
    "        combined_input = torch.cat([cls_output, additional_features], dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "#       x = self.dropout(torch.relu(self.fc1(combined_input)))\n",
    "        output = self.fc1(combined_input)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ab22460-aaa5-4d2e-bac3-53ed7afaf350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HF Wrapper for Custom Model.\n",
    "class HuggingFaceModelWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model         # Custom model.\n",
    "        self.config     = base_model.config  # Expose the base model's config.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, additional_features, labels=None):\n",
    "        # Forward pass through the base model.\n",
    "        output = self.base_model(input_ids           = input_ids, \n",
    "                                 attention_mask      = attention_mask, \n",
    "                                 additional_features = additional_features)\n",
    "        \n",
    "        # If labels are provided, calculate loss.\n",
    "        logits = output\n",
    "        loss   = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss    = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        # Delegate to the base model.\n",
    "        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79ed46-dd2c-480c-b717-35b9ae16c663",
   "metadata": {},
   "source": [
    "# 5. Fine-Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9310d-8589-4180-8a64-fca860410971",
   "metadata": {},
   "source": [
    "## 5.1. Evaluation Metrics : Spearman's Rank Correlation Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "606a8c02-68c4-40fc-835b-f072b4718ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions    = np.argmax(logits, axis=1) if logits.ndim == 3 else logits\n",
    "\n",
    "    # Calculate Spearman's correlation for each label.\n",
    "    spearman_corrs = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        corr, _ = spearmanr(predictions[:, i], labels[:, i])\n",
    "        spearman_corrs.append(corr)\n",
    "\n",
    "    # Return the mean of Spearman's correlation.\n",
    "    mean_spearman = np.nanmean(spearman_corrs)  # Handle NaNs if any.\n",
    "    return {\"spearman\": mean_spearman}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673204ee-bab3-4e59-b804-ab1dfb5216a8",
   "metadata": {},
   "source": [
    "## 5.2. Convert Dataset, `df` -> `ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a982d8eb-f466-494a-8b61-8515ec4b81c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def preprocess_data(df, labels):\n",
    "    return {\n",
    "        \"input_ids\"             : list(df['input_ids']),\n",
    "        \"attention_mask\"        : list(df['attention_mask']),\n",
    "        \"additional_features\"   : df.iloc[:, :-2].values.tolist(),  \n",
    "        \"labels\"                : labels.to_numpy().tolist()        \n",
    "    }\n",
    "\n",
    "# Convert train and test datasets\n",
    "train_dataset = Dataset.from_dict(preprocess_data(x_train_tokenized, y_train))\n",
    "test_dataset  = Dataset.from_dict(preprocess_data(x_test_tokenized, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca04f6-9647-4579-a97c-7821bc40566d",
   "metadata": {},
   "source": [
    "## 5.3. Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c05f874-acee-4085-a006-9e2a886fb50d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "checkpoint = checkpoints['bert']     # Other candidates : 'distilbert', 'roberta'.\n",
    "\n",
    "num_labels              = 30\n",
    "additional_features_dim = len(x_train_tokenized.columns) - 2    # Except 'input_ids' and 'attention_mask'.\n",
    "\n",
    "model = HuggingFaceModelWrapper(\n",
    "    base_model=CustomModel(checkpoint, num_labels, additional_features_dim))\n",
    "\n",
    "# Define Callbacks.\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience  = 3,     # Stop after this consecutive non-improving eval steps\n",
    "    early_stopping_threshold = 1e-5   # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Define TrainingArguments.\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "\n",
    "# Hyperparameters.\n",
    "batch_size    = 8\n",
    "gra_steps     = 1\n",
    "eval_steps    = 100\n",
    "warmup_steps  = 0\n",
    "logging_steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                   # Directory for saving model checkpoints\n",
    "    overwrite_output_dir=True,                # Training from scratch, not from last training\n",
    "    optim=\"adamw_bnb_8bit\",                   # 8-bits Quantization of Optimizer.\n",
    "    eval_strategy=\"steps\",                    # Evaluate every few steps\n",
    "    eval_steps=eval_steps,                    # Evaluation interval\n",
    "    logging_dir=\"./logs\",                     # Directory for TensorBoard logs\n",
    "    logging_steps=logging_steps,              # Logging interval\n",
    "    per_device_train_batch_size=batch_size,   # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,    # Batch size for evaluation\n",
    "    gradient_accumulation_steps=gra_steps,    # Steps for gradient accumulation\n",
    "    lr_scheduler_type=\"linear\",               # Learning scheduling: \"linear\"\n",
    "    warmup_steps=warmup_steps,                # Warmup steps: 150. \n",
    "    weight_decay=2e-2,                        # Weight decay\n",
    "    save_strategy=\"steps\",                    # Save model checkpoints periodically\n",
    "    save_steps=500,                           # Save every 500 steps\n",
    "    save_total_limit=3,                       # Keep the last 3 checkpoints\n",
    "    fp16=True,                                # Enable mixed precision (if supported)\n",
    "    load_best_model_at_end=True,              # Load the best model after training\n",
    "    metric_for_best_model=\"eval_spearman\",    # Use Spearman for metric for this competition.\n",
    "    greater_is_better=True,                   # Greater is better for Spearman.\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bb6e5c7a-d7ac-4a3f-9319-f7d6b86f6953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1700' max='6080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1700/6080 05:59 < 15:26, 4.73 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>0.426054</td>\n",
       "      <td>0.078708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.420127</td>\n",
       "      <td>0.135391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.423812</td>\n",
       "      <td>0.147738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.417900</td>\n",
       "      <td>0.413562</td>\n",
       "      <td>0.192561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.414500</td>\n",
       "      <td>0.408477</td>\n",
       "      <td>0.210495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.413362</td>\n",
       "      <td>0.208077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.406422</td>\n",
       "      <td>0.214082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.412028</td>\n",
       "      <td>0.217802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.405539</td>\n",
       "      <td>0.221833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.403151</td>\n",
       "      <td>0.228792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.402885</td>\n",
       "      <td>0.234114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.402333</td>\n",
       "      <td>0.234791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.399900</td>\n",
       "      <td>0.403345</td>\n",
       "      <td>0.230079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.401032</td>\n",
       "      <td>0.246033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.401888</td>\n",
       "      <td>0.237551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.394700</td>\n",
       "      <td>0.402245</td>\n",
       "      <td>0.237874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.402996</td>\n",
       "      <td>0.244298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at ./results\\checkpoint-1400\\pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='12160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  366/12160 00:34 < 18:35, 10.58 it/s, Epoch 0.60/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.399462</td>\n",
       "      <td>0.247460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.395700</td>\n",
       "      <td>0.398877</td>\n",
       "      <td>0.250791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.252654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:31\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\trainer.py:2473\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2471\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2472\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2473\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2475\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\transformers\\trainer.py:5130\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5129\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5130\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5131\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5132\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m--> 563\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:759\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    757\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m--> 759\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pin_memory_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:75\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# The sequence type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new sequence.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the sequence type is mutable.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)\n\u001b[0;32m     74\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m---> 75\u001b[0m         {k: pin_memory(sample, device) \u001b[38;5;28;01mfor\u001b[39;00m k, sample \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:75\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# The sequence type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new sequence.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the sequence type is mutable.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(data)\n\u001b[0;32m     74\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m---> 75\u001b[0m         {k: \u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, sample \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_prac\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py:64\u001b[0m, in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpin_memory\u001b[39m(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define Trainer and train.\n",
    "\n",
    "# Full-Training.\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.learning_rate    = 4e-5\n",
    "trainer_full = Trainer(\n",
    "    model            = model,                          \n",
    "    args             = training_args,              # TrainingArguments.\n",
    "    train_dataset    = train_dataset,              # Training dataset.\n",
    "    eval_dataset     = test_dataset,               # Validation dataset.\n",
    "    processing_class = tokenizer,                  # Tokenizer.\n",
    "    compute_metrics  = compute_metrics,            # Spearsman.\n",
    "    callbacks        = [early_stopping_callback]   # EarlyStopping callback.\n",
    ")\n",
    "trainer_full.train()\n",
    "\n",
    "# Freeze Body.\n",
    "for param in model.base_model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Head-only Training.\n",
    "training_args.num_train_epochs = 20\n",
    "training_args.learning_rate    = 3e-5\n",
    "trainer_head = Trainer(\n",
    "    model=model,                          \n",
    "    args=training_args,                   # TrainingArguments\n",
    "    train_dataset=train_dataset,          # Training dataset\n",
    "    eval_dataset=test_dataset,            # Validation dataset\n",
    "    processing_class=tokenizer,           # Tokenizer\n",
    "    compute_metrics=compute_metrics,      # Evaluation metric function\n",
    "    callbacks=[early_stopping_callback]   # EarlyStopping Callback.\n",
    ")\n",
    "trainer_head.train()\n",
    "\n",
    "# Notification for finish.\n",
    "# import winsound\n",
    "# winsound.PlaySound(\"Alarm03\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4b6aa0d9-703c-4803-bf02-742dea68bbd9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Rank = 1376 / 1572\n",
      "My Score = 0.2527\n",
      "Mean = 0.3212\n",
      "Median = 0.3572\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_spearman'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[260], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Show plot\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 51\u001b[0m \u001b[43mplot_learning_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m plot_learning_curve(trainer_head\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlog_history)\n",
      "Cell \u001b[1;32mIn[260], line 13\u001b[0m, in \u001b[0;36mplot_learning_curve\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     11\u001b[0m eval_loss  \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[0;32m     12\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[1;32m---> 13\u001b[0m spear_loss \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_spearman\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Training Loss curve\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[260], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m eval_loss  \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[0;32m     12\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m [log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[1;32m---> 13\u001b[0m spear_loss \u001b[38;5;241m=\u001b[39m [\u001b[43mlog\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_spearman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m history \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m log]\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Training Loss curve\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'eval_spearman'"
     ]
    }
   ],
   "source": [
    "# Check rank.\n",
    "history  = trainer_head.state.log_history   # Check `trainer_head` or `trainer_full`.\n",
    "my_score = max(\n",
    "    log[\"eval_spearman\"] for log in history if \"eval_spearman\" in log\n",
    ")\n",
    "check_rank(my_score)\n",
    "\n",
    "plot_learning_curve(trainer_full.state.log_history)\n",
    "plot_learning_curve(trainer_head.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6ba30-c100-48fc-a5fe-26171e8af955",
   "metadata": {},
   "source": [
    "### Hyperparameters for best model.\n",
    "| Parameter                 | Value                   |\n",
    "|---------------------------|-------------------------|\n",
    "| Model                     | distilbert-base-uncased |\n",
    "| Max Length                | 512                     |\n",
    "| Learning Rate (Full)      | 4e-5                    |\n",
    "| Learning Rate (Head)      | 3e-5                    |\n",
    "| Weight Decay              | 2e-2                    |\n",
    "| Warmup Steps              | 0                       |\n",
    "| Batch Size                | 8                       |\n",
    "| Gradient Accumulation     | 1                       |\n",
    "| Evaluation Steps          | 100                     |\n",
    "| Early Stopping Patience   | 3                       |\n",
    "| Early Stopping Threshold  | 1e-5                    |\n",
    "| Full Fine-Tuning Epochs   | 10      |\n",
    "| Head Fine-Tuning Epochs   | 20       |\n",
    "| Spearman Correlation      | 0.3783                  |\n",
    "| CPU Time                  | 7min 11s               |\n",
    "| Wall Time                 | 5min 50s               |\n",
    "| Train Runtime             | 111.5316 seconds        |\n",
    "| Train Samples Per Second  | 872.04                 |\n",
    "| Train Steps Per Second    | 109.027                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fb7a5-56b0-46b7-bf96-4009ae199e42",
   "metadata": {},
   "source": [
    "### BERT.\n",
    "\n",
    "| Parameter                   | Value                   |\n",
    "|-----------------------------|-------------------------|\n",
    "| Model                       | bert-base-uncased       |\n",
    "| Max Length                  | 512                     |\n",
    "| Learning Rate (Full)        | 4e-5                    |\n",
    "| Learning Rate (Head)        | 3e-5                    |\n",
    "| Weight Decay                | 2e-2                    |\n",
    "| Warmup Steps                | 0                       |\n",
    "| Batch Size                  | 8                       |\n",
    "| Gradient Accumulation Steps | 1                       |\n",
    "| Evaluation Steps            | 100                     |\n",
    "| Early Stopping Patience     | 3                       |\n",
    "| Early Stopping Threshold    | 1e-5                    |\n",
    "| Full Fine-Tuning Epochs     | 10      |\n",
    "| Head Fine-Tuning Epochs     | 20                      |\n",
    "| Spearman Correlation        | 0.3813 (127 / 1572)     |\n",
    "| CPU Time                    | 10min 32s              |\n",
    "| Wall Time                   | 9min 26s               |\n",
    "| Train Runtime               | 125.9416 seconds        |\n",
    "| Train Samples Per Second    | 772.262                |\n",
    "| Train Steps Per Second      | 96.553                 |\n",
    "| Training Loss               | 0.324978                |\n",
    "| Global Steps                | 1200                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f000a-1090-4e03-935c-628342804437",
   "metadata": {},
   "source": [
    "# 6. Summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4ddbb-1c4d-499b-b828-1e54612d0ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load.\n",
    "from sklearn.model_selection import train_test_split\n",
    "data                = pd.read_csv('./dataset/train.csv')\n",
    "train_set, test_set = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Train-Test Split.\n",
    "label_col_idx = train_set.columns.get_loc('question_asker_intent_understanding')  # Label Columns start from it.\n",
    "x_train       = train_set.iloc[:, :label_col_idx]\n",
    "y_train       = train_set.iloc[:, label_col_idx:]\n",
    "x_test        = test_set.iloc[:, :label_col_idx]\n",
    "y_test        = test_set.iloc[:, label_col_idx:]\n",
    "\n",
    "# Preprocessing.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess(x_train, n_hosts=10):\n",
    "    # 2.2.1. Drop redundant features.\n",
    "    redundant_features   = ['qa_id', 'question_user_name', 'question_user_page', \n",
    "                            'answer_user_page', 'answer_user_name', 'url']\n",
    "    x_train = x_train.drop(columns=redundant_features)\n",
    "\n",
    "    # 2.2.2. Encode categorical features.\n",
    "    # Converts other categories into 'Others'.\n",
    "    top_hosts = x_train['host'].value_counts().nlargest(n_hosts).index\n",
    "    x_train['host'] = x_train['host'].apply(lambda x: x if x in top_hosts else 'Others')\n",
    "\n",
    "    # Encode `category` and `host`.\n",
    "    categorical_features = ['category', 'host']\n",
    "    one_enc     = OneHotEncoder(handle_unknown='ignore')    # Zero vector for unknown category.\n",
    "    x_enc       = one_enc.fit_transform(x_train[categorical_features])\n",
    "\n",
    "    # Convert back to DataFrame.\n",
    "    enc_columns = one_enc.get_feature_names_out(categorical_features)\n",
    "    x_enc_df    = pd.DataFrame(x_enc.toarray(), columns=enc_columns, index=x_train.index)\n",
    "    x_train     = x_train.drop(columns=categorical_features)   # Drop original 'category' and 'host' columns.\n",
    "    x_train     = pd.concat([x_train, x_enc_df], axis=1)       # Concatenate the encoded columns back.\n",
    "\n",
    "    # 2.2.3. Merge txt columns.\n",
    "    cols_txt = ['question_title', 'question_body', 'answer']\n",
    "    x_train['txt_merged'] = x_train[cols_txt].apply(lambda row: ' '.join(row), axis=1)\n",
    "    x_train  = x_train.drop(columns=cols_txt)    # Drop original txt cols.\n",
    "    \n",
    "    # Return.\n",
    "    return x_train\n",
    "\n",
    "x_train_prep = preprocess(x_train, n_hosts=10)\n",
    "x_test_prep  = preprocess(x_test, n_hosts=10)\n",
    "\n",
    "# Tokenize.\n",
    "def tokenize(df):  \n",
    "    # Define Tokenizer.                   \n",
    "    tokenized = tokenizer(\n",
    "        list(df['txt_merged']),\n",
    "        padding          = True,\n",
    "        truncation       = True,\n",
    "        max_length       = max_length,\n",
    "#       stride           = 0,                 # Can be kept if you want overlapping tokens.\n",
    "        return_tensors   = \"np\"  \n",
    "    )\n",
    "\n",
    "    df['input_ids']      = list(tokenized['input_ids'])\n",
    "    df['attention_mask'] = list(tokenized['attention_mask'])\n",
    "    \n",
    "    df = df.drop(columns=['txt_merged'])      # Drop original text column.\n",
    "    \n",
    "    return df\n",
    "\n",
    "x_train_tokenized = tokenize(x_train_prep)\n",
    "x_test_tokenized  = tokenize(x_test_prep)\n",
    "\n",
    "# Convert datasets, `df` -> `ds`.\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(preprocess_data(x_train_tokenized, y_train))\n",
    "test_dataset  = Dataset.from_dict(preprocess_data(x_test_tokenized, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb3905-d9e9-4fb2-837e-0abaf28752ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train.\n",
    "\n",
    "# Initialize the model.\n",
    "checkpoint = checkpoints['bert']     # Other candidates : 'distilbert', 'roberta'.\n",
    "\n",
    "num_labels              = 30\n",
    "additional_features_dim = len(x_train_tokenized.columns) - 2    # Except 'input_ids' and 'attention_mask'.\n",
    "\n",
    "model = HuggingFaceModelWrapper(\n",
    "    base_model=CustomModel(checkpoint, num_labels, additional_features_dim))\n",
    "\n",
    "# Define Callbacks.\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience  = 3,     # Stop after this consecutive non-improving eval steps\n",
    "    early_stopping_threshold = 1e-5   # Minimum improvement threshold\n",
    ")\n",
    "\n",
    "# Define TrainingArguments.\n",
    "from transformers import AutoModel, TrainingArguments, Trainer\n",
    "\n",
    "# Hyperparameters.\n",
    "batch_size    = 8\n",
    "gra_steps     = 1\n",
    "eval_steps    = 100\n",
    "warmup_steps  = 0\n",
    "logging_steps = 100\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                   # Directory for saving model checkpoints\n",
    "    overwrite_output_dir=True,                # Training from scratch, not from last training\n",
    "    optim=\"adamw_bnb_8bit\",                   # 8-bits Quantization of Optimizer.\n",
    "    eval_strategy=\"steps\",                    # Evaluate every few steps\n",
    "    eval_steps=eval_steps,                    # Evaluation interval\n",
    "    logging_dir=\"./logs\",                     # Directory for TensorBoard logs\n",
    "    logging_steps=logging_steps,              # Logging interval\n",
    "    per_device_train_batch_size=batch_size,   # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size,    # Batch size for evaluation\n",
    "    gradient_accumulation_steps=gra_steps,    # Steps for gradient accumulation\n",
    "    lr_scheduler_type=\"linear\",               # Learning scheduling: \"linear\"\n",
    "    warmup_steps=warmup_steps,                # Warmup steps: 150. \n",
    "    weight_decay=2e-2,                        # Weight decay\n",
    "    save_strategy=\"steps\",                    # Save model checkpoints periodically\n",
    "    save_steps=500,                           # Save every 500 steps\n",
    "    save_total_limit=3,                       # Keep the last 3 checkpoints\n",
    "    fp16=True,                                # Enable mixed precision (if supported)\n",
    "    load_best_model_at_end=True,              # Load the best model after training\n",
    "    metric_for_best_model=\"eval_spearman\",    # Use Spearman for metric for this competition.\n",
    "    greater_is_better=True,                   # Greater is better for Spearman.\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af863bb6-883e-4ffc-81c0-78cd5e5220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define Trainer and train.\n",
    "\n",
    "# Full-Training.\n",
    "training_args.num_train_epochs = 10\n",
    "training_args.learning_rate    = 4e-5\n",
    "trainer_full = Trainer(\n",
    "    model            = model,                          \n",
    "    args             = training_args,              # TrainingArguments.\n",
    "    train_dataset    = train_dataset,              # Training dataset.\n",
    "    eval_dataset     = test_dataset,               # Validation dataset.\n",
    "    processing_class = tokenizer,                  # Tokenizer.\n",
    "    compute_metrics  = compute_metrics,            # Spearsman.\n",
    "    callbacks        = [early_stopping_callback]   # EarlyStopping callback.\n",
    ")\n",
    "trainer_full.train()\n",
    "\n",
    "# Freeze Body.\n",
    "for param in model.base_model.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Head-only Training.\n",
    "training_args.num_train_epochs = 20\n",
    "training_args.learning_rate    = 3e-5\n",
    "trainer_head = Trainer(\n",
    "    model=model,                          \n",
    "    args=training_args,                   # TrainingArguments\n",
    "    train_dataset=train_dataset,          # Training dataset\n",
    "    eval_dataset=test_dataset,            # Validation dataset\n",
    "    processing_class=tokenizer,           # Tokenizer\n",
    "    compute_metrics=compute_metrics,      # Evaluation metric function\n",
    "    callbacks=[early_stopping_callback]   # EarlyStopping Callback.\n",
    ")\n",
    "trainer_head.train()\n",
    "\n",
    "# Notification for finish.\n",
    "# import winsound\n",
    "# winsound.PlaySound(\"Alarm03\", winsound.SND_ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fac08a-9e79-444d-b536-052daaa0bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines.\n",
    "\n",
    "# Define Custom Model.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels, additional_feature_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        \n",
    "        # Load pretrained transformer.\n",
    "        self.transformer = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "        # Expose the transformer's config.\n",
    "        self.config = self.transformer.config\n",
    "        \n",
    "        # Combine transformer outputs with additional features\n",
    "        transformer_hidden_size = self.transformer.config.hidden_size\n",
    "        self.fc1 = nn.Linear(transformer_hidden_size + additional_feature_dim, num_labels)\n",
    "        \n",
    "#       self.fc2 = nn.Linear(256, num_labels)   # For complex Head.\n",
    "#       self.dropout = nn.Dropout(0.1)          # For dropout.\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, additional_features):\n",
    "        # Transformer output.\n",
    "        transformer_output = self.transformer(\n",
    "            input_ids      = input_ids,\n",
    "            attention_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token for concatenation.\n",
    "        cls_output     = transformer_output.last_hidden_state[:, 0, :]\n",
    "        combined_input = torch.cat([cls_output, additional_features], dim=1)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "#       x = self.dropout(torch.relu(self.fc1(combined_input)))\n",
    "        output = self.fc1(combined_input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define HF Wrapper for Custom Model.\n",
    "class HuggingFaceModelWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model         # Custom model.\n",
    "        self.config     = base_model.config  # Expose the base model's config.\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, additional_features, labels=None):\n",
    "        # Forward pass through the base model.\n",
    "        output = self.base_model(input_ids           = input_ids, \n",
    "                                 attention_mask      = attention_mask, \n",
    "                                 additional_features = additional_features)\n",
    "        \n",
    "        # If labels are provided, calculate loss.\n",
    "        logits = output\n",
    "        loss   = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "            loss    = loss_fn(logits, labels)\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        # Delegate to the base model.\n",
    "        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)\n",
    "\n",
    "# Spearman's Corr.\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions    = np.argmax(logits, axis=1) if logits.ndim == 3 else logits\n",
    "\n",
    "    # Calculate Spearman's correlation for each label.\n",
    "    spearman_corrs = []\n",
    "    for i in range(labels.shape[1]):\n",
    "        corr, _ = spearmanr(predictions[:, i], labels[:, i])\n",
    "        spearman_corrs.append(corr)\n",
    "\n",
    "    # Return the mean of Spearman's correlation.\n",
    "    mean_spearman = np.nanmean(spearman_corrs)  # Handle NaNs if any.\n",
    "    return {\"spearman\": mean_spearman}\n",
    "\n",
    "# Convert datasets, `df` -> `ds`.\n",
    "def preprocess_data(df, labels):\n",
    "    return {\n",
    "        \"input_ids\"             : list(df['input_ids']),\n",
    "        \"attention_mask\"        : list(df['attention_mask']),\n",
    "        \"additional_features\"   : df.iloc[:, :-2].values.tolist(),  \n",
    "        \"labels\"                : labels.to_numpy().tolist()        \n",
    "    }\n",
    "\n",
    "# Method to Check Rank.\n",
    "def check_rank(score):\n",
    "    leaderboard = pd.read_csv('./leaderboard.csv')\n",
    "    num_team    = len(leaderboard)\n",
    "    mean        = leaderboard['Score'].mean()\n",
    "    median      = leaderboard['Score'].median()\n",
    "\n",
    "    my_rank = (leaderboard['Score'] >= my_score).sum()\n",
    "\n",
    "    print(f'My Rank = {my_rank} / {num_team}')\n",
    "    print(f'My Score = {score:.4f}')\n",
    "    print(f'Mean = {mean:.4f}')\n",
    "    print(f'Median = {median:.4f}')\n",
    "\n",
    "# Plot Learning Curve.\n",
    "def plot_learning_curve(history):\n",
    "    eval_loss  = [log[\"eval_loss\"] for log in history if \"eval_loss\" in log]\n",
    "    train_loss = [log[\"loss\"] for log in history if \"loss\" in log]\n",
    "    spear_loss = [log[\"eval_spearman\"] for log in history if \"loss\" in log]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Training Loss curve\n",
    "    plt.plot(\n",
    "        [log[\"step\"] for log in history if \"loss\" in log],\n",
    "        train_loss,\n",
    "        label=\"Training Loss\",\n",
    "        marker=\"o\",\n",
    "    )\n",
    "    \n",
    "    # Validation Loss curve\n",
    "    plt.plot(\n",
    "        [log[\"step\"] for log in history if \"eval_loss\" in log],\n",
    "        eval_loss,\n",
    "        label=\"Validation Loss\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "\n",
    "    # Spearman Loss curve\n",
    "    plt.plot(\n",
    "        [log[\"step\"] for log in history if \"eval_loss\" in log],\n",
    "        spear_loss,\n",
    "        label=\"Spearman Loss\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "    \n",
    "    # Labels and legends\n",
    "    plt.title(\"Training and Validation Loss Curve\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921230b-92b2-497c-93e3-37424722d182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
