{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523b8bba-8447-47f2-aac4-ae903458c98c",
   "metadata": {},
   "source": [
    "# Introduction.\n",
    "- This simple code shows how pretrained LLM can be better for QA for wine recommendation, with RAG.\n",
    "- One of posts is used for vector DB and hence retrieved document.\n",
    "  - Website: https://www.marketviewliquor.com/blog/\n",
    "  - Post: https://www.marketviewliquor.com/blog/how-to-choose-a-good-wine/#How_to_Pick_out_Wine_for_Dinner\n",
    "- Base model is `google/flan-t5-large`, which is balanced for both performance and resources for real-time QA, in various running environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c448f6a5-fe00-4510-983b-d2b9e4816337",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 0. Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdfe815-cef8-4aa9-a054-1d4c6f025342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "import nbimporter\n",
    "\n",
    "# Random seeds.\n",
    "from transformers import set_seed\n",
    "import tensorflow as tf\n",
    "\n",
    "set_seed(42)                  # For HF.\n",
    "tf.random.set_seed(42)    # For tf, np, and python.\n",
    "\n",
    "# Suppress warnings\n",
    "import os\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661718e7-6043-447d-b5dd-37902a696f85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        float: left;\n",
       "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        float: left;\n",
    "        margin-right: 20px; /* Optional: Adds space between table and other content */\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3b2eb-f778-4026-abb5-227368a3d16e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce46fb2-63c9-4e57-8694-497670410b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808c0a60-e37d-49bc-842d-7faaffaa089f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\\\n",
    "You are chatbot that recommends a wine. Recommend a wine, based on Query and Retrieved Chunks.\n",
    "Query: {query}\n",
    "Retrieved Chunks:\n",
    "{retrieved_chunks}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1816c5-1b3f-49aa-9856-8ff757996cee",
   "metadata": {},
   "source": [
    "## 1.2. Fetch data from url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a45173b9-a4ee-4aa8-8c9d-4446d36744f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "async def fetch_url_async(session, url):\n",
    "    \"\"\"Fetch content from a URL asynchronously.\"\"\"\n",
    "    try:\n",
    "        async with session.get(url, timeout=10) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Custom rule for 'https://www.marketviewliquor.com/blog/how-to-choose-a-good-wine/?'.\n",
    "            paragraphs = soup.find('div', class_=\"entry-content\").find_all('p')\n",
    "            items      = soup.find('div', class_=\"lwptoc_item\")\n",
    "            spans      = soup.find('div', class_=\"entry-content\").find_all('span')\n",
    "            \n",
    "            txts = []\n",
    "            txts.extend(paragraphs)\n",
    "            txts.extend(items)\n",
    "            txts.extend(spans)\n",
    "            txts = \"\\n\\n\".join(txt.get_text(separator='. ', strip=True) for txt in txts)\n",
    "            \n",
    "            # Clean txts.\n",
    "            txts = txts.replace(u'\\xa0', u' ')\n",
    "            \n",
    "            return txts\n",
    "            \n",
    "    except asyncio.TimeoutError:\n",
    "        return \"Timeout occurred\"\n",
    "    except Exception as e:\n",
    "        return f\"Failed to load URL: {str(e)}\"\n",
    "\n",
    "async def fetch_all_urls_async(doc_urls):\n",
    "    \"\"\"Fetch all URLs concurrently using aiohttp.\"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_url_async(session, url) for url in doc_urls]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2622c022-e7c9-47fc-ad36-e5eb8456205b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare URLs.\n",
    "doc_urls = [\n",
    "    'https://www.marketviewliquor.com/blog/how-to-choose-a-good-wine/?'\n",
    "]\n",
    "\n",
    "# Load Contents from URLs.\n",
    "doc_contents = await fetch_all_urls_async(doc_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3167c9-d5d4-48a5-90c4-a722f6087384",
   "metadata": {},
   "source": [
    "## 1.3. Construct FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "296a389f-7b1c-43a8-b906-eef190842cac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress chunk size warning during split.\n",
    "import logging\n",
    "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
    "\n",
    "# Text splitter.\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "chunk_size    = 200\n",
    "chunk_overlap = 0\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=\". \", \n",
    "                                      chunk_size=chunk_size, \n",
    "                                      chunk_overlap=chunk_overlap)\n",
    "\n",
    "chunks = text_splitter.split_text(doc_contents[0])\n",
    "\n",
    "# Embedding model.\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_kwargs = {'device': device}\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                        model_kwargs   = model_kwargs)\n",
    "\n",
    "# Save on FAISS.\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_texts(chunks, embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e2cfe-eead-41dc-93e5-954eeaefb114",
   "metadata": {},
   "source": [
    "# 2. Query and Retrieve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6feae53-5a29-4265-b774-6ece827bf6fa",
   "metadata": {},
   "source": [
    "## 2.1. Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a4b0bede-ac3a-4fe3-b059-eab26624b1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_list = [\n",
    "    \"I'm new to wine. Which wine should I start?\",\n",
    "    \"What should I consider to choose a good wine?\",\n",
    "    \"Which food is good with sweet wine?\",\n",
    "    \"Does wine always become better as getting older?\",\n",
    "    \"Is price important for wine?\",\n",
    "    \"How long can I consume a wine after purchase?\",\n",
    "    \"Are wines with screw caps bad?\",\n",
    "    \"How can I log the history of wine consumption?\",\n",
    "    \"Which categories should I check on the label of wine?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abda01f-8188-42cc-8401-8c0234f57902",
   "metadata": {},
   "source": [
    "# Answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46416cfb-cf3e-4b26-9182-b59470688587",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## `google/flan-t5-base`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37734586-e1aa-4403-9c57-57f66ba9bec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. I'm new to wine. Which wine should I start with?  \n",
    "   - **No-RAG:** Sauvignon Blanc – Acceptable, as it's crisp and approachable.  \n",
    "   - **RAG:** White or Rosé – More accurate since both are beginner-friendly wines.\n",
    "\n",
    "2. What wine pairs well with spicy food?  \n",
    "   - **No-RAG:** Chardonnay – Incorrect; its flavors don't complement spice well.  \n",
    "   - **RAG:** Riesling – Correct; sweetness balances spice.\n",
    "\n",
    "3. Best dessert for wine  \n",
    "   - **No-RAG:** A chocolate cake – Reasonable but not ideal.  \n",
    "   - **RAG:** Cheesecake – More versatile and pairs better with most dessert wines.\n",
    "\n",
    "4. What is the best wine to serve at a wedding?  \n",
    "   - **No-RAG:** Rosé – Acceptable but not ideal for diverse preferences.  \n",
    "   - **RAG:** A wine with high acidity – Vague but applicable (e.g., sparkling wines).\n",
    "\n",
    "5. What is a good wine for a romantic dinner?  \n",
    "   - **No-RAG:** Sauvignon Blanc – Acceptable but lacks elegance for romance.  \n",
    "   - **RAG:** A higher-acidity wine – Vague but could include good options like Pinot Noir.\n",
    "\n",
    "6. Which wine is ideal for a vegetarian meal?  \n",
    "   - **No-RAG:** Sauvignon Blanc – Acceptable for many vegetarian dishes.  \n",
    "   - **RAG:** Riesling – More versatile for a variety of vegetarian cuisines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c56b2e6a-bb69-4fe0-bd76-f6be8cec0d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/flan-t5-base\n",
      "\n",
      "Question: I'm new to wine. Which wine should I start with?\n",
      "No-RAG: Sauvignon Blanc\n",
      "RAG: white or rose\n",
      "\n",
      "Question: What wine pairs well with spicy food?\n",
      "No-RAG: Chardonnay\n",
      "RAG: Riesling\n",
      "\n",
      "Question: Best dessert for wine\n",
      "No-RAG: a chocolate cake\n",
      "RAG: cheesecake\n",
      "\n",
      "Question: What is the best wine to serve at a wedding?\n",
      "No-RAG: rosé\n",
      "RAG: a wine with high acidity\n",
      "\n",
      "Question: What is a good wine for a romantic dinner?\n",
      "No-RAG: Sauvignon Blanc\n",
      "RAG: a higher-acidity wine\n",
      "\n",
      "Question: Which wine is ideal for a vegetarian meal?\n",
      "No-RAG: Sauvignon Blanc\n",
      "RAG: Riesling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "print(f\"Model: {checkpoint}\", end='\\n\\n')\n",
    "\n",
    "# Retrieval settings (for RAG)\n",
    "top_k_retrieval = 5\n",
    "\n",
    "for query in query_list:\n",
    "    # --- No-RAG ---\n",
    "    input_text_no_rag = f\"Answer the following question based on your knowledge: {query}\"\n",
    "    inputs_no_rag = tokenizer(input_text_no_rag, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs_no_rag = model.generate(\n",
    "        inputs_no_rag['input_ids'], \n",
    "        attention_mask=inputs_no_rag['attention_mask'],\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.85,\n",
    "        max_new_tokens=100,\n",
    "        repetition_penalty=2.0,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    reply_no_rag = tokenizer.decode(outputs_no_rag[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # --- RAG ---\n",
    "    # Perform retrieval\n",
    "    doc = vector_store.similarity_search(query, k=top_k_retrieval)\n",
    "    retrieved_docs = [d.page_content for d in doc]\n",
    "    retrieved_docs = ''.join(retrieved_docs)\n",
    "\n",
    "    # Prepare input\n",
    "    input_text_rag = f\"Answer the following question based on your knowledge and the context:\\n\\nQuestion: {query}\\nContext: {retrieved_docs}\\nAnswer:\"\n",
    "    inputs_rag = tokenizer(input_text_rag, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    outputs_rag = model.generate(\n",
    "        inputs_rag['input_ids'],\n",
    "        attention_mask=inputs_rag['attention_mask'],\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        max_new_tokens=50,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    reply_rag = tokenizer.decode(outputs_rag[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"No-RAG: {reply_no_rag}\")\n",
    "    print(f\"RAG: {reply_rag}\", end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a4d0f6-9bfd-43ab-927e-503882be295e",
   "metadata": {},
   "source": [
    "## `google/flan-t5-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5b9f5202-5d9b-4345-a660-893a29d37aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/flan-t5-large\n",
      "\n",
      "Output written to reply_2025-01-07_14-58-03.txt\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "print(f\"Model: {checkpoint}\", end='\\n\\n')\n",
    "\n",
    "# Retrieval settings (for RAG)\n",
    "top_k_retrieval = 1\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "for query in query_list:\n",
    "    # --- No-RAG ---\n",
    "    input_text_no_rag = f\"Answer the following question based on your knowledge: {query}\"\n",
    "    inputs_no_rag = tokenizer(input_text_no_rag, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs_no_rag = model.generate(\n",
    "        inputs_no_rag['input_ids'], \n",
    "        attention_mask=inputs_no_rag['attention_mask'],\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.85,\n",
    "        max_new_tokens=150,\n",
    "        repetition_penalty=2.0,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    reply_no_rag = tokenizer.decode(outputs_no_rag[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # --- RAG ---\n",
    "    # Perform retrieval\n",
    "    doc = vector_store.similarity_search(query, k=top_k_retrieval)\n",
    "    retrieved_docs = [d.page_content for d in doc]\n",
    "    retrieved_docs = ''.join(retrieved_docs)\n",
    "\n",
    "    # Prepare input\n",
    "    input_text_rag = f\"Answer the following question in detail based on your knowledge and the context:\\n\\nQuestion: {query}\\nContext: {retrieved_docs}\\nAnswer:\"\n",
    "    inputs_rag = tokenizer(input_text_rag, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    outputs_rag = model.generate(\n",
    "        inputs_rag['input_ids'],\n",
    "        attention_mask=inputs_rag['attention_mask'],\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.85,\n",
    "        max_new_tokens=150,\n",
    "        repetition_penalty=0.5,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    reply_rag = tokenizer.decode(outputs_rag[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Append the result to the list\n",
    "    result = f\"# Question: {query}\\n\"\n",
    "    result += f\"-- No-RAG: {reply_no_rag}\\n\"\n",
    "    result += f\"-- RAG: {reply_rag}\\n\"\n",
    "    \n",
    "    retrieved_docs = retrieved_docs.split('\\n')\n",
    "    result += \"-- Contexts:\\n\"\n",
    "    result += \"\\n\".join(retrieved_docs).replace(\"\\n\\n\", \"\\n\") + \"\\n\\n\"\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Write all results to the file after the loop\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"reply_{current_time}.txt\"\n",
    "\n",
    "with open(file_name, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(results)\n",
    "\n",
    "print(f\"Output written to {file_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbaffd-0176-4df1-bf74-d167743ac0de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
